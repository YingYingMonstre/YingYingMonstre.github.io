<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>高级神经网络结构（下） | 银河之家</title><meta name="keywords" content="PyTorch 神经网络"><meta name="author" content="银河"><meta name="copyright" content="银河"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="什么是自编码 (Autoencoder)今天我们会来聊聊用神经网络如何进行非监督形式的学习。也就是 autoencoder，自编码。 自编码 autoencoder 是一种什么码呢。他是不是条形码？二维码？打码？其中的一种呢？NONONONO。和他们统统没有关系。自编码是一种神经网络的形式。如果你一定要把他们扯上关系，我想也只能这样解释啦。 压缩与解压 有一个神经网络，它在做的事情是接收一张图片，">
<meta property="og:type" content="article">
<meta property="og:title" content="高级神经网络结构（下）">
<meta property="og:url" content="https://yingyingmonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/index.html">
<meta property="og:site_name" content="银河之家">
<meta property="og:description" content="什么是自编码 (Autoencoder)今天我们会来聊聊用神经网络如何进行非监督形式的学习。也就是 autoencoder，自编码。 自编码 autoencoder 是一种什么码呢。他是不是条形码？二维码？打码？其中的一种呢？NONONONO。和他们统统没有关系。自编码是一种神经网络的形式。如果你一定要把他们扯上关系，我想也只能这样解释啦。 压缩与解压 有一个神经网络，它在做的事情是接收一张图片，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969">
<meta property="article:published_time" content="2021-12-11T16:00:00.000Z">
<meta property="article:modified_time" content="2021-12-12T09:55:29.998Z">
<meta property="article:author" content="银河">
<meta property="article:tag" content="PyTorch 神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969"><link rel="shortcut icon" href="/YingYingMonstre.github.io/img/favicon.png"><link rel="canonical" href="https://yingyingmonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/YingYingMonstre.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/YingYingMonstre.github.io/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-12 17:55:29'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../../../../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/YingYingMonstre.github.io/atom.xml" title="银河之家" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/YingYingMonstre.github.io/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/YingYingMonstre.github.io/">银河之家</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">高级神经网络结构（下）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-11T16:00:00.000Z" title="发表于 2021-12-12 00:00:00">2021-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-12T09:55:29.998Z" title="更新于 2021-12-12 17:55:29">2021-12-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/YingYingMonstre.github.io/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="什么是自编码-Autoencoder"><a href="#什么是自编码-Autoencoder" class="headerlink" title="什么是自编码 (Autoencoder)"></a>什么是自编码 (Autoencoder)</h3><p>今天我们会来聊聊用神经网络如何进行非监督形式的学习。也就是 autoencoder，自编码。</p>
<p>自编码 autoencoder 是一种什么码呢。他是不是条形码？二维码？打码？其中的一种呢？NONONONO。和他们统统没有关系。自编码是一种神经网络的形式。如果你一定要把他们扯上关系，我想也只能这样解释啦。</p>
<h4 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/auto1.png"></p>
<p>有一个神经网络，它在做的事情是接收一张图片，然后 给它打码，最后再从打码后的图片中还原。太抽象啦？行，我们再具体点。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto2.png"></p>
<p>假设刚刚那个神经网络是这样，对应上刚刚的图片，可以看出图片其实是经过了压缩，再解压的这一道工序。当压缩的时候，原有的图片质量被缩减，解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片。为什么要这样做呢？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto3.png"></p>
<p>原来有时神经网络要接受大量的输入信息，比如输入信息是高清图片时，输入信息量可能达到上千万，让神经网络直接从上千万个信息源中学习是一件很吃力的工作。所以，何不压缩一下，提取出原图片中的最具代表性的信息，缩减输入信息量，再把缩减过后的信息放进神经网络学习。这样学习起来就简单轻松了。所以，自编码就能在这时发挥作用。通过将原数据白色的X 压缩，解压成黑色的X，然后通过对比黑白 X，求出预测误差，进行反向传递，逐步提升自编码的准确性。训练好的自编码中间这一部分就是能总结原数据的精髓。可以看出，从头到尾，我们只用到了输入数据 X，并没有用到 X 对应的数据标签，所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候。通常只会用到自编码前半部分。</p>
<h4 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h4><p><img src="https://static.mofanpy.com/results/ML-intro/auto4.png"></p>
<p>这部分也叫作 encoder 编码器。编码器能得到原数据的精髓，然后我们只需要再创建一个小的神经网络学习这个精髓的数据，不仅减少了神经网络的负担，而且同样能达到很好的效果。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto5.png"></p>
<p>这是一个通过自编码整理出来的数据，他能从原数据中总结出每种类型数据的特征，如果把这些特征类型都放在一张二维的图片上，每种类型都已经被很好的用原数据的精髓区分开来。如果你了解 PCA 主成分分析，再提取主要特征时，自编码和它一样，甚至超越了 PCA。换句话说，自编码可以像 PCA 一样给特征属性降维。</p>
<h4 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h4><p>至于解码器 Decoder，我们也能那它来做点事情。我们知道，解码器在训练的时候是要将精髓信息解压成原始信息，那么这就提供了一个解压器的作用，甚至我们可以认为是一个生成器 (类似于<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/intro-GAN">GAN</a>)。那做这件事的一种特殊自编码叫做 variational autoencoders，你能在<a target="_blank" rel="noopener" href="http://kvfrans.com/variational-autoencoders-explained/">这里</a>找到他的具体说明。</p>
<p>有一个例子就是让它能模仿并生成手写数字。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto6.jpg"></p>
<p><em>Python相关教程</em></p>
<ul>
<li><em>Tensorflow Autoencoder <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/autoencoder">链接</a></em></li>
<li><em>PyTorch RNN <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/autoencoder">例子</a></em></li>
<li><em>Keras Autoencoder <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/keras/autoencoder">链接</a></em></li>
</ul>
<h3 id="AutoEncoder-自编码-非监督学习"><a href="#AutoEncoder-自编码-非监督学习" class="headerlink" title="AutoEncoder (自编码/非监督学习)"></a>AutoEncoder (自编码/非监督学习)</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>神经网络也能进行非监督学习，只需要训练数据，不需要标签数据。自编码就是这样一种形式。自编码能自动分类数据，而且也能嵌套在半监督学习的上面，用少量的有标签样本和大量的无标签样本学习。如果对自编码还没有太多概念，强烈推荐我的这个<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-autoencoder">动画短片</a>，让你秒懂自编码。</p>
<p>这次我们还用 MNIST 手写数字数据来压缩再解压图片。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-4-1.gif"></p>
<p>然后用压缩的特征进行非监督分类。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-4-2.gif"></p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>自编码只用训练集就好了，而且只需要训练 training data 的 image，不用训练 labels。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">EPOCH = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>   <span class="comment"># 下过数据的话, 就可以设置成 False</span></span><br><span class="line">N_TEST_IMG = <span class="number">5</span>          <span class="comment"># 到时候显示 5张图片看效果, 如上图一</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mnist digits dataset</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./mnist/&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,                                     <span class="comment"># this is training data</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),    <span class="comment"># Converts a PIL.Image or numpy.ndarray to</span></span><br><span class="line">                                                    <span class="comment"># torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]</span></span><br><span class="line">    download=DOWNLOAD_MNIST,                        <span class="comment"># download it if you don&#x27;t have it</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/4-4-3.png"></p>
<p>这就是一张我们要训练的手写数字 4。</p>
<h4 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h4><p>AutoEncoder 形式很简单，分别是 <code>encoder</code> 和 <code>decoder</code>，压缩和解压，压缩后得到压缩的特征值，再从压缩的特征值解压成原图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AutoEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AutoEncoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 压缩</span></span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">12</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">3</span>),   <span class="comment"># 压缩成3个特征, 进行 3D 图像可视化</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 解压</span></span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3</span>, <span class="number">12</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            nn.Sigmoid(),       <span class="comment"># 激励函数让输出值在 (0, 1)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        encoded = self.encoder(x)</span><br><span class="line">        decoded = self.decoder(encoded)</span><br><span class="line">        <span class="keyword">return</span> encoded, decoded</span><br><span class="line"></span><br><span class="line">autoencoder = AutoEncoder()</span><br></pre></td></tr></table></figure>



<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练，并可视化训练的过程。我们可以有效的利用 <code>encoder</code> 和 <code>decoder</code> 来做很多事，比如这里我们用 <code>decoder</code> 的信息输出看和原图片的对比，还能用 <code>encoder</code> 来看经过压缩后，神经网络对原图片的理解。<code>encoder</code> 能将不同图片数据大概的分离开来。这样就是一个无监督学习的过程。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-4-1.gif"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, b_label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        b_x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)   <span class="comment"># batch x, shape (batch, 28*28)</span></span><br><span class="line">        b_y = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)   <span class="comment"># batch y, shape (batch, 28*28)</span></span><br><span class="line"></span><br><span class="line">        encoded, decoded = autoencoder(b_x)</span><br><span class="line"></span><br><span class="line">        loss = loss_func(decoded, b_y)      <span class="comment"># mean square error</span></span><br><span class="line">        optimizer.zero_grad()               <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                     <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                    <span class="comment"># apply gradients</span></span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/4-4-4.png"></p>
<h4 id="画3D图"><a href="#画3D图" class="headerlink" title="画3D图"></a>画3D图</h4><p><img src="https://static.mofanpy.com/results/torch/4-4-2.gif"></p>
<p>3D 的可视化图挺有趣的，还能挪动观看，更加直观，好理解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要观看的数据</span></span><br><span class="line">view_data = train_data.train_data[:<span class="number">200</span>].view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>).<span class="built_in">type</span>(torch.FloatTensor)/<span class="number">255.</span></span><br><span class="line">encoded_data, _ = autoencoder(view_data)    <span class="comment"># 提取压缩的特征值</span></span><br><span class="line">fig = plt.figure(<span class="number">2</span>)</span><br><span class="line">ax = Axes3D(fig)    <span class="comment"># 3D 图</span></span><br><span class="line"><span class="comment"># x, y, z 的数据值</span></span><br><span class="line">X = encoded_data.data[:, <span class="number">0</span>].numpy()</span><br><span class="line">Y = encoded_data.data[:, <span class="number">1</span>].numpy()</span><br><span class="line">Z = encoded_data.data[:, <span class="number">2</span>].numpy()</span><br><span class="line">values = train_data.train_labels[:<span class="number">200</span>].numpy()  <span class="comment"># 标签值</span></span><br><span class="line"><span class="keyword">for</span> x, y, z, s <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, Z, values):</span><br><span class="line">    c = cm.rainbow(<span class="built_in">int</span>(<span class="number">255</span>*s/<span class="number">9</span>))    <span class="comment"># 上色</span></span><br><span class="line">    ax.text(x, y, z, s, backgroundcolor=c)  <span class="comment"># 标位子</span></span><br><span class="line">ax.set_xlim(X.<span class="built_in">min</span>(), X.<span class="built_in">max</span>())</span><br><span class="line">ax.set_ylim(Y.<span class="built_in">min</span>(), Y.<span class="built_in">max</span>())</span><br><span class="line">ax.set_zlim(Z.<span class="built_in">min</span>(), Z.<span class="built_in">max</span>())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/4-4-5.png"></p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="什么是-DQN"><a href="#什么是-DQN" class="headerlink" title="什么是 DQN"></a>什么是 DQN</h3><p>今天我们会来说说强化学习中的一种强大武器，Deep Q Network 简称为 DQN。Google Deep mind 团队就是靠着这 DQN 使计算机玩电动玩得比我们还厉害。</p>
<h4 id="强化学习与神经网络"><a href="#强化学习与神经网络" class="headerlink" title="强化学习与神经网络"></a>强化学习与神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN1.png"></p>
<p>之前我们所谈论到的强化学习方法都是比较传统的方式，而如今，随着机器学习在日常生活中的各种应用，各种机器学习方法也在融汇、合并、升级。而我们今天所要探讨的强化学习则是这么一种融合了神经网络和 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 的方法，名字叫做 Deep Q Network。这种新型结构是为什么被提出来呢？原来，传统的表格形式的强化学习有这样一个瓶颈。</p>
<h4 id="神经网络的作用"><a href="#神经网络的作用" class="headerlink" title="神经网络的作用"></a>神经网络的作用</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN2.png"></p>
<p>我们使用表格来存储每一个状态 state，和在这个 state 每个行为 action 所拥有的 Q 值。而当今问题是在太复杂，状态可以多到比天上的星星还多(比如下围棋)。如果全用表格来存储它们，恐怕我们的计算机有再大的内存都不够，而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事。不过，在机器学习中，有一种方法对这种事情很在行，那就是神经网络。我们可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样我们就没必要在表格中记录 Q 值，而是直接使用神经网络生成 Q 值。还有一种形式的是这样，我们也能只输入状态值，输出所有的动作值，然后按照 Q learning 的原则，直接选择拥有最大值的动作当做下一步要做的动作。我们可以想象，神经网络接受外部的信息，相当于眼睛鼻子耳朵收集信息，然后通过大脑加工输出每种动作的值，最后通过强化学习的方式选择动作。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/DQN4.png"></p>
<p>我们通过 NN 预测出Q(s2, a1) 和 Q(s2,a2) 的值，这就是 Q 估计。然后我们选取 Q 估计中最大值的动作来换取环境中的奖励 reward。而 Q 现实中也包含从神经网络分析出来的两个 Q 估计值，不过这个 Q 估计是针对于下一步在 s’ 的估计。最后再通过刚刚所说的算法更新神经网络中的参数。但是这并不是 DQN 会玩电动的根本原因。还有两大因素支撑着 DQN 使得它变得无比强大。这两大因素就是 Experience replay 和 Fixed Q-targets。</p>
<h4 id="DQN-两大利器"><a href="#DQN-两大利器" class="headerlink" title="DQN 两大利器"></a>DQN 两大利器</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN5.png"></p>
<p>简单来说，DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过，Q learning 是一种 off-policy 离线学习法，它能学习当前经历着的，也能学习过去经历过的，甚至是学习别人的经历。所以每次 DQN 更新的时候，我们都可以随机抽取一些之前的经历进行学习。随机抽取这种做法打乱了经历之间的相关性，也使得神经网络更新更有效率。Fixed Q-targets 也是一种打乱相关性的机理，如果使用 fixed Q-targets，我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络，预测 Q 估计的神经网络具备最新的参数，而预测 Q 现实的神经网络使用的参数则是很久以前的。有了这两种提升手段，DQN 才能在一些游戏中超越人类。</p>
<blockquote>
<p>Q：在DQN中，为什么要用两个网络呢？</p>
<p>A：以前的研究者都是用一个网络，但是发现了问题，一个网络的抖动太厉害了，学习极不稳定。强化学习对于数据分布的变化非常敏感，所以有人尝试用两个网络，将target net半固定住，降低抖动，增强收敛性。</p>
</blockquote>
<h3 id="DQN-强化学习"><a href="#DQN-强化学习" class="headerlink" title="DQN 强化学习"></a>DQN 强化学习</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>Torch 是神经网络库，那么也可以拿来做强化学习，之前我用另一个强大神经网络库 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow</a> 来制作了这一个 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">从浅入深强化学习教程</a>，你同样也可以用 PyTorch 来实现，这次我们就举 DQN 的例子，我对比了我的 Tensorflow DQN 的代码，发现 PyTorch 写的要简单很多。如果对 DQN 或者强化学习还没有太多概念，强烈推荐我的这个<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">DQN动画短片</a>，让你秒懂DQN。还有强推这套花了我几个月来制作的<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习教程</a>！</p>
<h4 id="模块导入和参数设置"><a href="#模块导入和参数设置" class="headerlink" title="模块导入和参数设置"></a>模块导入和参数设置</h4><p>这次除了 Torch 自家模块，我们还要导入 Gym 环境库模块，<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/gym">如何安装 gym 模块请看这节教程</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">LR = <span class="number">0.01</span>                   <span class="comment"># learning rate</span></span><br><span class="line">EPSILON = <span class="number">0.9</span>               <span class="comment"># 最优选择动作百分比</span></span><br><span class="line">GAMMA = <span class="number">0.9</span>                 <span class="comment"># 奖励递减参数</span></span><br><span class="line">TARGET_REPLACE_ITER = <span class="number">100</span>   <span class="comment"># Q 现实网络的更新频率</span></span><br><span class="line">MEMORY_CAPACITY = <span class="number">2000</span>      <span class="comment"># 记忆库大小</span></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)   <span class="comment"># 立杆子游戏</span></span><br><span class="line">env = env.unwrapped</span><br><span class="line">N_ACTIONS = env.action_space.n  <span class="comment"># 杆子能做的动作</span></span><br><span class="line">N_STATES = env.observation_space.shape[<span class="number">0</span>]   <span class="comment"># 杆子能获取的环境信息数</span></span><br></pre></td></tr></table></figure>



<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p>DQN 当中的神经网络模式，我们将依据这个模式建立两个神经网络，一个是现实网络 (Target Net)，一个是估计网络 (Eval Net)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, </span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(N_STATES, <span class="number">10</span>)</span><br><span class="line">        self.fc1.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)   <span class="comment"># initialization</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">10</span>, N_ACTIONS)</span><br><span class="line">        self.out.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)   <span class="comment"># initialization</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        actions_value = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> actions_value</span><br></pre></td></tr></table></figure>



<h4 id="DQN体系"><a href="#DQN体系" class="headerlink" title="DQN体系"></a>DQN体系</h4><p>简化的 DQN 体系是这样，我们有两个 net，有选动作机制，有存经历机制，有学习机制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 建立 target net 和 eval net 还有 memory</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 根据环境观测值选择动作的机制</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="comment"># 存储记忆</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># target 网络更新</span></span><br><span class="line">        <span class="comment"># 学习记忆库中的记忆</span></span><br></pre></td></tr></table></figure>

<p>接下来就是具体的啦，在 DQN 中每个功能都是怎么做的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.eval_net, self.target_net = Net(), Net()</span><br><span class="line"></span><br><span class="line">        self.learn_step_counter = <span class="number">0</span>     <span class="comment"># 用于 target 更新计时</span></span><br><span class="line">        self.memory_counter = <span class="number">0</span>         <span class="comment"># 记忆库记数</span></span><br><span class="line">        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * <span class="number">2</span> + <span class="number">2</span>))     <span class="comment"># 初始化记忆库</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    <span class="comment"># torch 的优化器</span></span><br><span class="line">        self.loss_func = nn.MSELoss()   <span class="comment"># 误差公式</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = torch.unsqueeze(torch.FloatTensor(x), <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 这里只输入一个 sample</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; EPSILON:   <span class="comment"># 选最优动作</span></span><br><span class="line">            actions_value = self.eval_net.forward(x)</span><br><span class="line">            action = torch.<span class="built_in">max</span>(actions_value, <span class="number">1</span>)[<span class="number">1</span>].data.numpy()[<span class="number">0</span>, <span class="number">0</span>]     <span class="comment"># return the argmax</span></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 选随机动作</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, N_ACTIONS)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line">        <span class="comment"># 如果记忆库满了, 就覆盖老数据</span></span><br><span class="line">        index = self.memory_counter % MEMORY_CAPACITY</span><br><span class="line">        self.memory[index, :] = transition</span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># target net 参数更新</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % TARGET_REPLACE_ITER == <span class="number">0</span>:</span><br><span class="line">            self.target_net.load_state_dict(self.eval_net.state_dict())</span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 抽取记忆库中的批数据</span></span><br><span class="line">        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)</span><br><span class="line">        b_memory = self.memory[sample_index, :]</span><br><span class="line">        b_s = torch.FloatTensor(b_memory[:, :N_STATES])</span><br><span class="line">        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+<span class="number">1</span>].astype(<span class="built_in">int</span>))</span><br><span class="line">        b_r = torch.FloatTensor(b_memory[:, N_STATES+<span class="number">1</span>:N_STATES+<span class="number">2</span>])</span><br><span class="line">        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)</span></span><br><span class="line">        q_eval = self.eval_net(b_s).gather(<span class="number">1</span>, b_a)  <span class="comment"># shape (batch, 1)</span></span><br><span class="line">        q_next = self.target_net(b_s_).detach()     <span class="comment"># q_next 不进行反向传递误差, 所以 detach</span></span><br><span class="line">        q_target = b_r + GAMMA * q_next.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>]   <span class="comment"># shape (batch, 1)</span></span><br><span class="line">        loss = self.loss_func(q_eval, q_target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算, 更新 eval net</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br></pre></td></tr></table></figure>



<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>按照 Qlearning 的形式进行 off-policy 的更新。我们进行回合制更行，一个回合完了，进入下一回合。一直到他们将杆子立起来很久。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dqn = DQN() <span class="comment"># 定义 DQN 系统</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        env.render()    <span class="comment"># 显示实验动画</span></span><br><span class="line">        a = dqn.choose_action(s)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选动作, 得到环境反馈</span></span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 修改 reward, 使 DQN 快速学习</span></span><br><span class="line">        x, x_dot, theta, theta_dot = s_</span><br><span class="line">        r1 = (env.x_threshold - <span class="built_in">abs</span>(x)) / env.x_threshold - <span class="number">0.8</span></span><br><span class="line">        r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta)) / env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">        r = r1 + r2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存记忆</span></span><br><span class="line">        dqn.store_transition(s, a, r, s_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dqn.memory_counter &gt; MEMORY_CAPACITY:</span><br><span class="line">            dqn.learn() <span class="comment"># 记忆库满了就进行学习</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> done:    <span class="comment"># 如果回合结束, 进入下回合</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        s = s_</span><br></pre></td></tr></table></figure>

<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/405_DQN_Reinforcement_learning.py">github 代码</a> 中的每一步的意义啦。</p>
<h4 id="附加-A3C"><a href="#附加-A3C" class="headerlink" title="附加 A3C"></a>附加 A3C</h4><p>强化学习中还有一个非常厉害的算法，叫做 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.01783.pdf">A3C</a>。我做过一个这个算法的<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-A3C">简介视频</a>，它非常合理地运用了多核计算机的能力，让我们能使用多个核来训练强化学习。我也用 pytorch 将这个算法给<a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/pytorch-A3C">实现</a>了。代码非常简单，可以用来做连续动作的环境。训练的效果可以看到这里。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/6-3-8.png"></p>
<h3 id="什么是生成对抗网络-GAN"><a href="#什么是生成对抗网络-GAN" class="headerlink" title="什么是生成对抗网络 (GAN)"></a>什么是生成对抗网络 (GAN)</h3><p>GAN，又称生成对抗网络，也是 Generative Adversarial Nets 的简称。</p>
<h4 id="常见神经网络形式"><a href="#常见神经网络形式" class="headerlink" title="常见神经网络形式"></a>常见神经网络形式</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan2.png"></p>
<p>神经网络分很多种，有普通的前向传播神经网络，有分析图片的 CNN 卷积神经网络，有分析序列化数据，比如语音的 RNN 循环神经网络，这些神经网络都是用来输入数据，得到想要的结果，我们看中的是这些神经网络能很好的将数据与结果通过某种关系联系起来。</p>
<h4 id="生成网络"><a href="#生成网络" class="headerlink" title="生成网络"></a>生成网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan3.png"></p>
<p>但是还有另外一种形式的神经网络，他不是用来把数据对应上结果的，而是用来”凭空”捏造结果，这就是我们要说的生成网络啦。GAN 就是其中的一种形式。那么 GAN 是怎么做到的呢？当然这里的”凭空”并不是什么都没有的空盒子，而是一些随机数。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/gan4.png"></p>
<p>对，你没听错，我们就是用没有意义的随机数来生成有有意义的作品，比如著名画作。当然，这还不是全部，这只是一个 GAN 的一部分而已，这一部分的神经网络我们可以想象成是一个新手画家。</p>
<h4 id="新手画家"><a href="#新手画家" class="headerlink" title="新手画家"></a>新手画家</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan5.png"></p>
<p>画家作画都需要点灵感，他们都是依照自己的灵感来完成作品。有了灵感不一定有用，因为他的作画技术并没有我们想象得好，画出来有可能是一团糟。这可怎么办，聪明的新手画家找到了自己的一个正在学鉴赏的好朋友 – 新手鉴赏家。</p>
<h4 id="新手鉴赏家"><a href="#新手鉴赏家" class="headerlink" title="新手鉴赏家"></a>新手鉴赏家</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan6.png"></p>
<p>可是新手鉴赏家也没什么能耐，他也不知道如何鉴赏著名画作，所以坐在电脑旁边的你实在看不下去了，拿起几个标签往屏幕上一甩，然后新手鉴赏家就被你这样一次次的甩来甩去着甩乖了，慢慢也学会了怎么样区分著名画家的画了。重要的是，新手鉴赏家和新手画家是好朋友，他们总爱分享学习到的东西。</p>
<h4 id="新手鉴赏家和新手画家"><a href="#新手鉴赏家和新手画家" class="headerlink" title="新手鉴赏家和新手画家"></a>新手鉴赏家和新手画家</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan7.png"></p>
<p>所以新手鉴赏家告诉新手画家，“你的画实在太丑了，你看看人家达芬奇，你也学学它呀，比如这里要多加一点，这里要画淡一点。” 就这样，新手鉴赏家将他从你这里所学到的知识都分享给了新手画家，让好朋友新手画家也能越画越像达芬奇。这就是 GAN 的整套流程，我们在来理一下。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/gan8.png"></p>
<p>新手画家用随机灵感画画，新手鉴赏家会接收一些画作，但是他不知道这是新手画家画的还是著名画家画的，他说出他的判断，你来纠正他的判断，新手鉴赏家一边学如何判断，一边告诉新手画家要怎么画才能画得更像著名画家，新手画家就能学习到如何从自己的灵感画出更像著名画家的画了。GAN 也就这么回事。</p>
<h4 id="GAN-网络"><a href="#GAN-网络" class="headerlink" title="GAN 网络"></a>GAN 网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan9.png"></p>
<p>Generator 会根据随机数来生成有意义的数据，Discriminator 会学习如何判断哪些是真实数据，哪些是生成数据，然后将学习的经验反向传递给 Generator，让 Generator 能根据随机数生成更像真实数据的数据。这样训练出来的 Generator 可以有很多用途，比如最近有人就拿它来生成各种卧室的图片。</p>
<h4 id="GAN-应用"><a href="#GAN-应用" class="headerlink" title="GAN 应用"></a>GAN 应用</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan10.png"></p>
<p>甚至你还能玩点新花样，比如让图片来做加减法，戴眼镜的男人减去男人加上女人, 他居然能生成戴眼镜的女人的图片。甚至还能根据你随便画的几笔草图来生成可能是你需要的蓝天白云大草地图片。哈哈，看起来机器也能有想象力啦。如果你想试着动手做一个 GAN 的实践，却不知道如何做，不用担心，我也为准备好了一个使用 Python 和他神经网络模块搭建的最简单的 GAN 实践代码。欢迎大家访问莫烦 Python 了解更多机器学习的内容。</p>
<p><em>Python相关教程</em></p>
<ul>
<li><em>Tensorflow <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/406_GAN.py">50行 GAN 代码</a></em></li>
<li><em>PyTorch <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/GAN">GAN 教程</a></em></li>
</ul>
<h3 id="GAN-Generative-Adversarial-Nets-生成对抗网络"><a href="#GAN-Generative-Adversarial-Nets-生成对抗网络" class="headerlink" title="GAN (Generative Adversarial Nets 生成对抗网络)"></a>GAN (Generative Adversarial Nets 生成对抗网络)</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>GAN 是一个近几年比较流行的生成网络形式。对比起传统的生成模型，他减少了模型限制和生成器限制，他具有有更好的生成能力。人们常用假钞鉴定者和假钞制造者来打比喻，但是我不喜欢这个比喻，觉得没有真实反映出 GAN 里面的机理。</p>
<p>所以我的一句话介绍 GAN 就是：Generator 是新手画家，Discriminator 是新手鉴赏家，你是高级鉴赏家。你将著名画家的品和新手画家的作品都给新手鉴赏家评定，并告诉新手鉴赏家哪些是新手画家画的，哪些是著名画家画的，新手鉴赏家就慢慢学习怎么区分新手画家和著名画家的画，但是新手画家和新手鉴赏家是好朋友，新手鉴赏家会告诉新手画家要怎么样画得更像著名画家，新手画家就能将自己的突然来的灵感 (random noise) 画得更像著名画家。我用一个短动画形式来诠释了整个过程 (<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/intro-GAN">GAN 动画简介</a>)。</p>
<p>下面是本节内容的效果，绿线的变化是新手画家慢慢学习如何踏上画家之路的过程。而能被认定为著名的画作在 <code>upper bound</code> 和 <code>lower bound</code> 之间。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-6-1.gif"></p>
<h4 id="超参数设置"><a href="#超参数设置" class="headerlink" title="超参数设置"></a>超参数设置</h4><p>新手画家 (Generator) 在作画的时候需要有一些灵感 (random noise)，我们这些灵感的个数定义为 <code>N_IDEAS</code>。而一幅画需要有一些规格，我们将这幅画的画笔数定义一下，<code>N_COMPONENTS</code> 就是一条一元二次曲线(这幅画画)上的点个数。为了进行批训练，我们将一整批话的点都规定一下(<code>PAINT_POINTS</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LR_G = <span class="number">0.0001</span>           <span class="comment"># learning rate for generator</span></span><br><span class="line">LR_D = <span class="number">0.0001</span>           <span class="comment"># learning rate for discriminator</span></span><br><span class="line">N_IDEAS = <span class="number">5</span>             <span class="comment"># think of this as number of ideas for generating an art work (Generator)</span></span><br><span class="line">ART_COMPONENTS = <span class="number">15</span>     <span class="comment"># it could be total point G can draw in the canvas</span></span><br><span class="line">PAINT_POINTS = np.vstack([np.linspace(-<span class="number">1</span>, <span class="number">1</span>, ART_COMPONENTS) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(BATCH_SIZE)])</span><br></pre></td></tr></table></figure>



<h4 id="著名画家的画"><a href="#著名画家的画" class="headerlink" title="著名画家的画"></a>著名画家的画</h4><p>我们需要有很多画是来自著名画家的(real data)，将这些著名画家的画，和新手画家的画都传给新手鉴赏家，让鉴赏家来区分哪些是著名画家，哪些是新手画家的画。如何区分我们在后面呈现。这里我们生成一些著名画家的画 (batch 条不同的一元二次方程曲线)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">artist_works</span>():</span>     <span class="comment"># painting from the famous artist (real target)</span></span><br><span class="line">    a = np.random.uniform(<span class="number">1</span>, <span class="number">2</span>, size=BATCH_SIZE)[:, np.newaxis]</span><br><span class="line">    paintings = a * np.power(PAINT_POINTS, <span class="number">2</span>) + (a-<span class="number">1</span>)</span><br><span class="line">    paintings = torch.from_numpy(paintings).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> paintings</span><br></pre></td></tr></table></figure>

<p>下面就是会产生曲线的一个上限和下限。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-6-1.png"></p>
<h4 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h4><p>这里会创建两个神经网络，分别是 Generator (新手画家)，Discriminator(新手鉴赏家)。<code>G</code> 会拿着自己的一些灵感当做输入，输出一元二次曲线上的点 (<code>G</code> 的画)。</p>
<p><code>D</code> 会接收一幅画作 (一元二次曲线)，输出这幅画作到底是不是著名画家的画(是著名画家的画的概率)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">G = nn.Sequential(                      <span class="comment"># Generator</span></span><br><span class="line">    nn.Linear(N_IDEAS, <span class="number">128</span>),            <span class="comment"># random ideas (could from normal distribution)</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, ART_COMPONENTS),     <span class="comment"># making a painting from these random ideas</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">D = nn.Sequential(                      <span class="comment"># Discriminator</span></span><br><span class="line">    nn.Linear(ART_COMPONENTS, <span class="number">128</span>),     <span class="comment"># receive art work either from the famous artist or a newbie like G</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">    nn.Sigmoid(),                       <span class="comment"># tell the probability that the art work is made by artist</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h4 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h4><p>接着我们来同时训练 <code>D</code> 和 <code>G</code>。训练之前，我们来看看<code>G</code>作画的原理。<code>G</code> 首先会有些灵感，<code>G_ideas</code> 就会拿到这些随机灵感 (可以是正态分布的随机数)，然后 <code>G</code> 会根据这些灵感画画。接着我们拿着著名画家的画和 <code>G</code> 的画，让 <code>D</code> 来判定这两批画作是著名画家画的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    artist_paintings = artist_works()           <span class="comment"># real painting from artist</span></span><br><span class="line">    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)    <span class="comment"># random ideas</span></span><br><span class="line">    G_paintings = G(G_ideas())                  <span class="comment"># fake painting from G (random ideas)</span></span><br><span class="line"></span><br><span class="line">    prob_artist0 = D(artist_paintings)          <span class="comment"># D try to increase this prob</span></span><br><span class="line">    prob_artist1 = D(G_paintings)               <span class="comment"># D try to reduce this prob</span></span><br></pre></td></tr></table></figure>

<p>然后计算有多少来之画家的画猜对了，有多少来自 <code>G</code> 的画猜对了，我们想最大化这些猜对的次数。这也就是 <code>log(D(x)) + log(1-D(G(z))</code> 在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">论文</a>中的形式。而因为 torch 中提升参数的形式是最小化误差，那我们把最大化 <code>score</code> 转换成最小化 <code>loss</code>，在两个 <code>score</code> 的合的地方加一个符号就好。而 <code>G</code> 的提升就是要减小 <code>D</code> 猜测 <code>G</code> 生成数据的正确率，也就是减小 <code>D_score1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(<span class="number">1.</span> - prob_artist1))</span><br><span class="line">    G_loss = torch.mean(torch.log(<span class="number">1.</span> - prob_artist1))</span><br></pre></td></tr></table></figure>

<p>最后我们在根据 <code>loss</code> 提升神经网络就好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">opt_D.zero_grad()</span><br><span class="line">    D_loss.backward(retain_graph=<span class="literal">True</span>)      <span class="comment"># retain_graph 这个参数是为了再次使用计算图纸</span></span><br><span class="line">    opt_D.step()</span><br><span class="line"></span><br><span class="line">    opt_G.zero_grad()</span><br><span class="line">    G_loss.backward()</span><br><span class="line">    opt_G.step()</span><br></pre></td></tr></table></figure>

<p>上面的全部代码内容在我的 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py">github</a>。</p>
<h4 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h4><p>可视化的代码很简单，在这里就不会意义叙说了，大家直接看<a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py">代码</a> 吧。在本节的最上面就是这次的动图效果，最后达到收敛时，结果如下，<code>G</code> 能成功的根据自己的灵感，产生出一条很像 <code>artist</code> 画出的曲线，而 <code>D</code> 再也没有能力猜出这到底是 <code>G</code> 的画作还是 <code>artist</code> 的画作，他只能一半时间猜是 <code>G</code> 的，一半时间猜是 <code>artist</code>的。</p>
<p><img src="https://static.mofanpy.com/results-small/torch/4-6-2.png"></p>
<blockquote>
<p><strong>原文地址：</strong><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li>Tensorflow Autoencoder <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/autoencoder">链接</a></li>
<li>PyTorch RNN <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/autoencoder">例子</a></li>
<li>Keras Autoencoder <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/keras/autoencoder">链接</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py">第二节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/404_AutoEncoder.py">Tensorflow 的 50行 AutoEncoder 代码</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-autoencoder">我制作的 自编码 动画简介</a></li>
<li><a target="_blank" rel="noopener" href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">DQN Tensorflow Python 教程</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/DQN">DQN PyTorch Python 教程</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/405_DQN_Reinforcement_learning.py">第四节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/405_DQN_reinforcement_learning.py">Tensorflow 的 100行 DQN 代码</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">我制作的 DQN 动画简介</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">我的 DQN Tensorflow 教程</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">我的 强化学习 教程</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/GAN">PyTorch GAN 教程</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/406_GAN.py">Tensorflow 50行 GAN 代码</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">论文 Generative Adversarial Networks</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py">第六节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_conditional_GAN.py">Conditional GAN 代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/406_GAN.py">Tensorflow 50行 GAN 代码</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/intro-GAN">我制作的 GAN 动画简介</a></li>
</ul>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">银河</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yingyingmonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/">https://yingyingmonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://YingYingMonstre.github.io" target="_blank">银河之家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/YingYingMonstre.github.io/tags/PyTorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">PyTorch 神经网络</a></div><div class="post_share"><div class="social-share" data-image="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/"><img class="prev-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="onerror=null;src='/YingYingMonstre.github.io/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">PyTorch高阶内容</div></div></a></div><div class="next-post pull-right"><a href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/"><img class="next-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="onerror=null;src='/YingYingMonstre.github.io/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">高级神经网络结构（上）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/YingYingMonstre.github.io/2021/12/11/建造第一个神经网络/" title="建造第一个神经网络"><img class="cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-11</div><div class="title">建造第一个神经网络</div></div></a></div><div><a href="/YingYingMonstre.github.io/2021/12/11/高级神经网络结构（上）/" title="高级神经网络结构（上）"><img class="cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-11</div><div class="title">高级神经网络结构（上）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/YingYingMonstre.github.io/img/avatar.jpg" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">银河</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YingYingMonstre"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%BC%96%E7%A0%81-Autoencoder"><span class="toc-number">1.</span> <span class="toc-text">什么是自编码 (Autoencoder)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E4%B8%8E%E8%A7%A3%E5%8E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">压缩与解压</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="toc-number">1.2.</span> <span class="toc-text">编码器 Encoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="toc-number">1.3.</span> <span class="toc-text">解码器 Decoder</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AutoEncoder-%E8%87%AA%E7%BC%96%E7%A0%81-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.</span> <span class="toc-text">AutoEncoder (自编码&#x2F;非监督学习)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9"><span class="toc-number">2.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.2.</span> <span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AutoEncoder"><span class="toc-number">2.3.</span> <span class="toc-text">AutoEncoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%BB3D%E5%9B%BE"><span class="toc-number">2.5.</span> <span class="toc-text">画3D图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-DQN"><span class="toc-number">3.</span> <span class="toc-text">什么是 DQN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.1.</span> <span class="toc-text">强化学习与神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">3.2.</span> <span class="toc-text">神经网络的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DQN-%E4%B8%A4%E5%A4%A7%E5%88%A9%E5%99%A8"><span class="toc-number">3.3.</span> <span class="toc-text">DQN 两大利器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DQN-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text">DQN 强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-1"><span class="toc-number">4.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5%E5%92%8C%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">4.2.</span> <span class="toc-text">模块导入和参数设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DQN%E4%BD%93%E7%B3%BB"><span class="toc-number">4.4.</span> <span class="toc-text">DQN体系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">4.5.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%84%E5%8A%A0-A3C"><span class="toc-number">4.6.</span> <span class="toc-text">附加 A3C</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-GAN"><span class="toc-number">5.</span> <span class="toc-text">什么是生成对抗网络 (GAN)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BD%A2%E5%BC%8F"><span class="toc-number">5.1.</span> <span class="toc-text">常见神经网络形式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C"><span class="toc-number">5.2.</span> <span class="toc-text">生成网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B0%E6%89%8B%E7%94%BB%E5%AE%B6"><span class="toc-number">5.3.</span> <span class="toc-text">新手画家</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B0%E6%89%8B%E9%89%B4%E8%B5%8F%E5%AE%B6"><span class="toc-number">5.4.</span> <span class="toc-text">新手鉴赏家</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B0%E6%89%8B%E9%89%B4%E8%B5%8F%E5%AE%B6%E5%92%8C%E6%96%B0%E6%89%8B%E7%94%BB%E5%AE%B6"><span class="toc-number">5.5.</span> <span class="toc-text">新手鉴赏家和新手画家</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GAN-%E7%BD%91%E7%BB%9C"><span class="toc-number">5.6.</span> <span class="toc-text">GAN 网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GAN-%E5%BA%94%E7%94%A8"><span class="toc-number">5.7.</span> <span class="toc-text">GAN 应用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GAN-Generative-Adversarial-Nets-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">GAN (Generative Adversarial Nets 生成对抗网络)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-2"><span class="toc-number">6.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">6.2.</span> <span class="toc-text">超参数设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%91%97%E5%90%8D%E7%94%BB%E5%AE%B6%E7%9A%84%E7%94%BB"><span class="toc-number">6.3.</span> <span class="toc-text">著名画家的画</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="toc-number">6.4.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-2"><span class="toc-number">6.5.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">6.6.</span> <span class="toc-text">可视化训练过程</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/24/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%90%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AFLLVM/" title="深入浅出理解什么是LLVM"><img src="https://img1.baidu.com/it/u=3391566484,4223344013&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="深入浅出理解什么是LLVM"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/24/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%90%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AFLLVM/" title="深入浅出理解什么是LLVM">深入浅出理解什么是LLVM</a><time datetime="2021-12-24T04:00:00.000Z" title="发表于 2021-12-24 12:00:00">2021-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/" title="PyTorch高阶内容"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="PyTorch高阶内容"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/" title="PyTorch高阶内容">PyTorch高阶内容</a><time datetime="2021-12-12T12:40:00.000Z" title="发表于 2021-12-12 20:40:00">2021-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/" title="高级神经网络结构（下）"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="高级神经网络结构（下）"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/" title="高级神经网络结构（下）">高级神经网络结构（下）</a><time datetime="2021-12-11T16:00:00.000Z" title="发表于 2021-12-12 00:00:00">2021-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/" title="高级神经网络结构（上）"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="高级神经网络结构（上）"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/" title="高级神经网络结构（上）">高级神经网络结构（上）</a><time datetime="2021-12-11T14:00:00.000Z" title="发表于 2021-12-11 22:00:00">2021-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="建造第一个神经网络"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="建造第一个神经网络"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="建造第一个神经网络">建造第一个神经网络</a><time datetime="2021-12-11T11:30:00.000Z" title="发表于 2021-12-11 19:30:00">2021-12-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 银河</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/YingYingMonstre.github.io/js/utils.js"></script><script src="/YingYingMonstre.github.io/js/main.js"></script><script src="/YingYingMonstre.github.io/js/search/local-search.js"></script><div class="js-pjax"></div><script src="js/custom.js"></script><script src="../js/custom.js"></script><script src="../../js/custom.js"></script><script src="../../../../js/custom.js"></script></div></body></html>