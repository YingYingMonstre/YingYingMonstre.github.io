<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>PyTorch高阶内容 | 银河之家</title><meta name="keywords" content="PyTorch"><meta name="author" content="银河"><meta name="copyright" content="银河"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="为什么 Torch 是动态的要点听说过 Torch 的人都听说了 torch 是动态的，那他的动态到底是什么呢？我们用一个 RNN 的例子来展示一下动态计算到底长什么样。 动态?静态?对比静态动态，我们就得知道谁是静态的。在流行的神经网络模块中，Tensorflow 就是最典型的静态计算模块。下图是一种我在强化学习教程中的 Tensorflow 计算图。也就是说，大部分时候，用 Tensorflo">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch高阶内容">
<meta property="og:url" content="https://yingyingmonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/index.html">
<meta property="og:site_name" content="银河之家">
<meta property="og:description" content="为什么 Torch 是动态的要点听说过 Torch 的人都听说了 torch 是动态的，那他的动态到底是什么呢？我们用一个 RNN 的例子来展示一下动态计算到底长什么样。 动态?静态?对比静态动态，我们就得知道谁是静态的。在流行的神经网络模块中，Tensorflow 就是最典型的静态计算模块。下图是一种我在强化学习教程中的 Tensorflow 计算图。也就是说，大部分时候，用 Tensorflo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969">
<meta property="article:published_time" content="2021-12-12T12:40:00.000Z">
<meta property="article:modified_time" content="2021-12-12T13:35:06.312Z">
<meta property="article:author" content="银河">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969"><link rel="shortcut icon" href="/YingYingMonstre.github.io/img/favicon.png"><link rel="canonical" href="https://yingyingmonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/YingYingMonstre.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/YingYingMonstre.github.io/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-12 21:35:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../../../../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/YingYingMonstre.github.io/atom.xml" title="银河之家" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/YingYingMonstre.github.io/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/YingYingMonstre.github.io/">银河之家</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">PyTorch高阶内容</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-12T12:40:00.000Z" title="发表于 2021-12-12 20:40:00">2021-12-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-12T13:35:06.312Z" title="更新于 2021-12-12 21:35:06">2021-12-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/YingYingMonstre.github.io/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="为什么-Torch-是动态的"><a href="#为什么-Torch-是动态的" class="headerlink" title="为什么 Torch 是动态的"></a>为什么 Torch 是动态的</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>听说过 Torch 的人都听说了 torch 是动态的，那他的动态到底是什么呢？我们用一个 RNN 的例子来展示一下动态计算到底长什么样。</p>
<h4 id="动态-静态"><a href="#动态-静态" class="headerlink" title="动态?静态?"></a>动态?静态?</h4><p>对比静态动态，我们就得知道谁是静态的。在流行的神经网络模块中，Tensorflow 就是最典型的静态计算模块。下图是一种我在<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习教程</a>中的 Tensorflow 计算图。也就是说，大部分时候，用 Tensorflow 是先搭建好这样一个计算系统，一旦搭建好了，就不能改动了 (也有例外, 比如<code>dynamic_rnn()</code>，但是总体来说他还是运用了一个静态思维)，所有的计算都会在这种图中流动，当然很多情况，这样就够了，我们不需要改动什么结构。不动结构当然可以提高效率。但是一旦计算流程不是静态的，计算图要变动。最典型的例子就是 RNN，有时候 RNN 的 time step 不会一样，或者在 training 和 testing 的时候，<code>batch_size</code> 和 <code>time_step</code> 也不一样，这时，Tensorflow 就头疼了，Tensorflow 的人也头疼了。哈哈，如果用一个动态计算图的 Torch，我们就好理解多了，写起来也简单多了。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/6-2-2.png"></p>
<h4 id="动态RNN"><a href="#动态RNN" class="headerlink" title="动态RNN"></a>动态RNN</h4><p>我们拿 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/RNN-regression">这一节内容的 RNN</a> 来解释动态计算图。那节内容的<a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/11_RNN_regressor.py">代码在这</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">######################## 前面代码都一样, 下面开始不同 #########################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################ 那节内容的代码结构 (静态 time step) ##########</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    start, end = step * np.pi, (step+<span class="number">1</span>)*np.pi   <span class="comment"># time steps 都是一样长的</span></span><br><span class="line">    <span class="comment"># use sin predicts cos</span></span><br><span class="line">    steps = np.linspace(start, end, <span class="number">10</span>, dtype=np.float32)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################ 这节内容修改代码 (动态 time step) #########</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    dynamic_steps = np.random.randint(<span class="number">1</span>, <span class="number">4</span>)  <span class="comment"># 随机 time step 长度</span></span><br><span class="line">    start, end = step * np.pi, (step + dynamic_steps) * np.pi  <span class="comment"># different time steps length</span></span><br><span class="line">    step += dynamic_steps</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use sin predicts cos</span></span><br><span class="line">    steps = np.linspace(start, end, <span class="number">10</span> * dynamic_steps, dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################  这下面又一样了 ###########################</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(steps))   <span class="comment"># print how many time step feed to RNN</span></span><br><span class="line"></span><br><span class="line">    x_np = np.sin(steps)    <span class="comment"># float32 for converting torch FloatTensor</span></span><br><span class="line">    y_np = np.cos(steps)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出的动态time step 长</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">10</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">20</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>有人会说了，Tensorflow 也有类似的功能呀，比如说 <code>dynamic_rnn()</code>。对的，没错，不过大家是否想过，如果我在 Tensorflow 当中定义一个 input 的 <code>placeholder</code>，这个 <code>placeholder</code> 将会有 (<code>batch</code>, <code>time step</code>, <code>input size</code>) 这几个维度，<code>batch</code> 好说，随便什么大小都可以，可是 <code>time step</code> 可是固定的呀，这可不好改，或者说改起来很麻烦。那 PyTorch 中又可以变 <code>batch</code> 又可以变 <code>time step</code>，这不是很方便吗。这就体现了动态神经网络的好处。</p>
<p>经过这样的折腾，torch 还能 handle 住，已经很不容易啦。所以当你想要处理这些动态计算图的时候，Torch 还是你首选的神经网络模块。</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/501_why_torch_dynamic_graph.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="GPU-加速运算"><a href="#GPU-加速运算" class="headerlink" title="GPU 加速运算"></a>GPU 加速运算</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>在 GPU 训练可以大幅提升运算速度。而且 Torch 也有一套很好的 GPU 运算体系。但是要强调的是：* 你的电脑里有合适的 GPU 显卡(NVIDIA)，且支持 CUDA 模块。<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-gpus">请在NVIDIA官网查询</a> * 必须安装 GPU 版的 Torch，<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/install">点击这里查看如何安装</a></p>
<h4 id="用-GPU-训练-CNN"><a href="#用-GPU-训练-CNN" class="headerlink" title="用 GPU 训练 CNN"></a>用 GPU 训练 CNN</h4><p>这份 GPU 的代码是依据<a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py">之前这份CNN</a>的代码修改的。大概修改的地方包括将数据的形式变成 GPU 能读的形式，然后将 CNN 也变成 GPU 能读的形式。做法就是在后面加上 <code>.cuda()</code>，很简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./mnist/&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!! 修改 test data 形式 !!!!!!!!! #</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>].cuda()/<span class="number">255.</span>   <span class="comment"># Tensor on GPU</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>].cuda()</span><br></pre></td></tr></table></figure>

<p>再来把我们的 CNN 参数也变成 GPU 兼容形式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!! 转换 cnn 去 CUDA !!!!!!!!! #</span></span><br><span class="line">cnn.cuda()      <span class="comment"># Moves all model parameters and buffers to the GPU.</span></span><br></pre></td></tr></table></figure>

<p>然后就是在 train 的时候，将每次的training data 变成 GPU 形式 + <code>.cuda()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch ..:</span><br><span class="line">    <span class="keyword">for</span> step, ...:</span><br><span class="line">        <span class="comment"># !!!!!!!! 这里有修改 !!!!!!!!! #</span></span><br><span class="line">        b_x = x.cuda()    <span class="comment"># Tensor on GPU</span></span><br><span class="line">        b_y = y.cuda()    <span class="comment"># Tensor on GPU</span></span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            test_output = cnn(test_x)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># !!!!!!!! 这里有修改  !!!!!!!!! #</span></span><br><span class="line">            pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].cuda().data.squeeze()  <span class="comment"># 将操作放去 GPU</span></span><br><span class="line"></span><br><span class="line">            accuracy = torch.<span class="built_in">sum</span>(pred_y == test_y) / test_y.size(<span class="number">0</span>)</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">test_output = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!! 这里有修改 !!!!!!!!! #</span></span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].cuda().data.squeeze()  <span class="comment"># 将操作放去 GPU</span></span><br><span class="line">...</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">10</span>], <span class="string">&#x27;real number&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>大功告成~</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py">github 代码</a> 中的每一步的意义啦。</p>
<h4 id="转移至-CPU"><a href="#转移至-CPU" class="headerlink" title="转移至 CPU"></a>转移至 CPU</h4><p>如果你有些计算还是需要在 CPU 上进行的话呢，比如 <code>plt</code> 的可视化，我们需要将这些计算或者数据转移至 CPU。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpu_data = gpu_data.cpu()</span><br></pre></td></tr></table></figure>



<h3 id="什么是过拟合-Overfitting"><a href="#什么是过拟合-Overfitting" class="headerlink" title="什么是过拟合 (Overfitting)"></a>什么是过拟合 (Overfitting)</h3><p>今天我们会来聊聊机器学习中的过拟合 overfitting 现象，和解决过拟合的方法。</p>
<h4 id="过于自负"><a href="#过于自负" class="headerlink" title="过于自负"></a>过于自负</h4><p><img src="https://static.mofanpy.com/results/ML-intro/overfitting1.png"></p>
<p>在细说之前，我们先用实际生活中的一个例子来比喻一下过拟合现象。说白了，就是机器学习模型于自信。已经到了自负的阶段了。那自负的坏处，大家也知道，就是在自己的小圈子里表现非凡，不过在现实的大圈子里却往往处处碰壁。所以在这个简介里，我们把自负和过拟合画上等号。</p>
<h4 id="回归分类的过拟合"><a href="#回归分类的过拟合" class="headerlink" title="回归分类的过拟合"></a>回归分类的过拟合</h4><p><img src="https://static.mofanpy.com/results/ML-intro/overfitting2.png"></p>
<p>机器学习模型的自负又表现在哪些方面呢。这里是一些数据。如果要你画一条线来描述这些数据，大多数人都会这么画。对，这条线也是我们希望机器也能学出来的一条用来总结这些数据的线。这时蓝线与数据的总误差可能是10。可是有时候，机器过于纠结这误差值，他想把误差减到更小，来完成他对这一批数据的学习使命。所以，他学到的可能会变成这样。它几乎经过了每一个数据点，这样，误差值会更小。可是误差越小就真的好吗？看来我们的模型还是太天真了。当我拿这个模型运用在现实中的时候，他的自负就体现出来。小二，来一打现实数据。这时，之前误差大的蓝线误差基本保持不变。误差小的红线误差值突然飙高，自负的红线再也骄傲不起来，因为他不能成功的表达除了训练数据以外的其他数据。这就叫做过拟合。Overfitting。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/overfitting3.png"></p>
<p>那么在分类问题当中。过拟合的分割线可能是这样，小二，再上一打数据。我们明显看出，有两个黄色的数据并没有被很好的分隔开来。这也是过拟合在作怪。好了，既然我们时不时会遇到过拟合问题，那解决的方法有那些呢。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/overfitting4.png"></p>
<p>方法一：增加数据量，大部分过拟合产生的原因是因为数据量太少了。如果我们有成千上万的数据，红线也会慢慢被拉直，变得没那么扭曲。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/overfitting5.png"></p>
<p>方法二：运用正规化。L1, l2 regularization等等，这些方法适用于大多数的机器学习，包括神经网络。他们的做法大同小异，我们简化机器学习的关键公式为 y=Wx。W为机器需要学习到的各种参数。在过拟合中，W 的值往往变化得特别大或特别小。为了不让W变化太大，我们在计算误差上做些手脚。原始的 cost 误差是这样计算，cost = 预测值-真实值的平方。如果 W 变得太大，我们就让 cost 也跟着变大，变成一种惩罚机制。所以我们把 W 自己考虑进来。这里 abs 是绝对值。这一种形式的正规化，叫做 l1 正规化。L2 正规化和 l1 类似，只是绝对值换成了平方。其他的l3，l4 也都是换成了立方和4次方等等。形式类似。用这些方法，我们就能保证让学出来的线条不会过于扭曲。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/overfitting6.png"></p>
<p>还有一种专门用在神经网络的正规化的方法，叫作 dropout。在训练的时候，我们随机忽略掉一些神经元和神经联结，是这个神经网络变得”不完整”。用一个不完整的神经网络训练一次。</p>
<p>到第二次再随机忽略另一些，变成另一个不完整的神经网络。有了这些随机 drop 掉的规则，我们可以想象其实每次训练的时候，我们都让每一次预测结果都不会依赖于其中某部分特定的神经元。像l1，l2正规化一样，过度依赖的 W，也就是训练参数的数值会很大，l1，l2会惩罚这些大的参数。Dropout 的做法是从根本上让神经网络没机会过度依赖。</p>
<h3 id="Dropout-缓解过拟合"><a href="#Dropout-缓解过拟合" class="headerlink" title="Dropout 缓解过拟合"></a>Dropout 缓解过拟合</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>过拟合让人头疼，明明训练时误差已经降得足够低，可是测试的时候误差突然飙升。这很有可能就是出现了过拟合现象。强烈推荐通过<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-overfitting">这个动画</a>的形式短时间了解什么是过拟合，怎么解决过拟合。下面动图就显示了我们成功缓解了过拟合现象。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-3-1.gif"></p>
<h4 id="做点数据"><a href="#做点数据" class="headerlink" title="做点数据"></a>做点数据</h4><p>自己做一些伪数据，用来模拟真实情况。数据少，才能凸显过拟合问题，所以我们就做10个数据点。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-3-2.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">N_SAMPLES = <span class="number">20</span></span><br><span class="line">N_HIDDEN = <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training data</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, N_SAMPLES), <span class="number">1</span>)</span><br><span class="line">y = x + <span class="number">0.3</span>*torch.normal(torch.zeros(N_SAMPLES, <span class="number">1</span>), torch.ones(N_SAMPLES, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># test data</span></span><br><span class="line">test_x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, N_SAMPLES), <span class="number">1</span>)</span><br><span class="line">test_y = test_x + <span class="number">0.3</span>*torch.normal(torch.zeros(N_SAMPLES, <span class="number">1</span>), torch.ones(N_SAMPLES, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># show data</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy(), c=<span class="string">&#x27;magenta&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c=<span class="string">&#x27;cyan&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2.5</span>, <span class="number">2.5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h4><p>我们在这里搭建两个神经网络，一个没有 <code>dropout</code>，一个有 <code>dropout</code>。没有 <code>dropout</code> 的容易出现过拟合，那我们就命名为 <code>net_overfitting</code>，另一个就是 <code>net_dropped</code>。<code>torch.nn.Dropout(0.5)</code> 这里的 0.5 指的是随机有 50% 的神经元会被关闭/丢弃。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">net_overfitting = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, N_HIDDEN),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, <span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">net_dropped = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, N_HIDDEN),</span><br><span class="line">    torch.nn.Dropout(<span class="number">0.5</span>),  <span class="comment"># drop 50% of the neuron</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</span><br><span class="line">    torch.nn.Dropout(<span class="number">0.5</span>),  <span class="comment"># drop 50% of the neuron</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练的时候，这两个神经网络分开训练。训练的环境都一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">optimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">optimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    pred_ofit = net_overfitting(x)</span><br><span class="line">    pred_drop = net_dropped(x)</span><br><span class="line"></span><br><span class="line">    loss_ofit = loss_func(pred_ofit, y)</span><br><span class="line">    loss_drop = loss_func(pred_drop, y)</span><br><span class="line"></span><br><span class="line">    optimizer_ofit.zero_grad()</span><br><span class="line">    optimizer_drop.zero_grad()</span><br><span class="line">    loss_ofit.backward()</span><br><span class="line">    loss_drop.backward()</span><br><span class="line">    optimizer_ofit.step()</span><br><span class="line">    optimizer_drop.step()</span><br></pre></td></tr></table></figure>



<h4 id="对比测试结果"><a href="#对比测试结果" class="headerlink" title="对比测试结果"></a>对比测试结果</h4><p>在这个 <code>for</code> 循环里，我们加上画测试图的部分。注意在测试时，要将网络改成 <code>eval()</code> 形式，特别是 <code>net_dropped</code>，<code>net_overfitting</code> 改不改其实无所谓。画好图再改回 <code>train()</code> 模式。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-3-1.gif"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">    optimizer_ofit.step()</span><br><span class="line">    optimizer_drop.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">10</span> == <span class="number">0</span>:     <span class="comment"># 每 10 步画一次图</span></span><br><span class="line">        <span class="comment"># 将神经网络转换成测试形式, 画好图之后改回 训练形式</span></span><br><span class="line">        net_overfitting.<span class="built_in">eval</span>()</span><br><span class="line">        net_dropped.<span class="built_in">eval</span>()  <span class="comment"># 因为 drop 网络在 train 的时候和 test 的时候参数不一样.</span></span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        test_pred_ofit = net_overfitting(test_x)</span><br><span class="line">        test_pred_drop = net_dropped(test_x)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将两个网络改回 训练形式</span></span><br><span class="line">        net_overfitting.train()</span><br><span class="line">        net_dropped.train()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/5-3-3.png"></p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="什么是批标准化-Batch-Normalization"><a href="#什么是批标准化-Batch-Normalization" class="headerlink" title="什么是批标准化 (Batch Normalization)"></a>什么是批标准化 (Batch Normalization)</h3><p>今天我们会来聊聊批标准化 Batch Normalization。</p>
<h4 id="普通数据标准化"><a href="#普通数据标准化" class="headerlink" title="普通数据标准化"></a>普通数据标准化</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/NB1.png"></p>
<p>Batch Normalization，批标准化，和普通的数据标准化类似，是将分散的数据统一的一种做法，也是优化神经网络的一种方法。在之前 Normalization 的简介视频中我们一提到，具有统一规格的数据，能让机器学习更容易学习到数据之中的规律。</p>
<h4 id="每层都做标准化"><a href="#每层都做标准化" class="headerlink" title="每层都做标准化"></a>每层都做标准化</h4><p><img src="https://static.mofanpy.com/results/ML-intro/NB2.png"></p>
<p>在神经网络中，数据分布对训练会产生影响。比如某个神经元 x 的值为1，某个 Weights 的初始值为 0.1，这样后一层神经元计算结果就是 Wx = 0.1；又或者 x = 20，这样 Wx 的结果就为 2。现在还不能看出什么问题，但是，当我们加上一层激励函数，激活这个 Wx 值的时候，问题就来了。如果使用 像 tanh 的激励函数，Wx 的激活值就变成了 ~0.1 和 ~1，接近于 1 的部已经处在了 激励函数的饱和阶段，也就是如果 x 无论再怎么扩大，tanh 激励函数输出值也还是接近1。换句话说，神经网络在初始阶段已经不对那些比较大的 x 特征范围敏感了。这样很糟糕，想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别，这就证明我的感官系统失效了。当然我们是可以用之前提到的对数据做 normalization 预处理，使得输入的 x 变化范围不会太大，让输入值经过激励函数的敏感部分。但刚刚这个不敏感问题不仅仅发生在神经网络的输入层，而且在隐藏层中也经常会发生。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/NB3.png"></p>
<p>只是时候 x 换到了隐藏层当中，我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢？答案是可以的，因为大牛们发明了一种技术，叫做 batch normalization，正是处理这种情况。</p>
<h4 id="BN-添加位置"><a href="#BN-添加位置" class="headerlink" title="BN 添加位置"></a>BN 添加位置</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/NB4.png"></p>
<p>Batch normalization 的 batch 是批数据，把数据分成小批小批进行 stochastic gradient descent。而且在每批数据进行前向传递 forward propagation 的时候，对每一层都进行 normalization 的处理。</p>
<h4 id="BN-效果"><a href="#BN-效果" class="headerlink" title="BN 效果"></a>BN 效果</h4><p>Batch normalization 也可以被看做一个层面。在一层层的添加神经网络的时候，我们先有数据 X，再添加全连接层，全连接层的计算结果会经过激励函数成为下一层的输入，接着重复之前的操作。Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/NB5.png"></p>
<p>之前说过，计算结果在进入激励函数前的值很重要，如果我们不单单看一个值，我们可以说，计算结果值的分布对于激励函数很重要。对于数据值大多分布在这个区间的数据，才能进行更有效的传递。对比这两个在激活之前的值的分布。上者没有进行 normalization，下者进行了 normalization，这样当然是下者能够更有效地利用 tanh 进行非线性化的过程。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/NB6.png"></p>
<p>没有 normalize 的数据使用 tanh 激活以后，激活值大部分都分布到了饱和阶段，也就是大部分的激活值不是-1，就是1，而 normalize 以后，大部分的激活值在每个分布区间都还有存在。再将这个激活后的分布传递到下一层神经网络进行后续计算，每个区间都有分布的这一种对于神经网络就会更加有价值。Batch normalization 不仅仅 normalize 了一下数据，他还进行了反 normalize 的手续。为什么要这样呢？</p>
<h4 id="BN-算法"><a href="#BN-算法" class="headerlink" title="BN 算法"></a>BN 算法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/NB7.png"></p>
<p>我们引入一些 batch normalization 的公式。这三步就是我们在刚刚一直说的 normalization 工序，但是公式的后面还有一个反向操作，将 normalize 后的数据再扩展和平移。原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma，和平移参数 β，这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用，如果没有起到作用，我就使用 gamma 和 belt 来抵消一些 normalization 的操作。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/NB8.png"></p>
<p>最后我们来看看一张神经网络训练到最后，代表了每层输出值的结果的分布图。这样我们就能一眼看出 Batch normalization 的功效啦。让每一层的值在有效的范围内传递下去。</p>
<h3 id="Batch-Normalization-批标准化"><a href="#Batch-Normalization-批标准化" class="headerlink" title="Batch Normalization 批标准化"></a>Batch Normalization 批标准化</h3><h4 id="要点-3"><a href="#要点-3" class="headerlink" title="要点"></a>要点</h4><p>批标准化通俗来说就是对每一层神经网络进行标准化 (normalize) 处理，我们知道对输入数据进行标准化能让机器学习有效率地学习。如果把每一层后看成这种接受输入数据的模式，那我们何不 批标准化所有的层呢？具体而且清楚的解释请看到 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-batch-normalization">我制作的 什么批标准化 动画简介(推荐)</a>。</p>
<p>那我们就看看下面的两个动图，这就是在每层神经网络有无 batch normalization 的区别啦。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-1.gif"></p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-2.gif"></p>
<h4 id="做点数据-1"><a href="#做点数据-1" class="headerlink" title="做点数据"></a>做点数据</h4><p>自己做一些伪数据，用来模拟真实情况。而且 Batch Normalization (之后都简称BN) 还能有效的控制坏的参数初始化 (initialization)，比如说 <code>ReLU</code> 这种激励函数最怕所有的值都落在附属区间，那我们就将所有的参数都水平移动一个 -0.2 (<code>bias_initialization = -0.2</code>)，来看看 BN 的实力。</p>
<p><img src="https://static.mofanpy.com/results-small/torch/5-4-3.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">N_SAMPLES = <span class="number">2000</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line">LR = <span class="number">0.03</span></span><br><span class="line">N_HIDDEN = <span class="number">8</span></span><br><span class="line">ACTIVATION = F.tanh     <span class="comment"># 你可以换 relu 试试</span></span><br><span class="line">B_INIT = -<span class="number">0.2</span>   <span class="comment"># 模拟不好的 参数初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training data</span></span><br><span class="line">x = np.linspace(-<span class="number">7</span>, <span class="number">10</span>, N_SAMPLES)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">2</span>, x.shape)</span><br><span class="line">y = np.square(x) - <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="comment"># test data</span></span><br><span class="line">test_x = np.linspace(-<span class="number">7</span>, <span class="number">10</span>, <span class="number">200</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">2</span>, test_x.shape)</span><br><span class="line">test_y = np.square(test_x) - <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line">train_x, train_y = torch.from_numpy(x).<span class="built_in">float</span>(), torch.from_numpy(y).<span class="built_in">float</span>()</span><br><span class="line">test_x = torch.from_numpy(test_x).<span class="built_in">float</span>()</span><br><span class="line">test_y = torch.from_numpy(test_y).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">train_dataset = Data.TensorDataset(train_x, train_y)</span><br><span class="line">train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show data</span></span><br><span class="line">plt.scatter(train_x.numpy(), train_y.numpy(), c=<span class="string">&#x27;#FF9359&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.2</span>, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="搭建神经网络-1"><a href="#搭建神经网络-1" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h4><p>这里就教你如何构建带有 BN 的神经网络的。BN 其实可以看做是一个 layer (<code>BN layer</code>)。我们就像平时加层一样加 <code>BN layer</code> 就好了。注意，我还对输入数据进行了一个 BN 处理，因为如果你把输入数据看出是从前面一层来的输出数据，我们同样也能对它进行 BN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_normalization=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.do_bn = batch_normalization</span><br><span class="line">        self.fcs = []   <span class="comment"># 太多层了, 我们用 for loop 建立</span></span><br><span class="line">        self.bns = []</span><br><span class="line">        self.bn_input = nn.BatchNorm1d(<span class="number">1</span>, momentum=<span class="number">0.5</span>)   <span class="comment"># 给 input 的 BN</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_HIDDEN):               <span class="comment"># 建层</span></span><br><span class="line">            input_size = <span class="number">1</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="number">10</span></span><br><span class="line">            fc = nn.Linear(input_size, <span class="number">10</span>)</span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">&#x27;fc%i&#x27;</span> % i, fc)       <span class="comment"># 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug</span></span><br><span class="line">            self._set_init(fc)                  <span class="comment"># 参数初始化</span></span><br><span class="line">            self.fcs.append(fc)</span><br><span class="line">            <span class="keyword">if</span> self.do_bn:</span><br><span class="line">                bn = nn.BatchNorm1d(<span class="number">10</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">                <span class="built_in">setattr</span>(self, <span class="string">&#x27;bn%i&#x27;</span> % i, bn)   <span class="comment"># 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug</span></span><br><span class="line">                self.bns.append(bn)</span><br><span class="line"></span><br><span class="line">        self.predict = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)         <span class="comment"># output layer</span></span><br><span class="line">        self._set_init(self.predict)            <span class="comment"># 参数初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_set_init</span>(<span class="params">self, layer</span>):</span>     <span class="comment"># 参数初始化</span></span><br><span class="line">        init.normal_(layer.weight, mean=<span class="number">0.</span>, std=<span class="number">.1</span>)</span><br><span class="line">        init.constant_(layer.bias, B_INIT)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        pre_activation = [x]</span><br><span class="line">        <span class="keyword">if</span> self.do_bn: x = self.bn_input(x)    <span class="comment"># 判断是否要加 BN</span></span><br><span class="line">        layer_input = [x]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_HIDDEN):</span><br><span class="line">            x = self.fcs[i](x)</span><br><span class="line">            pre_activation.append(x)    <span class="comment"># 为之后出图</span></span><br><span class="line">            <span class="keyword">if</span> self.do_bn: x = self.bns[i](x)  <span class="comment"># 判断是否要加 BN</span></span><br><span class="line">            x = ACTIVATION(x)</span><br><span class="line">            layer_input.append(x)       <span class="comment"># 为之后出图</span></span><br><span class="line">        out = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> out, layer_input, pre_activation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立两个 net, 一个有 BN, 一个没有</span></span><br><span class="line">nets = [Net(batch_normalization=<span class="literal">False</span>), Net(batch_normalization=<span class="literal">True</span>)]</span><br></pre></td></tr></table></figure>



<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>训练的时候，这两个神经网络分开训练。训练的环境都一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">opts = [torch.optim.Adam(net.parameters(), lr=LR) <span class="keyword">for</span> net <span class="keyword">in</span> nets]</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">losses = [[], []]  <span class="comment"># 每个网络一个 list 来记录误差</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="keyword">for</span> net, opt <span class="keyword">in</span> <span class="built_in">zip</span>(nets, opts):     <span class="comment"># 训练两个网络</span></span><br><span class="line">            pred, _, _ = net(b_x)</span><br><span class="line">            loss = loss_func(pred, b_y)</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()    <span class="comment"># 这也会训练 BN 里面的参数</span></span><br></pre></td></tr></table></figure>



<h4 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h4><p>这个教程有几张图要画，首先我们画训练时的动态图。我单独定义了一个画动图的功能 <code>plot_histogram()</code>，因为不是重点，所以代码的具体细节请看我的 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">github</a>。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-2.gif"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">f, axs = plt.subplots(<span class="number">4</span>, N_HIDDEN+<span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_histogram</span>(<span class="params">l_in, l_in_bn, pre_ac, pre_ac_bn</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    layer_inputs, pre_acts = [], []</span><br><span class="line">    <span class="keyword">for</span> net, l <span class="keyword">in</span> <span class="built_in">zip</span>(nets, losses):</span><br><span class="line">        <span class="comment"># 一定要把 net 的设置成 eval 模式, eval下的 BN 参数会被固定</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        pred, layer_input, pre_act = net(test_x)</span><br><span class="line">        l.append(loss_func(pred, test_y).data[<span class="number">0</span>])</span><br><span class="line">        layer_inputs.append(layer_input)</span><br><span class="line">        pre_acts.append(pre_act)</span><br><span class="line">        <span class="comment"># 收集好信息后将 net 设置成 train 模式, 继续训练</span></span><br><span class="line">        net.train()</span><br><span class="line">    plot_histogram(*layer_inputs, *pre_acts)     <span class="comment"># plot histogram</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 后面接着之前 for loop 中的代码来</span></span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>后面还有两张图，一张是预测曲线，一张是误差变化曲线，具体代码不在这里呈现，想知道如何画图的朋友，请参考我的 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">github</a></p>
<h4 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h4><p>首先来看看这次对比的两个激励函数是长什么样：</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-8.png"></p>
<p>然后我们来对比使用不同激励函数的结果。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-1.png"></p>
<p><img src="https://static.mofanpy.com/results-small/torch/5-4-4.png"></p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-5.png"></p>
<p>上面是使用 <code>relu</code> 激励函数的结果，我们可以看到，没有使用 BN 的误差要高，线条不能拟合数据，原因是我们有一个 Bad initialization，初始 <code>bias = -0.2</code>，这一招，让 <code>relu</code> 无法捕捉到在负数区间的输入值。而有了 BN，这就不成问题了。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-2.png"></p>
<p><img src="https://static.mofanpy.com/results-small/torch/5-4-6.png"></p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-7.png"></p>
<p>上面结果是使用 <code>tanh</code> 作为激励函数的结果，可以看出，不好的初始化，让输入数据在激活前分散得非常离散，而有了 BN，数据都被收拢了。收拢的数据再放入激励函数就能很好地利用激励函数的非线性。而且可以看出没有 BN 的数据让激活后的结果都分布在 <code>tanh</code> 的两端，而这两端的梯度又非常的小，是的后面的误差都不能往前传，导致神经网络死掉了。</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">github 代码</a> 中的每一步的意义啦。</p>
<blockquote>
<p><strong>原文地址：</strong><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/501_why_torch_dynamic_graph.py">第一节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py">第二节的全部代码</a></li>
</ul>
<hr>
<ul>
<li>Tensorflow: dropout <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/dropout">教程</a></li>
<li>PyTorch: dropout <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/dropout">教程</a></li>
<li>Theano: l1 l2 regularization <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/theano/regularization">教程</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py">第四节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/501_dropout.py">Tensorflow 的 50行 Dropout 代码</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-overfitting">我制作的 什么是过拟合 动画简介</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/ML-intro/l1l2regularization">我制作的 L1/L2 正规化 动画简介</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/BN">Tensorflow 使用 Batch normalization</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/batch-normalization">PyTorch 使用 Batch normalization</a></li>
<li>论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>
<hr>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">第六节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/502_batch_normalization.py">Tensorflow 的70行 批标准化代码</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-batch-normalization">我制作的 什么批标准化 动画简介</a></li>
</ul>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">银河</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yingyingmonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/">https://yingyingmonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://YingYingMonstre.github.io" target="_blank">银河之家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/YingYingMonstre.github.io/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/YingYingMonstre.github.io/2021/12/24/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%90%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AFLLVM/"><img class="prev-cover" src="https://img1.baidu.com/it/u=3391566484,4223344013&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/YingYingMonstre.github.io/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深入浅出理解什么是LLVM</div></div></a></div><div class="next-post pull-right"><a href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/"><img class="next-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="onerror=null;src='/YingYingMonstre.github.io/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">高级神经网络结构（下）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/YingYingMonstre.github.io/2021/12/07/PyTorch神经网络基础/" title="PyTorch神经网络基础"><img class="cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-07</div><div class="title">PyTorch神经网络基础</div></div></a></div><div><a href="/YingYingMonstre.github.io/2021/12/06/PyTorch简介/" title="PyTorch简介"><img class="cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-06</div><div class="title">PyTorch简介</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/YingYingMonstre.github.io/img/avatar.jpg" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">银河</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/archives/"><div class="headline">文章</div><div class="length-num">16</div></a></div><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YingYingMonstre"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-Torch-%E6%98%AF%E5%8A%A8%E6%80%81%E7%9A%84"><span class="toc-number">1.</span> <span class="toc-text">为什么 Torch 是动态的</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%80%81-%E9%9D%99%E6%80%81"><span class="toc-number">1.2.</span> <span class="toc-text">动态?静态?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%80%81RNN"><span class="toc-number">1.3.</span> <span class="toc-text">动态RNN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPU-%E5%8A%A0%E9%80%9F%E8%BF%90%E7%AE%97"><span class="toc-number">2.</span> <span class="toc-text">GPU 加速运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-1"><span class="toc-number">2.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8-GPU-%E8%AE%AD%E7%BB%83-CNN"><span class="toc-number">2.2.</span> <span class="toc-text">用 GPU 训练 CNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AC%E7%A7%BB%E8%87%B3-CPU"><span class="toc-number">2.3.</span> <span class="toc-text">转移至 CPU</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%87%E6%8B%9F%E5%90%88-Overfitting"><span class="toc-number">3.</span> <span class="toc-text">什么是过拟合 (Overfitting)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%87%E4%BA%8E%E8%87%AA%E8%B4%9F"><span class="toc-number">3.1.</span> <span class="toc-text">过于自负</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E7%9A%84%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">3.2.</span> <span class="toc-text">回归分类的过拟合</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">解决方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout-%E7%BC%93%E8%A7%A3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.</span> <span class="toc-text">Dropout 缓解过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-2"><span class="toc-number">4.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%9A%E7%82%B9%E6%95%B0%E6%8D%AE"><span class="toc-number">4.2.</span> <span class="toc-text">做点数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.</span> <span class="toc-text">搭建神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">4.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="toc-number">4.5.</span> <span class="toc-text">对比测试结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96-Batch-Normalization"><span class="toc-number">5.</span> <span class="toc-text">什么是批标准化 (Batch Normalization)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%99%AE%E9%80%9A%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">5.1.</span> <span class="toc-text">普通数据标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%8F%E5%B1%82%E9%83%BD%E5%81%9A%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">5.2.</span> <span class="toc-text">每层都做标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN-%E6%B7%BB%E5%8A%A0%E4%BD%8D%E7%BD%AE"><span class="toc-number">5.3.</span> <span class="toc-text">BN 添加位置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN-%E6%95%88%E6%9E%9C"><span class="toc-number">5.4.</span> <span class="toc-text">BN 效果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BN-%E7%AE%97%E6%B3%95"><span class="toc-number">5.5.</span> <span class="toc-text">BN 算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization-%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">Batch Normalization 批标准化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-3"><span class="toc-number">6.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%9A%E7%82%B9%E6%95%B0%E6%8D%AE-1"><span class="toc-number">6.2.</span> <span class="toc-text">做点数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="toc-number">6.3.</span> <span class="toc-text">搭建神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">6.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%BB%E5%9B%BE"><span class="toc-number">6.5.</span> <span class="toc-text">画图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E7%BB%93%E6%9E%9C"><span class="toc-number">6.6.</span> <span class="toc-text">对比结果</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/24/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%90%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AFLLVM/" title="深入浅出理解什么是LLVM"><img src="https://img1.baidu.com/it/u=3391566484,4223344013&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="深入浅出理解什么是LLVM"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/24/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E7%90%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AFLLVM/" title="深入浅出理解什么是LLVM">深入浅出理解什么是LLVM</a><time datetime="2021-12-24T04:00:00.000Z" title="发表于 2021-12-24 12:00:00">2021-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/" title="PyTorch高阶内容"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="PyTorch高阶内容"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/" title="PyTorch高阶内容">PyTorch高阶内容</a><time datetime="2021-12-12T12:40:00.000Z" title="发表于 2021-12-12 20:40:00">2021-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/" title="高级神经网络结构（下）"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="高级神经网络结构（下）"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/" title="高级神经网络结构（下）">高级神经网络结构（下）</a><time datetime="2021-12-11T16:00:00.000Z" title="发表于 2021-12-12 00:00:00">2021-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/" title="高级神经网络结构（上）"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="高级神经网络结构（上）"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/" title="高级神经网络结构（上）">高级神经网络结构（上）</a><time datetime="2021-12-11T14:00:00.000Z" title="发表于 2021-12-11 22:00:00">2021-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="建造第一个神经网络"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="建造第一个神经网络"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="建造第一个神经网络">建造第一个神经网络</a><time datetime="2021-12-11T11:30:00.000Z" title="发表于 2021-12-11 19:30:00">2021-12-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 银河</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/YingYingMonstre.github.io/js/utils.js"></script><script src="/YingYingMonstre.github.io/js/main.js"></script><script src="/YingYingMonstre.github.io/js/search/local-search.js"></script><div class="js-pjax"></div><script src="js/custom.js"></script><script src="../js/custom.js"></script><script src="../../js/custom.js"></script><script src="../../../../js/custom.js"></script></div></body></html>