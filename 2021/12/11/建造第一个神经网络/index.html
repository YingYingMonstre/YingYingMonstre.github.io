<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>建造第一个神经网络 | 银河之家</title><meta name="keywords" content="PyTorch 神经网络"><meta name="author" content="银河"><meta name="copyright" content="银河"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="关系拟合 (回归)要点我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示。或者说，是如何在数据当中找到他们的关系，然后用神经网络模型来建立一个可以代表他们关系的线条。  建立数据集我们创建一些假数据来模拟真实的情况。比如一个一元二次函数: y &#x3D; a * x^2 + b，我们给 y 数据加上一点噪声来更加真实的展示它。 123456789import torchimport m">
<meta property="og:type" content="article">
<meta property="og:title" content="建造第一个神经网络">
<meta property="og:url" content="https://yingyingmonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="银河之家">
<meta property="og:description" content="关系拟合 (回归)要点我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示。或者说，是如何在数据当中找到他们的关系，然后用神经网络模型来建立一个可以代表他们关系的线条。  建立数据集我们创建一些假数据来模拟真实的情况。比如一个一元二次函数: y &#x3D; a * x^2 + b，我们给 y 数据加上一点噪声来更加真实的展示它。 123456789import torchimport m">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969">
<meta property="article:published_time" content="2021-12-11T11:30:00.000Z">
<meta property="article:modified_time" content="2021-12-11T14:05:41.731Z">
<meta property="article:author" content="银河">
<meta property="article:tag" content="PyTorch 神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969"><link rel="shortcut icon" href="/YingYingMonstre.github.io/img/favicon.png"><link rel="canonical" href="https://yingyingmonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/YingYingMonstre.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/YingYingMonstre.github.io/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-11 22:05:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><link rel="stylesheet" href="css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="../../../../css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/YingYingMonstre.github.io/atom.xml" title="银河之家" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/YingYingMonstre.github.io/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/archives/"><div class="headline">文章</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/YingYingMonstre.github.io/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/YingYingMonstre.github.io/">银河之家</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/YingYingMonstre.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">建造第一个神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-11T11:30:00.000Z" title="发表于 2021-12-11 19:30:00">2021-12-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-11T14:05:41.731Z" title="更新于 2021-12-11 22:05:41">2021-12-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/YingYingMonstre.github.io/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="关系拟合-回归"><a href="#关系拟合-回归" class="headerlink" title="关系拟合 (回归)"></a>关系拟合 (回归)</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示。或者说，是如何在数据当中找到他们的关系，然后用神经网络模型来建立一个可以代表他们关系的线条。</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-2.gif"></p>
<h4 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h4><p>我们创建一些假数据来模拟真实的情况。比如一个一元二次函数: <code>y = a * x^2 + b</code>，我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())                 <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h4><p>建立一个神经网络我们可以直接运用 torch 中的体系。先定义所有的层属性(<code>__init__()</code>)，然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接。建立关系的时候，我们会用到激励函数，如果还不清楚激励函数用途的同学，这里有非常好的<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">一篇动画教程</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练的步骤很简单，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss()      <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>



<h4 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h4><p>为了可视化整个训练的过程，更好的理解是如何训练，我们如下操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/3-1-1.png"></p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="区分类型-分类"><a href="#区分类型-分类" class="headerlink" title="区分类型 (分类)"></a>区分类型 (分类)</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>这次我们也是用最简单的途径来看看神经网络是怎么进行事物的分类。</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-3.gif"></p>
<h4 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h4><p>我们创建一些假数据来模拟真实的情况。比如两个二次分布的数据，不过他们的均值都不一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)         <span class="comment"># 数据的基本形态</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># 类型0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># 类型0 y data (tensor), shape=(100, )</span></span><br><span class="line">x1 = torch.normal(-<span class="number">2</span>*n_data, <span class="number">1</span>)     <span class="comment"># 类型1 x data (tensor), shape=(100, 1)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># 类型1 y data (tensor), shape=(100, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).<span class="built_in">type</span>(torch.FloatTensor)  <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).<span class="built_in">type</span>(torch.LongTensor)    <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap=&#x27;RdYlGn&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h4><p>建立一个神经网络我们可以直接运用 torch 中的体系。先定义所有的层属性(<code>__init__()</code>)，然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接。这个和我们在前面 regression 的时候的神经网络基本没差。建立关系的时候，我们会用到激励函数，如果还不清楚激励函数用途的同学，这里有非常好的<a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">一篇动画教程</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (2 -&gt; 10)</span></span><br><span class="line"><span class="string">  (out): Linear (10 -&gt; 2)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练的步骤很简单，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>



<h4 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h4><p>为了可视化整个训练的过程，更好的理解是如何训练，我们如下操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        <span class="comment"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></span><br><span class="line">        prediction = torch.<span class="built_in">max</span>(F.softmax(out), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">&#x27;RdYlGn&#x27;</span>)</span><br><span class="line">        accuracy = <span class="built_in">sum</span>(pred_y == target_y)/<span class="number">200.</span>  <span class="comment"># 预测中有多少和真实值一样</span></span><br><span class="line">        plt.text(<span class="number">1.5</span>, -<span class="number">4</span>, <span class="string">&#x27;Accuracy=%.2f&#x27;</span> % accuracy, fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">plt.ioff()  <span class="comment"># 停止画图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/3-2-1.png"></p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="快速搭建法"><a href="#快速搭建法" class="headerlink" title="快速搭建法"></a>快速搭建法</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>Torch 中提供了很多方便的途径，同样是神经网络，能快则快，我们看看如何用更简单的方式搭建同样的回归神经网络。</p>
<h4 id="快速搭建"><a href="#快速搭建" class="headerlink" title="快速搭建"></a>快速搭建</h4><p>我们先看看之前写神经网络时用到的步骤。我们用 <code>net1</code> 代表这种方式搭建的神经网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net1 = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)   <span class="comment"># 这是我们用这种方式搭建的 net1</span></span><br></pre></td></tr></table></figure>

<p>我们用 class 继承了一个 torch 中的神经网络结构，然后对其进行了修改，不过还有更快的一招，用一句话就概括了上面所有的内容！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net2 = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们再对比一下两者的结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net1)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(net2)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Sequential (</span></span><br><span class="line"><span class="string">  (0): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (1): ReLU ()</span></span><br><span class="line"><span class="string">  (2): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们会发现 <code>net2</code> 多显示了一些内容，这是为什么呢？原来他把激励函数也一同纳入进去了，但是 <code>net1</code> 中，激励函数实际上是在 <code>forward()</code> 功能中才被调用的。这也就说明了，相比 <code>net2</code>，<code>net1</code> 的好处就是，你可以根据你的个人需要更加个性化你自己的前向传播过程，比如(RNN)。不过如果你不需要七七八八的过程，相信 <code>net2</code> 这种形式更适合你。</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h3><h4 id="要点-3"><a href="#要点-3" class="headerlink" title="要点"></a>要点</h4><p>训练好了一个模型，我们当然想要保存它，留到下次要用的时候直接提取直接用，这就是这节的内容啦。我们用回归的神经网络举例实现保存提取。</p>
<h4 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h4><p>我们快速地建造数据，搭建网络：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())  <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span>():</span></span><br><span class="line">    <span class="comment"># 建网络</span></span><br><span class="line">    net1 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    optimizer = torch.optim.SGD(net1.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        prediction = net1(x)</span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>

<p>接下来我们有两种途径来保存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net1, <span class="string">&#x27;net.pkl&#x27;</span>)  <span class="comment"># 保存整个网络</span></span><br><span class="line">torch.save(net1.state_dict(), <span class="string">&#x27;net_params.pkl&#x27;</span>)   <span class="comment"># 只保存网络中的参数 (速度快, 占内存少)</span></span><br></pre></td></tr></table></figure>



<h4 id="提取网络"><a href="#提取网络" class="headerlink" title="提取网络"></a>提取网络</h4><p>这种方式将会提取整个神经网络，网络大的时候可能会比较慢。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span>():</span></span><br><span class="line">    <span class="comment"># restore entire net1 to net2</span></span><br><span class="line">    net2 = torch.load(<span class="string">&#x27;net.pkl&#x27;</span>)</span><br><span class="line">    prediction = net2(x)</span><br></pre></td></tr></table></figure>



<h4 id="只提取网络参数"><a href="#只提取网络参数" class="headerlink" title="只提取网络参数"></a>只提取网络参数</h4><p>这种方式将会提取所有的参数，然后再放到你的新建网络中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span>():</span></span><br><span class="line">    <span class="comment"># 新建 net3</span></span><br><span class="line">    net3 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将保存的参数复制到 net3</span></span><br><span class="line">    net3.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>))</span><br><span class="line">    prediction = net3(x)</span><br></pre></td></tr></table></figure>



<h4 id="显示结果"><a href="#显示结果" class="headerlink" title="显示结果"></a>显示结果</h4><p>调用上面建立的几个功能，然后出图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存 net1 (1. 整个网络, 2. 只有参数)</span></span><br><span class="line">save()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取整个网络</span></span><br><span class="line">restore_net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取网络参数, 复制到新网络</span></span><br><span class="line">restore_params()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/3-4-1.png"></p>
<p>这样我们就能看出三个网络完全一模一样啦。</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/304_save_reload.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h3><h4 id="要点-4"><a href="#要点-4" class="headerlink" title="要点"></a>要点</h4><p>Torch 中提供了一种帮你整理你的数据结构的好东西，叫做 <code>DataLoader</code>，我们能用它来包装自己的数据，进行批训练。而且批训练可以有很多种途径，详情请见 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-speed-up-learning">我制作的 训练优化器 动画简介</a>。</p>
<h4 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h4><p><code>DataLoader</code> 是 torch 给你用来包装你的数据的工具。所以你要将自己的 (numpy array 或其他) 数据形式转换成 Tensor，然后再放进这个包装器中。使用 <code>DataLoader</code> 有什么好处呢？就是他们帮你有效地迭代数据，举例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)       <span class="comment"># x data (torch tensor)</span></span><br><span class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)       <span class="comment"># y data (torch tensor)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先转换成 torch 能识别的 Dataset</span></span><br><span class="line">torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 dataset 放入 DataLoader</span></span><br><span class="line">loader = Data.DataLoader(</span><br><span class="line">    dataset=torch_dataset,      <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=BATCH_SIZE,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># 要不要打乱数据 (打乱比较好)</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># 多线程来读数据</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):   <span class="comment"># 训练所有!整套!数据 3 次</span></span><br><span class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):  <span class="comment"># 每一步 loader 释放一小批数据用来学习</span></span><br><span class="line">        <span class="comment"># 假设这里就是你训练的地方...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打出来一些数据</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| Step: &#x27;</span>, step, <span class="string">&#x27;| batch x: &#x27;</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">&#x27;| batch y: &#x27;</span>, batch_y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  0 | batch x:  [ 6.  7.  2.  3.  1.] | batch y:  [  5.   4.   9.   8.  10.]</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  1 | batch x:  [  9.  10.   4.   8.   5.] | batch y:  [ 2.  1.  7.  3.  6.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.] | batch y:  [ 8.  7.  9.  2.  1.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  1 | batch x:  [ 1.  7.  8.  5.  6.] | batch y:  [ 10.   4.   3.   6.   5.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  0 | batch x:  [ 3.  9.  2.  6.  7.] | batch y:  [ 8.  2.  9.  5.  4.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  1 | batch x:  [ 10.   4.   8.   1.   5.] | batch y:  [  1.   7.   3.  10.   6.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以看出，每步都导出了5个数据进行学习。然后每个 epoch 的导出数据都是先打乱了以后再导出。</p>
<p>真正方便的还不是这点。如果我们改变一下 <code>BATCH_SIZE = 8</code>，这样我们就知道，<code>step=0</code> 会导出8个数据，但是，<code>step=1</code> 时数据库中的数据不够 8个，这时怎么办呢：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">8</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ...:</span><br><span class="line">    <span class="keyword">for</span> ...:</span><br><span class="line">        ...</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| Step: &#x27;</span>, step, <span class="string">&#x27;| batch x: &#x27;</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">&#x27;| batch y: &#x27;</span>, batch_y.numpy())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  0 | batch x:  [  6.   7.   2.   3.   1.   9.  10.   4.] | batch y:  [  5.   4.   9.   8.  10.   2.   1.   7.]</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  1 | batch x:  [ 8.  5.] | batch y:  [ 3.  6.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.   1.   7.   8.] | batch y:  [  8.   7.   9.   2.   1.  10.   4.   3.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  1 | batch x:  [ 5.  6.] | batch y:  [ 6.  5.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  0 | batch x:  [  3.   9.   2.   6.   7.  10.   4.   8.] | batch y:  [ 8.  2.  9.  5.  4.  1.  7.  3.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  1 | batch x:  [ 1.  5.] | batch y:  [ 10.   6.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>这时，在 <code>step=1</code> 就只给你返回这个 epoch 中剩下的数据就好了。</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/305_batch_train.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="加速神经网络训练-Speed-Up-Training"><a href="#加速神经网络训练-Speed-Up-Training" class="headerlink" title="加速神经网络训练 (Speed Up Training)"></a>加速神经网络训练 (Speed Up Training)</h3><p>今天我们会来聊聊在怎么样加速你的神经网络训练过程。</p>
<p>包括以下几种模式:</p>
<ul>
<li>Stochastic Gradient Descent (SGD)</li>
<li>Momentum</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup1.png"></p>
<p>越复杂的神经网络、越多的数据，我们需要在训练神经网络的过程上花费的时间也就越多。原因很简单，就是因为计算量太大了。可是往往有时候为了解决复杂的问题，复杂的结构和大数据又是不能避免的，所以我们需要寻找一些方法，让神经网络聪明起来，快起来。</p>
<h4 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup2.png"></p>
<p>所以，最基础的方法就是 SGD 啦，想像红色方块是我们要训练的 data，如果用普通的训练方法，就需要重复不断的把整套数据放入神经网络 NN训练，这样消耗的计算资源会很大。</p>
<p>我们换一种思路，如果把这些数据拆分成小批小批的，然后再分批不断放入 NN 中计算，这就是我们常说的 SGD 的正确打开方式了。每次使用批数据，虽然不能反映整体数据的情况，不过却很大程度上加速了 NN 的训练过程，而且也不会丢失太多准确率。如果运用上了 SGD，你还是嫌训练速度慢，那怎么办？</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/speedup3.png"></p>
<p>没问题，事实证明，SGD 并不是最快速的训练方法，红色的线是 SGD，但它到达学习目标的时间是在这些方法中最长的一种。我们还有很多其他的途径来加速训练。</p>
<h4 id="Momentum-更新方法"><a href="#Momentum-更新方法" class="headerlink" title="Momentum 更新方法"></a>Momentum 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup4.png"></p>
<p>大多数其他途径是在更新神经网络参数那一步上动动手脚。传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx)。这种方法可能会让学习过程曲折无比，看起来像喝醉的人回家时, 摇摇晃晃走了很多弯路。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup5.png"></p>
<p>所以我们把这个人从平地上放到了一个斜坡上，只要他往下坡的方向走一点点，由于向下的惯性，他不自觉地就一直往下走，走的弯路也变少了。这就是 Momentum 参数更新。另外一种加速方法叫AdaGrad。</p>
<h4 id="AdaGrad-更新方法"><a href="#AdaGrad-更新方法" class="headerlink" title="AdaGrad 更新方法"></a>AdaGrad 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup6.png"></p>
<p>这种方法是在学习率上面动手脚，使得每一个参数更新都会有自己与众不同的学习率，他的作用和 momentum 类似，不过不是给喝醉酒的人安排另一个下坡，而是给他一双不好走路的鞋子，使得他一摇晃着走路就脚疼，鞋子成为了走弯路的阻力，逼着他往前直着走。他的数学形式是这样的。接下来又有什么方法呢？如果把下坡和不好走路的鞋子合并起来，是不是更好呢？没错，这样我们就有了 RMSProp 更新方法。</p>
<h4 id="RMSProp-更新方法"><a href="#RMSProp-更新方法" class="headerlink" title="RMSProp 更新方法"></a>RMSProp 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup7.png"></p>
<p>有了 momentum 的惯性原则，加上 adagrad 的对错误方向的阻力，我们就能合并成这样。让 RMSProp同时具备他们两种方法的优势。不过细心的同学们肯定看出来了，似乎在 RMSProp 中少了些什么。原来是我们还没把 Momentum合并完全，RMSProp 还缺少了 momentum 中的这一部分。所以，我们在 Adam 方法中补上了这种想法。</p>
<h4 id="Adam-更新方法"><a href="#Adam-更新方法" class="headerlink" title="Adam 更新方法"></a>Adam 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup8.png"></p>
<p>计算m 时有 momentum 下坡的属性，计算 v 时有 adagrad 阻力的属性，然后再更新参数时把 m 和 V 都考虑进去。实验证明，大多数时候，使用 adam 都能又快又好的达到目标，迅速收敛。所以说，在加速神经网络训练的时候，一个下坡，一双破鞋子，功不可没。</p>
<h3 id="Optimizer-优化器"><a href="#Optimizer-优化器" class="headerlink" title="Optimizer 优化器"></a>Optimizer 优化器</h3><h4 id="要点-5"><a href="#要点-5" class="headerlink" title="要点"></a>要点</h4><p>这节内容主要是用 Torch 实践 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-speed-up-learning">这个 优化器 动画简介</a> 中起到的几种优化器，这几种优化器具体的优势不会在这个节内容中说了，所以想快速了解的话，上面的那个动画链接是很好的去处。</p>
<p>下图就是这节内容对比各种优化器的效果：</p>
<p><img src="https://static.mofanpy.com/results-small/torch/3-6-2.png"></p>
<h4 id="伪数据"><a href="#伪数据" class="headerlink" title="伪数据"></a>伪数据</h4><p>为了对比各种优化器的效果，我们需要有一些数据，今天我们还是自己编一些伪数据，这批数据是这样的：</p>
<p><img src="https://static.mofanpy.com/results/torch/3-6-1.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fake dataset</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span>*torch.normal(torch.zeros(*x.size()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot dataset</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用上节内容提到的 data loader</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line">loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>,)</span><br></pre></td></tr></table></figure>



<h4 id="每个优化器优化一个神经网络"><a href="#每个优化器优化一个神经网络" class="headerlink" title="每个优化器优化一个神经网络"></a>每个优化器优化一个神经网络</h4><p>为了对比每一种优化器，我们给他们各自创建一个神经网络，但这个神经网络都来自同一个 <code>Net</code> 形式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认的 network 形式</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(<span class="number">1</span>, <span class="number">20</span>)   <span class="comment"># hidden layer</span></span><br><span class="line">        self.predict = torch.nn.Linear(<span class="number">20</span>, <span class="number">1</span>)   <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># activation function for hidden layer</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># linear output</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个优化器创建一个 net</span></span><br><span class="line">net_SGD         = Net()</span><br><span class="line">net_Momentum    = Net()</span><br><span class="line">net_RMSprop     = Net()</span><br><span class="line">net_Adam        = Net()</span><br><span class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</span><br></pre></td></tr></table></figure>



<h4 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器 Optimizer"></a>优化器 Optimizer</h4><p>接下来在创建不同的优化器，用来训练不同的网络。并创建一个 <code>loss_func</code> 用来计算误差。我们用几种常见的优化器，<code>SGD</code>，<code>Momentum</code>，<code>RMSprop</code>，<code>Adam</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># different optimizers</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">losses_his = [[], [], [], []]   <span class="comment"># 记录 training 时不同神经网络的 loss</span></span><br></pre></td></tr></table></figure>



<h4 id="训练-出图"><a href="#训练-出图" class="headerlink" title="训练/出图"></a>训练/出图</h4><p>接下来训练和 loss 画图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对每个优化器, 优化属于他的神经网络</span></span><br><span class="line">        <span class="keyword">for</span> net, opt, l_his <span class="keyword">in</span> <span class="built_in">zip</span>(nets, optimizers, losses_his):</span><br><span class="line">            output = net(b_x)              <span class="comment"># get output for every net</span></span><br><span class="line">            loss = loss_func(output, b_y)  <span class="comment"># compute loss for every net</span></span><br><span class="line">            opt.zero_grad()                <span class="comment"># clear gradients for next train</span></span><br><span class="line">            loss.backward()                <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">            opt.step()                     <span class="comment"># apply gradients</span></span><br><span class="line">            l_his.append(loss.data.numpy())     <span class="comment"># loss recoder</span></span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/3-6-2.png"></p>
<p><code>SGD</code> 是最普通的优化器，也可以说没有加速效果，而 <code>Momentum</code> 是 <code>SGD</code> 的改良版，它加入了动量原则。后面的 <code>RMSprop</code> 又是 <code>Momentum</code> 的升级版。而 <code>Adam</code> 又是 <code>RMSprop</code> 的升级版。不过从这个结果中我们看到，<code>Adam</code> 的效果似乎比 <code>RMSprop</code> 要差一点。所以说并不是越先进的优化器，结果越佳。我们在自己的试验中可以尝试不同的优化器，找到那个最适合你数据/网络的优化器。</p>
<p>所以这也就是在我 <a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py">github 代码</a> 中的每一步的意义啦。</p>
<blockquote>
<p><strong>原文地址：</strong><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py">第一节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/301_simple_regression.py">用 Tensorflow 达到第一节同样效果的代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py">第二节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/302_simple_classification.py">用 Tensorflow 达到第二节同样效果的代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py">第三节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/304_save_reload.py">第四节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/305_batch_train.py">第五节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py">第六节的全部代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/304_optimizer.py">Tensorflow 的 Optimizer 代码</a></li>
<li><a target="_blank" rel="noopener" href="http://pytorch.org/docs/optim.html">PyTorch 优化器网页</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/303_save_reload.py">Tensorflow 的保存读取代码</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-NN">我制作的 什么是神经网络 动画简介</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-speed-up-learning">我制作的 训练优化器 动画简介</a></li>
<li>英文学习<a target="_blank" rel="noopener" href="http://sebastianruder.com/optimizing-gradient-descent/">资料</a></li>
<li><a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/torch/optimizer">PyTorch 可视化优化器</a></li>
<li><a target="_blank" rel="noopener" href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">银河</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yingyingmonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://yingyingmonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://YingYingMonstre.github.io" target="_blank">银河之家</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/YingYingMonstre.github.io/tags/PyTorch-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">PyTorch 神经网络</a></div><div class="post_share"><div class="social-share" data-image="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/"><img class="prev-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="onerror=null;src='/YingYingMonstre.github.io/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">高级神经网络结构（上）</div></div></a></div><div class="next-post pull-right"><a href="/YingYingMonstre.github.io/2021/12/07/PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"><img class="next-cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="onerror=null;src='/YingYingMonstre.github.io/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PyTorch神经网络基础</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/YingYingMonstre.github.io/2021/12/11/高级神经网络结构（上）/" title="高级神经网络结构（上）"><img class="cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-11</div><div class="title">高级神经网络结构（上）</div></div></a></div><div><a href="/YingYingMonstre.github.io/2021/12/12/高级神经网络结构（下）/" title="高级神经网络结构（下）"><img class="cover" src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&refer=http%3A%2F%2Ftva1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1641393896&t=2c5167116c4f1c9fa1474342824c9969" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-12</div><div class="title">高级神经网络结构（下）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/YingYingMonstre.github.io/img/avatar.jpg" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">银河</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/archives/"><div class="headline">文章</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div><div class="card-info-data-item is-center"><a href="/YingYingMonstre.github.io/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YingYingMonstre"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%8B%9F%E5%90%88-%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">关系拟合 (回归)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9"><span class="toc-number">1.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.</span> <span class="toc-text">建立数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.3.</span> <span class="toc-text">建立神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">1.5.</span> <span class="toc-text">可视化训练过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%BA%E5%88%86%E7%B1%BB%E5%9E%8B-%E5%88%86%E7%B1%BB"><span class="toc-number">2.</span> <span class="toc-text">区分类型 (分类)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-1"><span class="toc-number">2.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">2.2.</span> <span class="toc-text">建立数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="toc-number">2.3.</span> <span class="toc-text">建立神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C-1"><span class="toc-number">2.4.</span> <span class="toc-text">训练网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B-1"><span class="toc-number">2.5.</span> <span class="toc-text">可视化训练过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">快速搭建法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-2"><span class="toc-number">3.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA"><span class="toc-number">3.2.</span> <span class="toc-text">快速搭建</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%8F%90%E5%8F%96"><span class="toc-number">4.</span> <span class="toc-text">保存提取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-3"><span class="toc-number">4.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98"><span class="toc-number">4.2.</span> <span class="toc-text">保存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C"><span class="toc-number">4.3.</span> <span class="toc-text">提取网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AA%E6%8F%90%E5%8F%96%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0"><span class="toc-number">4.4.</span> <span class="toc-text">只提取网络参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA%E7%BB%93%E6%9E%9C"><span class="toc-number">4.5.</span> <span class="toc-text">显示结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E8%AE%AD%E7%BB%83"><span class="toc-number">5.</span> <span class="toc-text">批训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-4"><span class="toc-number">5.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DataLoader"><span class="toc-number">5.2.</span> <span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E9%80%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83-Speed-Up-Training"><span class="toc-number">6.</span> <span class="toc-text">加速神经网络训练 (Speed Up Training)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Stochastic-Gradient-Descent-SGD"><span class="toc-number">6.1.</span> <span class="toc-text">Stochastic Gradient Descent (SGD)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Momentum-%E6%9B%B4%E6%96%B0%E6%96%B9%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">Momentum 更新方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AdaGrad-%E6%9B%B4%E6%96%B0%E6%96%B9%E6%B3%95"><span class="toc-number">6.3.</span> <span class="toc-text">AdaGrad 更新方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RMSProp-%E6%9B%B4%E6%96%B0%E6%96%B9%E6%B3%95"><span class="toc-number">6.4.</span> <span class="toc-text">RMSProp 更新方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adam-%E6%9B%B4%E6%96%B0%E6%96%B9%E6%B3%95"><span class="toc-number">6.5.</span> <span class="toc-text">Adam 更新方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimizer-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">7.</span> <span class="toc-text">Optimizer 优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9-5"><span class="toc-number">7.1.</span> <span class="toc-text">要点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%AA%E6%95%B0%E6%8D%AE"><span class="toc-number">7.2.</span> <span class="toc-text">伪数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%8F%E4%B8%AA%E4%BC%98%E5%8C%96%E5%99%A8%E4%BC%98%E5%8C%96%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.3.</span> <span class="toc-text">每个优化器优化一个神经网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8-Optimizer"><span class="toc-number">7.4.</span> <span class="toc-text">优化器 Optimizer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-%E5%87%BA%E5%9B%BE"><span class="toc-number">7.5.</span> <span class="toc-text">训练&#x2F;出图</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/" title="PyTorch高阶内容"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="PyTorch高阶内容"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/" title="PyTorch高阶内容">PyTorch高阶内容</a><time datetime="2021-12-12T12:40:00.000Z" title="发表于 2021-12-12 20:40:00">2021-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/" title="高级神经网络结构（下）"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="高级神经网络结构（下）"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/" title="高级神经网络结构（下）">高级神经网络结构（下）</a><time datetime="2021-12-11T16:00:00.000Z" title="发表于 2021-12-12 00:00:00">2021-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/" title="高级神经网络结构（上）"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="高级神经网络结构（上）"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/" title="高级神经网络结构（上）">高级神经网络结构（上）</a><time datetime="2021-12-11T14:00:00.000Z" title="发表于 2021-12-11 22:00:00">2021-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="建造第一个神经网络"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="建造第一个神经网络"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="建造第一个神经网络">建造第一个神经网络</a><time datetime="2021-12-11T11:30:00.000Z" title="发表于 2021-12-11 19:30:00">2021-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/YingYingMonstre.github.io/2021/12/07/PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="PyTorch神经网络基础"><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Ftva1.sinaimg.cn%2Flarge%2F006tNbRwgy1g9qk7x6noxj31400g4jsd.jpg&amp;refer=http%3A%2F%2Ftva1.sinaimg.cn&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641393896&amp;t=2c5167116c4f1c9fa1474342824c9969" onerror="this.onerror=null;this.src='/YingYingMonstre.github.io/img/404.jpg'" alt="PyTorch神经网络基础"/></a><div class="content"><a class="title" href="/YingYingMonstre.github.io/2021/12/07/PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="PyTorch神经网络基础">PyTorch神经网络基础</a><time datetime="2021-12-07T15:30:00.000Z" title="发表于 2021-12-07 23:30:00">2021-12-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 银河</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/YingYingMonstre.github.io/js/utils.js"></script><script src="/YingYingMonstre.github.io/js/main.js"></script><script src="/YingYingMonstre.github.io/js/search/local-search.js"></script><div class="js-pjax"></div><script src="js/custom.js"></script><script src="../js/custom.js"></script><script src="../../js/custom.js"></script><script src="../../../../js/custom.js"></script></div></body></html>