<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MacBook Air M1安装gym问题汇总</title>
    <url>/YingYingMonstre.github.io/2021/11/03/MacBook%20Air%20M1%E5%AE%89%E8%A3%85gym%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>安装gym花了一天时间，中途出了很多问题，特此记录。</p>
<ol start="0">
<li>关于本机</li>
</ol>
<ul>
<li>macOS Monterey版本12.0.1</li>
<li>MacBook Air（M1，2020）</li>
<li>芯片 Apple M1</li>
</ul>
<ol>
<li>环境相关</li>
</ol>
<p>创建pycharm环境参考文章：<a href="https://zhuanlan.zhihu.com/p/410961551%E3%80%82%E4%B8%BB%E8%A6%81%E6%98%AF%E4%B8%BA%E4%BA%86%E5%AE%89%E8%A3%85pytorch%E3%80%82">https://zhuanlan.zhihu.com/p/410961551。主要是为了安装pytorch。</a></p>
<ol start="2">
<li>下载gym</li>
</ol>
<p>可以直接在终端上pip install gym，但是在pycharm中使用时会报错缺一些组件，比如No module named ‘pyglet’。本人使用的是下边的方法：</p>
<p>git clone <a href="https://github.com/openai/gym.git">https://github.com/openai/gym.git</a></p>
<p>cd gym</p>
<p>pip install -e ‘.[all]’</p>
<ol start="3">
<li>No matching distribution found for ale-py~=0.7.1 (from gym==0.21.0)</li>
</ol>
<p>问题状况：在pip install -e ‘.[all]’时报错。</p>
<p>解决方法：直接pip install ale-py，如果不能下载（具体报错的原因忘了），尝试conda update pip，我是在更新完pip后可以下载的。</p>
<ol start="4">
<li>unable to execute ‘swig’: No such file or directory</li>
</ol>
<p>问题状况：解决了第3步的问题后，在pip install -e ‘.[all]’过程中出现。</p>
<p>解决方法：brew install swig</p>
<ol start="5">
<li>zsh: command not found: brew</li>
</ol>
<p>问题状况：在第4步输入brew install swig后报错。</p>
<p>解决方法：mac安装homebrew，</p>
<p>用以下命令安装，序列号选择中科大（1）的</p>
<p>/bin/zsh -c “$(curl -fsSL <a href="https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;">https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;</a></p>
<p>原文地址：<a href="https://links.jianshu.com/go?to=https://blog.csdn.net/wangyun71/article/details/108560873">https://blog.csdn.net/wangyun71/article/details/108560873</a></p>
<ol start="6">
<li>from . import multiarray等</li>
</ol>
<p>问题状况：在解决了第4、5步的问题后继续运行pip install -e ‘.[all]’，成功安装。但是在测试用例代码时报的错误。提示检查python和numpy的版本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></span><br></pre></td></tr></table></figure>

<p>解决方法：重新安装了numpy之后就可以用了。</p>
<p>pip uninstall numpy</p>
<p>pip install numpy</p>
<ol start="7">
<li>zsh:killed</li>
</ol>
<p>问题状况：偶然遇到的，在conda activate环境后不能用clear、pip等命令，不知道是什么原因。</p>
<p>解决方法：删除环境重新搭建环境就可以了</p>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ModbusTCP协议学习</title>
    <url>/YingYingMonstre.github.io/2021/09/17/ModbusTCP%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Modbus由MODICON公司于1979年开发，是一种<strong>工业现场总线</strong>协议标准。1996年<strong>施耐德</strong>公司推出基于<strong>以太网TCP/IP</strong>的Modbus协议：<strong>Modbus TCP</strong>。</p>
<p>Modbus协议是一项应用层报文传输协议，包括ASCII、RTU、<strong>TCP</strong>三种报文类型。</p>
<p>标准的Modbus协议物理层接口有RS232、RS422、RS485和<strong>以太网</strong>接口，采用<strong>master/slave</strong>方式通信。</p>
<h2 id="Modbus-TCP数据帧"><a href="#Modbus-TCP数据帧" class="headerlink" title="Modbus TCP数据帧"></a>Modbus TCP数据帧</h2><p>Modbus TCP的数据帧可分为两部分：<strong>MBAP</strong>+<strong>PDU</strong>。其协议特征如图所示。</p>
<p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fwww.51wendang.com%2Fpic%2Fe5675f007044fd1446490edf%2F2-537-png_6_0_0_464_608_341_194_892.8_1262.699-947-0-526-947.jpg&refer=http%3A%2F%2Fwww.51wendang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1634438115&t=92f4357504d2885188a29589d7de4e7f"></p>
<center>Modbus TCP协议特征</center>

<h3 id="报文头MBAP"><a href="#报文头MBAP" class="headerlink" title="报文头MBAP"></a>报文头MBAP</h3><p>MBAP为报文头，长度为7字节，组成如下：</p>
<table>
<thead>
<tr>
<th align="left">事务处理标识</th>
<th align="left">协议标识</th>
<th align="left">长度</th>
<th align="left">单元标识符</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2字节</td>
<td align="left">2字节</td>
<td align="left">2字节</td>
<td align="left">1字节</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td><strong>事务处理标识</strong></td>
<td>可以理解为报文的序列号，一般每次通信之后就要加1以区别不同的通信数据报文。</td>
</tr>
<tr>
<td><strong>协议标识符</strong></td>
<td>00 00表示Modbus TCP协议。</td>
</tr>
<tr>
<td><strong>长度</strong></td>
<td>表示接下来的数据长度，单位为字节。</td>
</tr>
<tr>
<td><strong>单元标识符</strong></td>
<td>可以理解为设备地址。</td>
</tr>
</tbody></table>
<h3 id="帧结构PDU"><a href="#帧结构PDU" class="headerlink" title="帧结构PDU"></a>帧结构PDU</h3><p>PDU由<strong>功能码+数据</strong>组成。功能码为1字节，数据长度不定，由具体功能决定。</p>
<p><strong>功能码</strong></p>
<p>Modbus的操作对象有四种：线圈、离散输入、保持寄存器、输入寄存器。</p>
<table>
<thead>
<tr>
<th align="center">对象</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线圈</td>
<td align="center">PLC的输出位，开关量，在Modbus中可读可写</td>
</tr>
<tr>
<td align="center">离散量</td>
<td align="center">PLC的输入位，开关量，在Modbus中只读</td>
</tr>
<tr>
<td align="center">输入寄存器</td>
<td align="center">PLC中只能从模拟量输入端改变的寄存器，在Modbus中只读</td>
</tr>
<tr>
<td align="center">保持寄存器</td>
<td align="center">PLC中用于输出模拟量信号的寄存器，在Modbus中可读可写</td>
</tr>
</tbody></table>
<p>根据对象的不同，Modbus的功能码有：</p>
<table>
<thead>
<tr>
<th align="center">功能码</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0x01</td>
<td align="center">读线圈</td>
</tr>
<tr>
<td align="center">0x05</td>
<td align="center">写单个线圈</td>
</tr>
<tr>
<td align="center">0x0F</td>
<td align="center">写多个线圈</td>
</tr>
<tr>
<td align="center">0x02</td>
<td align="center">读离散量输入</td>
</tr>
<tr>
<td align="center">0x04</td>
<td align="center">读输入寄存器</td>
</tr>
<tr>
<td align="center">0x03</td>
<td align="center">读保持寄存器</td>
</tr>
<tr>
<td align="center">0x06</td>
<td align="center">写单个保持寄存器</td>
</tr>
<tr>
<td align="center">0x10</td>
<td align="center">写多个保持寄存器</td>
</tr>
</tbody></table>
<p>说明更详细的表</p>
<table>
<thead>
<tr>
<th align="center">代码</th>
<th align="center">中文名称</th>
<th align="center">英文名</th>
<th align="center">位操作/字操作</th>
<th align="center">操作数量</th>
</tr>
</thead>
<tbody><tr>
<td align="center">01</td>
<td align="center">读线圈状态</td>
<td align="center">READ COIL STATUS</td>
<td align="center">位操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">02</td>
<td align="center">读离散输入状态</td>
<td align="center">READ INPUT STATUS</td>
<td align="center">位操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">03</td>
<td align="center">读保持寄存器</td>
<td align="center">READ HOLDING REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">04</td>
<td align="center">读输入寄存器</td>
<td align="center">READ INPUT REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">05</td>
<td align="center">写线圈状态</td>
<td align="center">WRITE SINGLE COIL</td>
<td align="center">位操作</td>
<td align="center">单个</td>
</tr>
<tr>
<td align="center">06</td>
<td align="center">写单个保持寄存器</td>
<td align="center">WRITE SINGLE REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个</td>
</tr>
<tr>
<td align="center">15</td>
<td align="center">写多个线圈</td>
<td align="center">WRITE MULTIPLE COIL</td>
<td align="center">位操作</td>
<td align="center">多个</td>
</tr>
<tr>
<td align="center">16</td>
<td align="center">写多个保持寄存器</td>
<td align="center">WRITE MULTIPLE REGISTER</td>
<td align="center">字操作</td>
<td align="center">多个</td>
</tr>
</tbody></table>
<h2 id="PDU详细结构"><a href="#PDU详细结构" class="headerlink" title="PDU详细结构"></a>PDU详细结构</h2><p>测试软件：mod_RSsim5.3</p>
<table>
<thead>
<tr>
<th align="center">模式</th>
<th align="center">对应</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线圈</td>
<td align="center">Coil Outputs</td>
</tr>
<tr>
<td align="center">离散量</td>
<td align="center">Digital Inputs</td>
</tr>
<tr>
<td align="center">输入寄存器</td>
<td align="center">Analogue Inputs</td>
</tr>
<tr>
<td align="center">保持寄存器</td>
<td align="center">Holding Registers</td>
</tr>
</tbody></table>
<p><strong>0x01：读线圈</strong></p>
<p>在从站中读1~2000个连续线圈状态，ON=1,OFF=0</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 数量H 数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 数据（一个地址的数据为1位）</li>
<li>如：在从站0x01中，读取开始地址为0x0002的线圈数据，读0x0008位<br>00 01 00 00 00 06 01 01 00 02 00 08</li>
<li>回：数据长度为0x01个字节，数据为0x01，第一个线圈为ON，其余为OFF<br>00 01 00 00 00 04 01 01 01 01</li>
</ul>
<p><strong>0x05：写单个线圈</strong></p>
<p>将从站中的一个输出写成ON或OFF，0xFF00请求输出为ON,0x000请求输出为OFF</p>
<ul>
<li>请求：MBAP 功能码 输出地址H 输出地址L 输出值H 输出值L（共12字节）</li>
<li>响应：MBAP 功能码 输出地址H 输出地址L 输出值H 输出值L（共12字节）</li>
<li>如：将地址为0x0003的线圈设为ON<br>00 01 00 00 00 06 01 05 00 03 FF 00</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 05 00 03 FF 00</li>
</ul>
<p><strong>0x0F：写多个线圈</strong></p>
<p>将一个从站中的一个线圈序列的每个线圈都强制为ON或OFF，数据域中置1的位请求相应输出位ON，置0的位请求响应输出为OFF</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 输出数量H 输出数量L 字节长度 输出值H 输出值L</li>
<li>响应：MBAP 功能码 起始地址H 起始地址L 输出数量H 输出数量L</li>
</ul>
<p><strong>0x02：读离散量输入</strong></p>
<p>从一个从站中读1~2000个连续的离散量输入状态</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 数量H 数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 数据（长度：9+ceil（数量/8））</li>
<li>如：从地址0x0000开始读0x0012个离散量输入<br>00 01 00 00 00 06 01 02 00 00 00 12</li>
<li>回：数据长度为0x03个字节，数据为0x01 04 00，表示第一个离散量输入和第11个离散量输入为ON，其余为OFF<br>00 01 00 00 00 06 01 02 03 01 04 00</li>
</ul>
<p><strong>0x04：读输入寄存器</strong></p>
<p>从一个远程设备中读1~2000个连续输入寄存器</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 寄存器数据(长度：9+寄存器数量×2)</li>
<li>如：读起始地址为0x0002，数量为0x0005的寄存器数据<br>00 01 00 00 00 06 01 04 00 02 00 05</li>
<li>回：数据长度为0x0A，第一个寄存器的数据为0x0c，其余为0x00<br>00 01 00 00 00 0D 01 04 0A 00 0C 00 00 00 00 00 00 00 00</li>
</ul>
<p><strong>0x03：读保持寄存器</strong></p>
<p>从远程设备中读保持寄存器连续块的内容</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 寄存器数据(长度：9+寄存器数量×2)</li>
<li>如：起始地址是0x0000，寄存器数量是 0x0003<br>00 01 00 00 00 06 01 03 00 00 00 03</li>
<li>回：数据长度为0x06，第一个寄存器的数据为0x21，其余为0x00<br>00 01 00 00 00 09 01 03 06 00 21 00 00 00 00</li>
</ul>
<p><strong>0x06：写单个保持寄存器</strong></p>
<p>在一个远程设备中写一个保持寄存器</p>
<ul>
<li>请求：MBAP 功能码 寄存器地址H 寄存器地址L 寄存器值H 寄存器值L（共12字节）</li>
<li>响应：MBAP 功能码 寄存器地址H 寄存器地址L 寄存器值H 寄存器值L（共12字节）</li>
<li>如：向地址是0x0000的寄存器写入数据0x000A<br>00 01 00 00 00 06 01 06 00 00 00 0A</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 06 00 00 00 0A</li>
</ul>
<p><strong>0x10：写多个保持寄存器</strong></p>
<p>在一个远程设备中写连续寄存器块（1~123个寄存器）</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L 字节长度 寄存器值（13+寄存器数量×2）</li>
<li>响应：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>如：向起始地址为0x0000，数量为0x0001的寄存器写入数据，数据长度为0x02，数据为0x000F<br>00 01 00 00 00 09 01 10 00 00 00 01 02 00 0F</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 10 00 00 00 01</li>
</ul>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    HOST = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    TCP_IP = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    TCP_PORT = <span class="number">502</span></span><br><span class="line">    MaxBytes = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建socket连接</span></span><br><span class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s.connect((TCP_IP, TCP_PORT))</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;error&#x27;</span>, e)</span><br><span class="line">        s.close()</span><br><span class="line">        sys.exit()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 连接成功</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;have connected with server&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 十进制、十六进制都可以</span></span><br><span class="line">    <span class="comment"># 示例为写一个保持寄存器</span></span><br><span class="line">    arr = [<span class="number">00</span>, <span class="number">1</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">0x0A</span>]</span><br><span class="line">    data = struct.pack(<span class="string">&quot;%dB&quot;</span> % (<span class="built_in">len</span>(arr)), *arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        s.settimeout(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># s.sendall(data) 发送数据包</span></span><br><span class="line">        sendBytes = s.send(data)</span><br><span class="line">        <span class="keyword">if</span> sendBytes &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接受响应信息</span></span><br><span class="line">        recvData = s.recv(MaxBytes)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> recvData:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;接收数据为空，我要退出了&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        localTime = time.asctime(time.localtime(time.time()))</span><br><span class="line">        <span class="built_in">print</span>(localTime, <span class="string">&#x27; 接收到数据字节数:&#x27;</span>, <span class="built_in">len</span>(recvData))</span><br><span class="line">        <span class="built_in">print</span>(struct.unpack(<span class="string">&quot;%dB&quot;</span> % (<span class="built_in">len</span>(recvData)), recvData))</span><br><span class="line"></span><br><span class="line">        localTime = time.asctime(time.localtime(time.time()))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;我已经退出了，后会无期&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="Modbus-TCP-示例报文"><a href="#Modbus-TCP-示例报文" class="headerlink" title="Modbus TCP 示例报文"></a>Modbus TCP 示例报文</h2><p>ModBusTcp与串行链路Modbus的数据域是一致的，具体数据域可以参考串行Modbus。这里给出几个ModbusTcp的链路解析说明，辅助新人分析报文。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b0fa067f61a600643c84f36ea69c49bb.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2a92c75438fa7336652d27ecd6081a08.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/616f2f4ff46da56aacec9bf7266043db.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c62b523c4e499d641ae06973b6b29c95.png"></p>
<blockquote>
<p>功能码 0x10：写多个保持寄存器。上面图片3和图片4都写错了。</p>
</blockquote>
<h2 id="ModbusTCP通信"><a href="#ModbusTCP通信" class="headerlink" title="ModbusTCP通信"></a>ModbusTCP通信</h2><p><strong>通信方式</strong></p>
<p>Modbus设备可分为主站(poll)和从站(slave)。主站只有一个，从站有多个，主站向各从站发送请求帧，从站给予响应。在使用TCP通信时，主站为client端，主动建立连接；从站为server端，等待连接。</p>
<ul>
<li>主站请求：功能码+数据</li>
<li>从站正常响应：请求功能码+响应数据</li>
<li>从站异常响应：异常功能码+异常码，其中异常功能码即将请求功能码的最高有效位置1，异常码指示差错类型</li>
<li><strong>注意：需要超时管理机制，避免无期限的等待可能不出现的应答</strong></li>
</ul>
<p>IANA（Internet Assigned Numbers Authority，互联网编号分配管理机构）给Modbus协议赋予TCP端口号为<strong>502</strong>，这是目前在仪表与自动化行业中唯一分配到的端口号。</p>
<p><strong>通信过程</strong></p>
<ol>
<li>connect 建立TCP连接</li>
<li>准备Modbus报文</li>
<li>使用send命令发送报文</li>
<li>在同一连接下等待应答</li>
<li>使用recv命令读取报文，完成一次数据交换</li>
<li>通信任务结束时，关闭TCP连接</li>
</ol>
<h2 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h2><p>在工业自动化控制中，经常会遇到开关量，数字量，模拟量，离散量，脉冲量等各种概念，而人们在实际应用中，对于这些概念又很容易混淆。现将各种概念罗列如下：</p>
<p><strong>1.开关量：</strong></p>
<p>一般指的是触点的“开”与“关”的状态，一般在计算机设备中也会用“0”或“1”来表示开关量的状态。开关量分为有源开关量信号和无源开关量信号，有源开关量信号指的是“开”与“关”的状态是带电源的信号，专业叫法为跃阶信号，可以理解为脉冲量，一般的都有220VAC, 110VAC,24VDC,12VDC等信号，无源开关量信号指的是“开”和“关”的状态时不带电源的信号，一般又称之为干接点。电阻测试法为电阻0或无穷大。</p>
<p><strong>2.数字量：</strong></p>
<p>很多人会将数字量与开关量混淆，也将其与模拟量混淆。数字量在时间和数量上都是离散的物理量，其表示的信号则为数字信号。数字量是由0和1组成的信号，经过编码形成有规律的信号，量化后的模拟量就是数字量。</p>
<p><strong>3.模拟量：</strong></p>
<p>模拟量的概念与数字量相对应，但是经过量化之后又可以转化为数字量。模拟量是在时间和数量上都是连续的物理量，其表示的信号则为模拟信号。模拟量在连续的变化过程中任何一个取值都是一个具体有意义的物理量，如温度，电压，电流等。</p>
<p><strong>4.离散量：</strong></p>
<p>离散量是将模拟量离散化之后得到的物理量。即任何仪器设备对于模拟量都不可能有个完全精确的表示，因为他们都有一个采样周期，在该采样周期内，其物理量的数值都是不变的，而实际上的模拟量则是变化的。这样就将模拟量离散化，成为了离散量。</p>
<p><strong>5.脉冲量：</strong></p>
<p>脉冲量就是瞬间电压或电流由某一值跃变到另一值的信号量。在量化后，其变化持续有规律就是数字量，如果其由0变成某一固定值并保持不变，其就是开关量。</p>
<blockquote>
<p>综上所述，模拟量就是在某个过程中时间和数量连续变化的物理量，由于在实际的应用中，所有的仪器设备对于外界数据的采集都有一个采样周期，其采集的数据只有在下一个采样周期开始时才有变动，采样周期内其数值并不随模拟量的变化而变动。</p>
<p>这样就将模拟量离散化了，例如：某设备的采样周期为1秒，其在第五秒的时间采集的温度为35度，而第六秒的温度为36度，该设备就只能标称第五秒时间温度35度，第六秒时间温度36度，而第五点五秒的时间其标称也只是35度，但是其实际的模拟量是35.5度。这样就将模拟信号离散化。其采集的数据就是离散化了，不再是连续的模拟量信号。</p>
<p>由于计算机只识别0和1两个信号，即开关量信号，用其来表示数值都是使用数字串来表示，由于计算能力的问题，其数字串不能无限长，即其表达的精度也是有限的，同样的以温度为例，由于数字串限制，其表达温度的精度只能达到0.1度，小于该单位的数值则不能被标称，这样就必须将离散量进行量化，将其变为数字量。即35.68度的温度则表示为35.6度。</p>
</blockquote>
<p>参考文章：<a href="https://www.cnblogs.com/ioufev/articles/10830028.html">ModbusTCP协议 - ioufev - 博客园 (cnblogs.com)</a></p>
]]></content>
      <tags>
        <tag>Modbus</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title>XCTF-Web基础篇</title>
    <url>/YingYingMonstre.github.io/2021/09/26/XCTF-Web%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
    <content><![CDATA[<h2 id="view-source"><a href="#view-source" class="headerlink" title="view_source"></a>view_source</h2><p>方法一：不能用右键审查元素，则按F12打开开发者工具查看，找到flag</p>
<p>方法二：在url中通过view-source:的方法来访问源码，在url中提交view-source:+url</p>
<p>方法三：通过Burpsuite抓包查看源代码</p>
<h2 id="robots"><a href="#robots" class="headerlink" title="robots"></a>robots</h2><p>[原理]</p>
<p>robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。</p>
<p>[步骤]</p>
<p>1.根据提示robots,可以直接想到robots.txt</p>
<p>2.或通过扫目录也可以扫到: <code>python dirsearch.py -u http://10.10.10.175:32793/ -e *</code>（这个脚本在cmd中可以用，git bash不行）</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/robots/1.png" alt="img"></p>
<p>3.访问<code>http://111.198.29.45:33982/robots.txt</code>发现<code>f1ag_1s_h3re.php</code></p>
<p>4.访问<code>http://111.198.29.45:33982/f1ag_1s_h3re.php</code>得到flag</p>
<h2 id="backup"><a href="#backup" class="headerlink" title="backup"></a>backup</h2><p><strong>[目标]</strong></p>
<p>掌握有关备份文件的知识</p>
<p>常见的备份文件后缀名有: <code>.git .svn .swp .svn .~ .bak .bash_history</code></p>
<p><strong>[环境]</strong></p>
<p>无</p>
<p><strong>[工具]</strong></p>
<p>扫目录脚本dirsearch(项目地址：<code>https://github.com/maurosoria/dirsearch</code>(<code>https://github.com/maurosoria/dirsearch</code>))</p>
<p><strong>[步骤]</strong></p>
<p>1.可以手动猜测,也可以使用扫目录脚本/软件,扫一下,这里使用的是github上的脚本dirsearch,命令行下: <code>py python3 dirsearch.py -u http://10.10.10.175:32770 -e *</code></p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/backup/1.png" alt="img"></p>
<p>2.看到存在备份文件index.php.bak访问 <code>http://10.10.10.175:32770/index.php.bak</code></p>
<p>3.保存到本地打开，即可看到flag</p>
<h2 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h2><p>[原理]</p>
<p> Cookie是当主机访问Web服务器时，由 Web 服务器创建的，将信息存储在用户计算机上的文件。一般网络用户习惯用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 Session 跟踪而存储在用户本地终端上的数据，而这些数据通常会经过加密处理。</p>
<p><strong>[目的]</strong></p>
<p>掌握有关cookie的知识，了解cookie所在位置</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.浏览器按下F12键打开开发者工具，刷新后，在存储一栏，可看到名为look-here的cookie的值为cookie.php</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/1.png" alt="img"></p>
<p>2.访问<code>http://111.198.29.45:47911/cookie.php</code>，提示查看http响应包，在网络一栏，可看到访问cookie.php的数据包</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/2.png" alt="img"></p>
<p>3.点击查看数据包，在消息头内可发现flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/3.png" alt="img"></p>
<p>用dirsearch可以找到cookie.php文件，再用burpsuite抓包也可行。</p>
<h2 id="disabled-button"><a href="#disabled-button" class="headerlink" title="disabled_button"></a>disabled_button</h2><p><strong>[目标]</strong></p>
<p>初步了解前端知识</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开开发者工具（得用火狐浏览器），在查看器窗口审查元素，发现存在disabled=””字段，</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/1.png" alt="img"></p>
<p>2.将<code>disabled=&quot;&quot;</code>删除后，按钮可按，按下后得到flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/2.png" alt="img"></p>
<p>3.或审计from表单代码，使用hackbar（不能用，现在收费了），用post方式传递auth=flag，同样可以获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/3.png" alt="img"></p>
<h2 id="weak-auth"><a href="#weak-auth" class="headerlink" title="weak_auth"></a>weak_auth</h2><p><strong>[目标]</strong></p>
<p>了解弱口令，掌握爆破方法</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<ul>
<li>burpsuite</li>
<li>字典<code>&lt;https://github.com/rootphantomer/Blasting_dictionary/blob/master/%E5%B8%B8%E7%94%A8%E5%AF%86%E7%A0%81.txt&gt;</code></li>
</ul>
<p><strong>[步骤]</strong></p>
<p>1.随便输入下用户名和密码,提示要用admin用户登入,然后跳转到了check.php,查看下源代码提示要用字典。</p>
<p>2.用burpsuite截下登录的数据包,把数据包发送到intruder爆破</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/1.png" alt="img"></p>
<p>2.设置爆破点为password</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/2.png" alt="img"></p>
<p>3.加载字典</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/3.png" alt="img"></p>
<p>4.开始攻击，查看响应包列表，发现密码为123456时，响应包的长度和别的不一样.</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/4.png" alt="img"></p>
<p>5.点进去查看响应包，获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/5.png" alt="img"></p>
<h2 id="simple-php"><a href="#simple-php" class="headerlink" title="simple_php"></a>simple_php</h2><p><strong>[原理]</strong></p>
<p>php中有两种比较符号</p>
<p>=== 会同时比较字符串的值和类型</p>
<p>== 会先将字符串换成相同类型（先类型转换），再作比较，属于弱类型比较</p>
<p><strong>[目地]</strong></p>
<p>掌握php的弱类型比较</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开页面，进行代码审计，发现同时满足 $a==0 和 $a 时，显示flag1。</p>
<p>2.php中的弱类型比较会使’abc’ == 0为真，所以输入a=abc时，可得到flag1，如图所示。（abc可换成任意字符）。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_php/1.png" alt="img"></p>
<p>3.is_numeric() 函数会判断如果是数字和数字字符串则返回 TRUE，否则返回 FALSE,且php中弱类型比较时，会使(‘1234a’ == 1234)为真，所以当输入a=abc&amp;b=1235a，可得到flag2，如图所示。==（数字和字符混合的字符串转换为整数后只保留数字）==</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_php/2.png" alt="img"></p>
<p> php类型比较表<a href="https://www.php.net/manual/zh/types.comparisons.php%E3%80%82">https://www.php.net/manual/zh/types.comparisons.php。</a></p>
<h2 id="get-post"><a href="#get-post" class="headerlink" title="get_post"></a>get_post</h2><p><strong>[原理]</strong></p>
<p>HTTP工作原理</p>
<p><strong>[目的]</strong></p>
<p>掌握常用http请求方式</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox和cmd</p>
<p><strong>[步骤]</strong></p>
<p>GET请求在url后加?a=1即可</p>
<p>POST请求需要用curl POST -d “b=2” <a href="http://111.200.241.244:65197/?a=1">http://111.200.241.244:65197/?a=1</a></p>
<p>返回如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl: (6) Could not resolve host: POST</span><br><span class="line">﻿&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang&#x3D;&quot;en&quot;&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;title&gt;POST&amp;GET&lt;&#x2F;title&gt;</span><br><span class="line">    &lt;link href&#x3D;&quot;http:&#x2F;&#x2F;libs.baidu.com&#x2F;bootstrap&#x2F;3.0.3&#x2F;css&#x2F;bootstrap.min.css&quot; rel&#x3D;&quot;stylesheet&quot; &#x2F;&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;请用GET方式提交一个名为a,值为1的变量&lt;&#x2F;h1&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;请再以POST方式随便提交一个名为b,值为2的变量&lt;&#x2F;h1&gt;&lt;h1&gt;cyberpeace&#123;51117d20e10dd8646f8d2eed10942e46&#125;&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>

<p>或者用burpsuite实现GET、POST的抓包：</p>
<p>GET的实现：在GET / HTTP/1.1中加入?a=1得到GET /?a=1 HTTP/1.1</p>
<p>POST的实现：GET / HTTP/1.1改为POST /?a=1 HTTP/1.1，在最下面（正文）加b=2，报文头加上Content-Type: application/x-www-form-urlencoded。</p>
<h2 id="xff-referer"><a href="#xff-referer" class="headerlink" title="xff_referer"></a>xff_referer</h2><p>[原理]</p>
<p>X-Forwarded-For:简称XFF头，它代表客户端，也就是HTTP的请求端真实的IP，只有在通过了HTTP 代理或者负载均衡服务器时才会添加该项</p>
<p>HTTP Referer是header的一部分，当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的</p>
<p><strong>[目的]</strong></p>
<p>掌握有关X-Forwarded-For和Referer的知识</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox、burpsuite</p>
<p><strong>[步骤]</strong></p>
<p>1.打开firefox和burp，使用burp对firefox进行代理拦截，在请求头添加<code>X-Forwarded-For: 123.123.123.123</code>，然后放包</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/xff_referer/1.png" alt="img"></p>
<p>2.接着继续在请求头内添加<code>Referer: https://www.google.com</code>，可获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/xff_referer/2.png" alt="img"></p>
<h2 id="webshell"><a href="#webshell" class="headerlink" title="webshell"></a>webshell</h2><p><strong>[目标]</strong></p>
<p>了解php一句话木马、如何使用webshell</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox、hackbar</p>
<p>蚁剑下载地址<code>https://github.com/AntSwordProject/antSword/releases</code>(<code>https://github.com/AntSwordProject/antSword/releases</code>)</p>
<p><strong>[步骤]</strong></p>
<p>1.直接提示给了php一句话，可以用菜刀或蚁剑连接,此处用蚁剑链接:</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/1.png" alt="img"></p>
<p>2.连接后在网站目录下发现了flag.txt文件，查看文件可获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/2.png" alt="img"></p>
<p>3.也可以使用hackbar，使用post方式传递shell=system(‘cat flag.txt’); 获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/3.png" alt="img"></p>
<h2 id="command-execution"><a href="#command-execution" class="headerlink" title="command_execution"></a>command_execution</h2><p><strong>[原理]</strong></p>
<p>| 的作用为将前一个命令的结果传递给后一个命令作为输入</p>
<p>&amp;&amp;的作用是前一条命令执行成功时，才执行后一条命令</p>
<p><strong>[目地]</strong></p>
<p>掌握命令拼接的方法</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开浏览器，在文本框内输入127.0.0.1 |  find / -name “flag.txt” （将 | 替换成 &amp; 或 &amp;&amp; 都可以）,查找flag所在位置，如图所示。</p>
<p>127.0.0.1 | ls ../../../可以找到一个home文件夹，里面有flag.txt文件。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/command_execution/1.png" alt="img"></p>
<p>2.在文本框内输入 127.0.0.1 |  cat /home/flag.txt 可得到flag，如图所示。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/command_execution/2.png" alt="img"></p>
<h2 id="simple-js"><a href="#simple-js" class="headerlink" title="simple_js"></a>simple_js</h2><p><strong>[原理]</strong></p>
<p>javascript的代码审计</p>
<p><strong>[目地]</strong></p>
<p>掌握简单的javascript函数</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开页面，查看源代码，可以发现js代码，如图所示。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_js/1.png" alt="img"></p>
<p>2.进行代码审计，发现不论输入什么都会跳到假密码，真密码位于 fromCharCode 。</p>
<p>3.先将字符串用python处理一下，得到数组[55,56,54,79,115,69,114,116,107,49,50]，exp如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="string">&quot;\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30&quot;</span></span><br><span class="line"><span class="built_in">print</span> (s)</span><br></pre></td></tr></table></figure>

<p>4.将得到的数字分别进行ascii处理，可得到字符串786OsErtk12，exp如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a = [55,56,54,79,115,69,114,116,107,49,50]</span></span><br><span class="line"><span class="string">c = &quot;&quot;</span></span><br><span class="line"><span class="string">for i in a:</span></span><br><span class="line"><span class="string">    b = chr(i)</span></span><br><span class="line"><span class="string">    c = c + b</span></span><br><span class="line"><span class="string">print(c)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">string = <span class="string">&quot;\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37&quot;</span> \</span><br><span class="line">         <span class="string">&quot;\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31&quot;</span> \</span><br><span class="line">         <span class="string">&quot;\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30&quot;</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">label = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> string:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&quot;,&quot;</span>:</span><br><span class="line">        label += <span class="built_in">chr</span>(count)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        count = count * <span class="number">10</span> + <span class="built_in">int</span>(i)</span><br><span class="line"></span><br><span class="line">label += <span class="built_in">chr</span>(count)</span><br><span class="line"><span class="built_in">print</span>(label)</span><br></pre></td></tr></table></figure>

<p>5.规范flag格式，可得到Cyberpeace{786OsErtk12}</p>
<h2 id="baby-web"><a href="#baby-web" class="headerlink" title="baby_web"></a>baby_web</h2><p><strong>【实验原理】</strong></p>
<p>web请求头中的location作用</p>
<p><strong>【实验目的】</strong></p>
<p>掌握web响应包头部常见参数</p>
<p><strong>【实验环境】</strong></p>
<p>Windows</p>
<p><strong>【实验工具】</strong></p>
<p>firefox</p>
<p><strong>【实验步骤】</strong></p>
<p>1.根据提示，在url中输入index.php,发现打开的仍然还是1.php</p>
<p>2.打开火狐浏览器的开发者模式，选择网络模块，再次请求index.php,查看返回包，可以看到location参数被设置了1.php，并且得到flag。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/baby_web/1.png" alt="img"></p>
<p>我的：burpsuite抓包，看到有flag。</p>
]]></content>
      <tags>
        <tag>XCTF</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（简介）</title>
    <url>/YingYingMonstre.github.io/2021/10/31/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是强化学习"><a href="#什么是强化学习" class="headerlink" title="什么是强化学习"></a>什么是强化学习</h3><h4 id="从无到有"><a href="#从无到有" class="headerlink" title="从无到有"></a>从无到有</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL1.png"></p>
<p>强化学习是一类算法，是让计算机实现从一开始什么都不懂，脑袋里没有一点想法，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程。实际中的强化学习例子有很多。比如近期最有名的 Alpha go，机器头一次在围棋场上战胜人类高手；让计算机自己学着玩经典游戏 Atari，这些都是让计算机在不断的尝试中更新自己的行为准则，从而一步步学会如何下好围棋、如何操控游戏得到高分。既然要让计算机自己学，那计算机通过什么来学习呢？</p>
<h4 id="虚拟老师"><a href="#虚拟老师" class="headerlink" title="虚拟老师"></a>虚拟老师</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL2.png"></p>
<p>原来计算机也需要一位虚拟的老师，这个老师比较吝啬，他不会告诉你如何移动、如何做决定，他为你做的事只有给你的行为打分，那我们应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，我只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为。</p>
<p>比如老师会根据我的开心程度来打分——我开心时，可以得到高分，我不开心时得到低分。有了这些被打分的经验，我就能判断为了拿到高分，我应该选择一张开心的脸， 避免选到伤心的脸。这也是强化学习的核心思想。可以看出在强化学习中，一种行为的分数是十分重要的。所以强化学习具有分数导向性。我们换一个角度来思考。这种分数导向性好比我们在监督学习中的正确标签。</p>
<h4 id="对比监督学习"><a href="#对比监督学习" class="headerlink" title="对比监督学习"></a>对比监督学习</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL3.png"></p>
<p>我们知道监督学习，是已经有了数据和数据对应的正确标签，比如这样。监督学习就能学习出那些脸对应哪种标签。不过强化学习还要更进一步，一开始它并没有数据和标签。</p>
<p>他要通过一次次在环境中的尝试，获取这些数据和标签，然后再学习通过哪些数据能够对应哪些标签，通过学习到的这些规律，尽可能地选择带来高分的行为 (比如这里的开心脸)。这也就证明了在强化学习中，分数标签就是他的老师，他和监督学习中的老师也差不多。</p>
<h4 id="RL-算法们"><a href="#RL-算法们" class="headerlink" title="RL 算法们"></a>RL 算法们</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL4.png"></p>
<p>强化学习是一个大家族，他包含了很多种算法，我们也会一一提到之中一些比较有名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的Q learning、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习 等等。</p>
<h3 id="强化学习方法汇总"><a href="#强化学习方法汇总" class="headerlink" title="强化学习方法汇总"></a>强化学习方法汇总</h3><p>了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法。比如说比较知名的控制方法 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>，<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a>，还有基于对环境的理解的 model-based RL 等等。接下来我们通过分类的方式来了解他们的区别。</p>
<h4 id="Model-free-和-Model-based"><a href="#Model-free-和-Model-based" class="headerlink" title="Model-free 和 Model-based"></a>Model-free 和 Model-based</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd1.png"></p>
<p>我们可以将所有强化学习的方法分为理不理解所处环境，如果我们不尝试去理解环境，环境给了我们什么就是什么。我们就把这种方法叫做 model-free，这里的 model 就是用模型来表示环境，那理解了环境也就是学会了用一个模型来代表环境，所以这种就是 model-based 方法。我们想象：现在环境就是我们的世界，我们的机器人正在这个世界里玩耍，他不理解这个世界是怎样构成的，也不理解世界对于他的行为会怎么样反馈。举个例子，他决定丢颗原子弹去真实的世界，结果把自己给炸死了，所有结果都是那么现实。不过如果采取的是 model-based RL，机器人会通过过往的经验，先理解真实世界是怎样的，并建立一个模型来模拟现实世界的反馈，最后他不仅可以在现实世界中玩耍，也能在模拟的世界中玩耍，这样就没必要去炸真实世界，连自己也炸死了，他可以像玩游戏一样炸炸游戏里的世界，也保住了自己的小命。那我们就来说说这两种方式的强化学习各用那些方法吧。</p>
<p>Model-free 的方法有很多，像 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">Sarsa</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a> 都是从环境中得到反馈然后从中学习。而 model-based RL 只是多了一道程序——为真实世界建模，也可以说他们都是 model-free 的强化学习，只是 model-based 多出了一个虚拟环境，我们不仅可以像 model-free 那样在现实中玩耍，还能在游戏中玩耍，而玩耍的方式也都是 model-free 中那些玩耍方式，最终 model-based 还有一个杀手锏是 model-free 超级羡慕的，那就是想象力。</p>
<p>Model-free 中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 model-based，他能通过想象来预判断接下来将要发生的所有情况，然后选择这些想象情况中最好的那种。并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来，我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p>
<h4 id="基于概率-和-基于价值"><a href="#基于概率-和-基于价值" class="headerlink" title="基于概率 和 基于价值"></a>基于概率 和 基于价值</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd2.png"></p>
<p>基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，我们会根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为铁定、毫不留情，就选价值最高的；而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他。</p>
<p>我们现在说的动作都是一个一个不连续的动作，而对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个概率分布在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢?</p>
<p>比如在基于概率这边，有 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a>，在基于价值这边有 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">Sarsa</a> 等。而且我们还能结合这两类方法的优势之处，创造更牛逼的一种方法，叫做 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-AC">Actor-Critic</a>，actor 会基于概率做出动作，而 critic 会对做出的动作给出动作的价值，这样就在原有的 policy gradients 上加速了学习过程。</p>
<h4 id="回合更新-和-单步更新"><a href="#回合更新-和-单步更新" class="headerlink" title="回合更新 和 单步更新"></a>回合更新 和 单步更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd3.png"></p>
<p>强化学习还能用另外一种方式分类，回合更新和单步更新，想象强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，我们要等待游戏结束，然后再总结这一回合中的所有转折点，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束，这样我们就能边玩边学习了。</p>
<p>再来说说方法，Monte-carlo learning 和基础版的 policy gradients 等 都是回合更新制，Q learning、Sarsa、升级版的 policy gradients 等都是单步更新制。因为单步更新更有效率，所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</p>
<h4 id="在线学习-和-离线学习"><a href="#在线学习-和-离线学习" class="headerlink" title="在线学习 和 离线学习"></a>在线学习 和 离线学习</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd4.png"></p>
<p>这个视频的最后一种分类方式是 在线学习和离线学习。所谓在线学习，就是指我必须本人在场，并且一定是本人边玩边学习；而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。或者我也不必要边玩边学习，我可以白天先存储下来玩耍时的记忆，然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢?</p>
<p>最典型的在线学习就是 Sarsa 了，还有一种优化 Sarsa 的算法，叫做 Sarsa lambda，最典型的离线学习就是 Q learning，后来人也根据离线学习的属性，开发了更强大的算法，比如让计算机学会玩电动的 Deep-Q-Network。</p>
<p>这就是我们从各种不同的角度来对比了强化学习中的多种算法。</p>
<h3 id="为什么用强化学习-Why"><a href="#为什么用强化学习-Why" class="headerlink" title="为什么用强化学习 Why?"></a>为什么用强化学习 Why?</h3><h4 id="强化学习介绍"><a href="#强化学习介绍" class="headerlink" title="强化学习介绍"></a>强化学习介绍</h4><p>强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支，由于近些年来的技术突破，和深度学习 (Deep Learning) 的整合，使得强化学习有了进一步的运用。比如让计算机学着玩游戏，AlphaGo 挑战世界围棋高手，都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生，成长为一个在环境中游刃有余的高手。</p>
<p>这些教程的教学，不依赖于任何强化学习的 python 模块。因为强化学习的复杂性、多样性，到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p>
<h4 id="模拟程序提前看"><a href="#模拟程序提前看" class="headerlink" title="模拟程序提前看"></a>模拟程序提前看</h4><p>以下是我们将要在后续的课程中实现的牛逼的自学程序。</p>
<p>Youtube 的模拟视频都在这里:</p>
<p><a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O">https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O</a>_.</p>
<p>优酷的模拟视频在这里:</p>
<p><a href="http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1">http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1</a></p>
<p>下面是其中一些模拟视频:</p>
<ul>
<li>Maze</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa_lambda.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>


<ul>
<li>Cartpole</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/cartpole%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>


<ul>
<li>Mountain car</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h3 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h3><h4 id="教程必备模块"><a href="#教程必备模块" class="headerlink" title="教程必备模块"></a>教程必备模块</h4><p>强化学习有一些现成的模块可以使用，但是那些模块并不全面，而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学习，可能 RL 的代码就不同。所以我们要抱着以不变应万变的心态，用基础的模块，从基础学起。懂了原理，再复杂的环境也不在话下。</p>
<p>所以用到的模块和对应的教程:</p>
<ul>
<li><a href="https://mofanpy.com/tutorials/data-manipulation/np-pd/">Numpy, Pandas</a> (必学), 用于学习的数据处理</li>
<li><a href="https://mofanpy.com/tutorials/data-manipulation/plt/">Matplotlib</a> (可学), 偶尔会用来呈现误差曲线什么的</li>
<li><a href="https://mofanpy.com/tutorials/python-basic/tkinter/">Tkinter</a> (可学), 你可以自己用它来编写模拟环境</li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow</a> (可学), 后面实现神经网络与强化学习结合的时候用到</li>
<li><a href="https://gym.openai.com/">OpenAI gym</a> (可学), 提供了很多现成的模拟环境</li>
</ul>
<h4 id="快速了解强化学习"><a href="#快速了解强化学习" class="headerlink" title="快速了解强化学习"></a>快速了解强化学习</h4><p>我也会制作每种强化学习对应的简介视频 (在这个学习列表里: <a href="https://mofanpy.com/tutorials/machine-learning/ML-intro/">有趣的机器学习</a>)，大家可以只花很少的时间来观看了解这些学习方法的不同之处。有了一定概念和基础，我们在这套教材里实现起来就容易多了。而且不懂的时候也能只花很少的时间回顾就行。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>XCTF-逆向基础篇</title>
    <url>/YingYingMonstre.github.io/2021/09/26/XCTF-%E9%80%86%E5%90%91%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
    <content><![CDATA[<h2 id="insanity"><a href="#insanity" class="headerlink" title="insanity"></a>insanity</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/1.png" alt="img"></p>
<p>发现这不是PE文件,是ELF文件，将程序在Linux环境下运行</p>
<p>0x02,查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/2.png" alt="img"></p>
<p>发现是32位的程序</p>
<p>0x03.IDA </p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/3.png" alt="img"></p>
<p>F5查看伪函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/4.png" alt="img"></p>
<p>发现一个关键的字符串，&amp;strs,发现是取这个字符串输出，然后，跟进strs</p>
<p>(shift + f12 字符串窗口)</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/5.png" alt="img"></p>
<p>发现有一个明显的提示：This_is_a_flag</p>
<p>猜测9447{This_is_a_flag}是最后的falg</p>
<p>或者直接用记事本打开，仔细找也能找到。</p>
<h2 id="python-trade"><a href="#python-trade" class="headerlink" title="python-trade"></a>python-trade</h2><p><strong>[工具]</strong></p>
<p>在线python反编译</p>
<p><strong>[分析过程]</strong></p>
<p>0x01.下载附件</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/1.png" alt="img"></p>
<p>注：</p>
<p>pyc文件是py文件编译后生成的字节码文件</p>
<p>0x02.在线Python反编译</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/2.png" alt="img"></p>
<p>这是生成的py文件</p>
<p>然后，对这个文件的运算逻辑进行逆向</p>
<p>0x03.写EXP</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line">correct = <span class="string">&quot;XlNkVmtUI1MgXWBZXCFeKY+AaXNt&quot;</span></span><br><span class="line">s = base64.b64decode(correct)</span><br><span class="line">flag = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">    flag += <span class="built_in">chr</span>((i-<span class="number">16</span>) ^ <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(flag)</span><br></pre></td></tr></table></figure>

<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/3.png" alt="img"></p>
<p>先对字符串进行b64decode,然后，再进行xor运算得到最后的flag:nctf{d3c0mpil1n9_PyC}</p>
<p>0x04.运行脚本</p>
<p>nctf{d3c0mpil1n9_PyC}</p>
<h2 id="re1"><a href="#re1" class="headerlink" title="re1"></a>re1</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>参考</p>
<p>0x01.运行程序</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/1.png" alt="img"></p>
<p>可以看到需要输入正确的flag</p>
<p>那么现在，我们需要判断程序是多少位的，有没有加壳</p>
<p>0x02.exeinfope查详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/2.png" alt="img"></p>
<p>可以看到程序是32位的，是Microsoft Visual c++编译的，并且没有加壳</p>
<p>注：查壳工具还有PEID，EID，但是推荐EID或者exeinfope，因为，PEID查壳的时候有时候不准确</p>
<p>那么，我们可以用静态分析神器 IDA 打开，进一步分析了</p>
<p>0x03.</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/3.png" alt="img"></p>
<p>然后，查找主函数main,可以看到右侧的是反汇编的汇编代码，这时候，我们可以直接分析汇编语言，但是，汇编语言看起来太多，费劲。这个时候就可以是有IDA是最强大的功能F5了，它能够直接将汇编代码生成C语言代码，虽然和这个程序的源码不完全一样，但是逻辑关系是一样的</p>
<p>F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/4.png" alt="img"></p>
<p>这是整个main函数的运算逻辑</p>
<p>可以看到一个关键的字符串，print(aFlag)，那么证明这就是输入正确flag，然后，会输出aFlag证明你的flag正确，然后，继续往上分析，可以看到v3的值，是由strcmp()决定的，比较v5和输入的字符串，如果一样就会进入后面的if判断，所以,我们继续往上分析，看看哪里又涉及v5，可以看到开头的_mm_storeu_si128(），对其进行分析发现它类似于memset(),将xmmword_413E34的值赋值给v5，所以，我们可以得到正确的flag应该在xmmword_413E34中，然后，我们双击413E34进行跟进</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/5.png" alt="img"></p>
<p>可以看到一堆十六进制的数</p>
<p>这时，我们使用IDA的另一个功能 R ，能够将十六进制的数转换为字符串。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/6.png" alt="img"></p>
<p>这就是我们最后的flag了</p>
<p>注：这里要跟大家普及一个知识了，及大端与小端</p>
<p>假设一个十六进制数0x12345678</p>
<p>大端的存储方式是：12,34,56,78，然后读取的时候也是从前往后读</p>
<p>小端的存储方式是：78,56,34,12，然后读取的时候是从后往前读取</p>
<p>所以，最后的flag应该是：DUTCTF{We1c0met0DUTCTF}</p>
<p>0x04.运行程序输入正确的flag</p>
<p>方法二：记事本打开，也能找到。</p>
<p>方法三：OD–》插件–》中文搜索引擎–》ASCII也能搜到。</p>
<h2 id="game"><a href="#game" class="headerlink" title="game"></a>game</h2><p>我的做法（暴力破解）：先用exeinfo pe查壳，发现是32位未加壳。用OD打开，用插件里的中文搜索ASCII，能找到done flag is的字符串。然后找到关键CALL（00B301BB和00B30359），修改跳转（从%d那里直接改为CALL xxxx跳到done那里）。运行到那里之后就可以得到flag了。</p>
<p>直接玩游戏：从1输到8。</p>
<p>爆破方法二：找到F5后伪代码最后那部分判断的代码（空格切换到图形视图，对着最后的那部分再空格切换回来），patch修改，正好有8个JNZ，改5个为jz，然后Edit–》Patch program–》Apply patches to input file，点OK，再回去运行就可以得到了。</p>
<p>IDA分析代码逻辑：先是判断是输入的是否是1-8，然后进入后面的if判断然后进行循环，这个时候应该就是程序的亮暗的显示，然后，如果byte_532E28每一位都是1，那么，就会进入sub_457AB4,然后我们猜测这里应该就是最后的flag的地方。然后跟进 sub_457AB4。（注：这里说明一下，如果IDA不能正确的获得自定义函数的名字，那么IDA会用sub__加上自定义函数的起始地址来定义函数的名字）</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/8.png" alt="img"></p>
<p>这里只截取了后面的部分，发现函数进行了两次xor运算，xor的逆运算也是xor，那么我们就可以根据这个运算来写脚本得到最后的flag。</p>
<p>这里看到v2和v59这就证明了这是两个数组的运算，所以我们应该将上面的字符串分成两个数组，分别从v2和v59开始</p>
<p>0x05.写EXP</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/9.png" alt="img"></p>
<p>这里先是通过循环，将a和b数组的值进行xor运算，然后再将数组a的值与0x13xor运算</p>
<p>chr()：是将十六进制转换为字符串</p>
<p>0x05.运行脚本</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/10.png" alt="img"></p>
<p>得到最后的flag: zsctf{T9is_tOpic_1s_v5ry_int7resting_b6t_others_are_n0t}。</p>
<p>广度优先搜索法：未证。</p>
<h2 id="Hello，CTF"><a href="#Hello，CTF" class="headerlink" title="Hello，CTF"></a>Hello，CTF</h2><p>查壳，32位、无壳。</p>
<p>IDA从main开始分析，F5查看伪代码。首先，可以看到先是将字符串复制到v13的位置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">strcpy(&amp;v13, &quot;437261636b4d654a757374466f7246756e&quot;);</span><br></pre></td></tr></table></figure>

<p>然后，后面对输入进行了判断，输入的字符串不能大于17接着，将字符串以十六进制输出，然后，再将得到的十六进制字符添加到v10最后，进行比较，看输入的字符串是否和v10的字符串相等，如果相等，则得到真确的flag。最后将字符串转换为十六进制。</p>
<h2 id="open-source"><a href="#open-source" class="headerlink" title="open-source"></a>open-source</h2><p>代码审计：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">first = <span class="number">0xcafe</span></span><br><span class="line">flag = first * <span class="number">31337</span> + (second % <span class="number">17</span>) * <span class="number">11</span> + <span class="built_in">len</span>(<span class="string">&quot;h4cky0u&quot;</span>) - <span class="number">1615810207</span></span><br></pre></td></tr></table></figure>

<p>关键在于second的取值。观察代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> second = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">if</span> (second % <span class="number">5</span> == <span class="number">3</span> || second % <span class="number">17</span> != <span class="number">8</span>) &#123;</span><br><span class="line">    	<span class="built_in">printf</span>(<span class="string">&quot;ha, you won&#x27;t get it!\n&quot;</span>);</span><br><span class="line">    	<span class="built_in">exit</span>(<span class="number">3</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>这里想到要get it就要将if里的逻辑取反，即second % 5 != 3 &amp;&amp; second % 17 == 8。</p>
<p>故(second % 17) * 11=8*11=88。得到flag=12648430–化为16进制–》c0ffee。</p>
<h2 id="simple-unpack"><a href="#simple-unpack" class="headerlink" title="simple_unpack"></a>simple_unpack</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, upx </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/1.png" alt="img"></p>
<p>发现有upx壳。</p>
<p>注：windows下的文件是PE文件，Linux/Unix下的文件是ELF文件</p>
<p>0x02.UPX 脱壳</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/2.png" alt="img"></p>
<p>upx -d 即可对upx壳进行脱壳</p>
<p>0x03.载入IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/3.png" alt="img"></p>
<p>还是从main函数开始分析，结果我们再右侧发现了意外惊喜</p>
<p>运行程序，输入我们看到的flag:flag{Upx_1s_n0t_a_d3liv3r_c0mp4ny}</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/4.png" alt="img"></p>
<h2 id="logmein"><a href="#logmein" class="headerlink" title="logmein"></a>logmein</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/1.png" alt="img"></p>
<p>发现程序是一个ELF文件，将其放入Linux环境中进行分析</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/2.png" alt="img"></p>
<p>发现程序是64位的，使用静态分析工具IDA进行分析</p>
<p>0x02.IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/3.png" alt="img"></p>
<p>从main函数开始分析，使用F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/4.png" alt="img"></p>
<p>发现main函数的整个运算逻辑</p>
<p>先是，将指定字符串复制到v8</p>
<p>s是用户输入的字符串，先进行比较长度，如果长度比v8小，则进入sub_4007c0函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/5.png" alt="img"></p>
<p>可以看出输出字符串Incorrect password,然后，退出</p>
<p>如果长度大于或等与v8则进入下面的循环</p>
<p>看到判断如果输入的字符串和经过运算后的后字符串不等，则进入sub_4007c0,输出Incorrect password,</p>
<p>如果想得，则进入sub_4007f0函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/6.png" alt="img"></p>
<p>证明输入的字符串就是flag</p>
<p>接下来写脚本</p>
<p>0x03.Write EXP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">v7 &#x3D; &quot;harambe&quot;</span><br><span class="line">v6 &#x3D; 7</span><br><span class="line">v8 &#x3D; &quot;:\&quot;AL_RT^L*.?+6&#x2F;46&quot;</span><br><span class="line">flag &#x3D; &quot;&quot;</span><br><span class="line">for i in range(0, len(v8)):</span><br><span class="line">    flag +&#x3D; chr(ord((v7[i % 7])) ^ ord(v8[i]))</span><br><span class="line">print(flag)</span><br></pre></td></tr></table></figure>

<p>由于程序是小段的存储方式，所以，ebmarah就得变成harambe（C语言数据在内存中是小端存储，一开始v7是一个数据，so）</p>
<p>ord():是将字符串转换为ascii格式，为了方便运算</p>
<p>chr():是将ascii转换为字符串</p>
<p>运行脚本得到最后的flag:RC3-2016-XORISGUD</p>
<p>（直接用C语言更方便些）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#define BYTE unsigned char</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void main() &#123;</span><br><span class="line">	int i;</span><br><span class="line">	int v6 &#x3D; 7;</span><br><span class="line">	__int64 v7 &#x3D; 28537194573619560LL;</span><br><span class="line">	char v8[18] &#x3D; &quot;:\&quot;AL_RT^L*.?+6&#x2F;46&quot;;</span><br><span class="line">	char s[18] &#x3D; &quot;&quot;;</span><br><span class="line">	for ( i &#x3D; 0; i &lt; strlen(v8); ++i ) &#123;</span><br><span class="line">		s[i] +&#x3D; (char)(*((BYTE*)&amp;v7 + i % v6) ^ v8[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	printf(&quot;%s\n&quot;,s);</span><br><span class="line"></span><br><span class="line">	system(&quot;pause&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="no-strings-attached"><a href="#no-strings-attached" class="headerlink" title="no-strings-attached"></a>no-strings-attached</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, GDB</p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/1.png" alt="img"></p>
<p>说明程序是ELF文件，32位</p>
<p>0x02.使用静态分析工具IDA进行分析</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/2.png" alt="img"></p>
<p>然后对main函数使用F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/3.png" alt="img"></p>
<p>然后，对每个函数进行跟进，最后发现authenricate(),符合获得flag的函数，对其进行跟进</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/4.png" alt="img"></p>
<p>然后我们发现一个特殊的函数decrypt,根据字面的意思是加密，那么我们可以大概的猜测是一个对dword_8048A90所对应的字符串进行加密，</p>
<p>加密得到的就应该是我们需要的flag，后面的判断应该就是将字符串输出。</p>
<p>这里我们有两种思维方式:</p>
<p>第一种就是跟进decrypt然后分析它的运算逻辑，然后，自己写脚本，得到最后的flag</p>
<p>第二种就涉及逆向的另一种调试方式，及动态调试，这里我就用动态调试了，之前的一直是静态调试</p>
<p>0x03.GDB动态调试</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/5.png" alt="img"></p>
<p>gdb ./no_strings_attached 将文件加载到GDB中</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/6.png" alt="img"></p>
<p>既然是动态调试，那么如果让它一直不停，那我不就相当于运行了嘛，所以，我们就需要下断点，断点就是让程序运行到断点处就停止</p>
<p>之前通过IDA，我们知道关键函数是decrypt,所以我们把断点设置在decrypt处，b在GDB中就是下断点的意思，及在decrypt处下断点</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/7.png" alt="img"></p>
<p>r就是运行的意思，这里运行到了我们之前下的断点处，停止。</p>
<p>我们要的是经过decrypt函数，生成的字符串，所以我们这里就需要运行一步，GDB中用n来表示运行一步</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/8.png" alt="img"></p>
<p>然后我们就需要去查看内存了，去查找最后生成的字符串</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/9.png" alt="img"></p>
<p>通过IDA生成的汇编指令，我们可以看出进过decrypt函数后，生成的字符串保存在EAX寄存器中，所以，我们在GDB就去查看eax寄存器的值</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/10.png" alt="img"></p>
<p>x:就是用来查看内存中数值的，后面的200代表查看多少个，wx代表是以word字节查看看，$eax代表的eax寄存器中的值</p>
<p>在这里我们看到0x00000000，这就证明这个字符串结束了，因为，在C中，代表字符串结尾的就是”\0”,那么前面的就是经过decrypt函数生成的falg</p>
<p>那我们就需要将这些转换为字符串的形式</p>
<p>0x04.Write EXP</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/11.png" alt="img"></p>
<p>首先将寄存器中的值提取出来，然后利用Python的decode函数，通过”hex”的方式转化为字符串，然后输出</p>
<p>0x05.运行脚本</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/12.png" alt="img"></p>
<p>得到最后的flag: 9447{you_are_an_international_mystery}</p>
<p>IDA分析就分析它的decrypt函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; [0x3a, 0x36, 0x37, 0x3b, 0x80, 0x7a, 0x71, 0x78,</span><br><span class="line">     0x63, 0x66, 0x73, 0x67, 0x62, 0x65, 0x73, 0x60,</span><br><span class="line">     0x6b, 0x71, 0x78, 0x6a, 0x73, 0x70, 0x64, 0x78,</span><br><span class="line">     0x6e, 0x70, 0x70, 0x64, 0x70, 0x64, 0x6e, 0x7b,</span><br><span class="line">     0x76, 0x78, 0x6a, 0x73, 0x7b, 0x80]</span><br><span class="line">v6 &#x3D; len(s)</span><br><span class="line">a2 &#x3D; [1, 2, 3, 4, 5]</span><br><span class="line">v7 &#x3D; len(a2)</span><br><span class="line">v2 &#x3D; v6</span><br><span class="line">dest &#x3D; s</span><br><span class="line">v4 &#x3D; 0</span><br><span class="line">while v4 &lt; v6:</span><br><span class="line">        dest[v4] -&#x3D; a2[v4 % 5]</span><br><span class="line">        v4 +&#x3D; 1</span><br><span class="line"></span><br><span class="line">flag &#x3D; &#39;&#39;</span><br><span class="line">for j in dest:</span><br><span class="line">    flag +&#x3D; chr(j)</span><br><span class="line">print(flag)</span><br></pre></td></tr></table></figure>



<h2 id="getit"><a href="#getit" class="headerlink" title="getit"></a>getit</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, GDB  </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/1.png" alt="img"></p>
<p>可以看出这是一个ELF文件，64位</p>
<p>0x02.IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/2.png" alt="img"></p>
<p>对main函数进行F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/3.png" alt="img"></p>
<p>可以看到先判断v5是否大于s存储字符串的长度，然后通过运算，最后将得到的flag写入文件。</p>
<p>但是有意思的地方在flag.txt文件所在的位置是/tmp目录，这个目录是Linux下的临时文件夹，程序运行完，生成flag的txt文件被清理了，所以我们找不到文件</p>
<p>我们这时候通过IDA查看汇编代码，按空格键可以生成所有的汇编文件</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/4.png" alt="img"></p>
<p>然后我们向下追踪，追踪到for循环的位置，因为，flag是在这里存入文件的，所以，我们可以在内存中找到正要存储的字符串</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/5.png" alt="img"></p>
<p>我们将鼠标指向strlen(),在下面可以看到汇编所在的地址，然后我们根据大概的地址去看汇编代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/6.png" alt="img"></p>
<p>可以看到这是调用strlen()函数的汇编指令</p>
<p>我们通过上一个图片，可以知道经过for()的判断条件后，还要进行一步fseek函数，所以，根据汇编代码，可以确定jnb loc_4008B5就是fseek()函数，那么，mov eax,[rbp+var_3C]肯定就是最后要得到的flag了</p>
<p>0x04.GDB</p>
<p>这里我们用linux下的动态调试工具gdb进行动态调试，这里介绍一下，对gdb进行强化的两个工具peda和pwndbg，这两个工具可以强化视觉效果，可以更加清楚的显示堆栈，内存，寄存机的情况</p>
<p>先加载程序</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/7.png" alt="img"></p>
<p>然后，用b 下断点</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/8.png" alt="img"></p>
<p>然后，运行 R</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/9.png" alt="img"></p>
<p>这里我们可以看出，程序停止在0x400832的位置，然后，要被移动的字符串在RDX的位置</p>
<p>注：</p>
<p>这里介绍一下一下RDX，RDX存的是i/0指针，0x6010e0,这个位置存的字符串是最后的flag:SharifCTF{b70c59275fcfa8aebf2d5911223c6589}</p>
<p>以为这里涉及的是程序读写函数，所以涉及的就是i/o指针</p>
<p>所以我们能得到最后的flag: SharifCTF{b70c59275fcfa8aebf2d5911223c6589}</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; &quot;c61b68366edeb7bdce3c6820314b7498&quot;</span><br><span class="line">v3 &#x3D; 0</span><br><span class="line">v5 &#x3D; 0</span><br><span class="line">t &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">while v5 &lt; len(s):</span><br><span class="line">    if v5 &amp; 1:</span><br><span class="line">        v3 &#x3D; 1</span><br><span class="line">    else:</span><br><span class="line">        v3 &#x3D; -1</span><br><span class="line">    t +&#x3D; chr(ord(s[v5]) + v3)</span><br><span class="line">    v5 +&#x3D; 1</span><br><span class="line"></span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>



<h2 id="csaw2013reversing2"><a href="#csaw2013reversing2" class="headerlink" title="csaw2013reversing2"></a>csaw2013reversing2</h2><p><a href="https://blog.csdn.net/weixin_43784056/article/details/103655968">XCTF-csaw2013reversing2_臭nana的博客-CSDN博客</a></p>
<p>默认if条件不成立，跳过了sub_401000的解码，输出一堆乱码。</p>
<h2 id="maze"><a href="#maze" class="headerlink" title="maze"></a>maze</h2><p>迷宫问题</p>
<p><img src="https://www.pianshen.com/images/271/7d91ebf895b8ed648ed3ab80de463137.JPEG" alt="在这里插入图片描述"></p>
]]></content>
      <tags>
        <tag>XCTF</tag>
        <tag>逆向</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（QLearning）</title>
    <url>/YingYingMonstre.github.io/2021/11/08/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88QLearning%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-Q-Learning"><a href="#什么是-Q-Learning" class="headerlink" title="什么是 Q Learning"></a>什么是 Q Learning</h3><p>今天我们会来说说强化学习中一个很有名的算法——Q-learning。</p>
<h4 id="行为准则"><a href="#行为准则" class="headerlink" title="行为准则"></a>行为准则</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q1.png"></p>
<p>我们做事情都会有一个自己的行为准则，比如小时候爸妈常说”不写完作业就不准看电视”。所以我们在写作业的这种状态下，好的行为就是继续写作业，直到写完它，我们还可以得到奖励。不好的行为就是没写完就跑去看电视了，被爸妈发现，后果很严重。小时候这种事情做多了，也就变成我们不可磨灭的记忆。这和我们要提到的Q learning 有什么关系呢?原来 Q learning 也是一个决策过程，和小时候的这种情况差不多。我们举例说明。</p>
<p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视，所以现在我们有两种选择：1.继续写作业，2. 跑去看电视。因为以前没有被罚过，所以我选看电视，然后现在的状态变成了看电视，我又选了继续看电视，接着我还是看电视。最后爸妈回家，发现我没写完作业就去看电视了，狠狠地惩罚了我一次，我也深刻地记下了这一次经历，并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为，我们在看看 Q learning 根据很多这样的经历是如何来决策的吧。</p>
<h4 id="QLearning-决策"><a href="#QLearning-决策" class="headerlink" title="QLearning 决策"></a>QLearning 决策</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q2.png"></p>
<p>假设我们的行为准则已经学习好了，现在我们处于状态s1，我在写作业，我有两个行为 a1、a2，分别是看电视和写作业，根据我的经验，在这种 s1 状态下, a2 写作业带来的潜在奖励要比 a1 看电视高，这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替，在我的记忆Q表格中, Q(s1, a1)=-2 要小于 Q(s1, a2)=1，所以我们判断要选择 a2 作为下一个行为。现在我们的状态更新成 s2，我们还是有两个同样的选择，重复上面的过程，在行为准则Q表中寻找 Q(s2, a1) Q(s2, a2) 的值, 并比较他们的大小，选取较大的一个。接着根据 a2 我们到达 s3 并在此重复上面的决策过程。Q learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改，提升的。</p>
<h4 id="QLearning-更新"><a href="#QLearning-更新" class="headerlink" title="QLearning 更新"></a>QLearning 更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q3.png"></p>
<p>所以我们回到之前的流程，根据 Q 表的估计，因为在 s1 中, a2 的值比较大，通过之前的决策方法，我们在 s1 采取了 a2，并到达 s2，这时我们开始更新用于决策的 Q 表，接着我们并没有在实际中采取任何行为，而是想象自己在 s2 上采取了每种行为，分别看看两种行为哪一个的 Q 值大。比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0)，因为会获取实实在在的奖励 R，我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值。所以有了现实和估计值，我们就能更新Q(s1, a2)，根据估计与现实的差距，将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值 变成新的值。但时刻记住，我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做。这就是 off-policy 的 Q learning 是如何决策和学习优化决策的过程。</p>
<h4 id="QLearning-整体算法"><a href="#QLearning-整体算法" class="headerlink" title="QLearning 整体算法"></a>QLearning 整体算法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q4.png"></p>
<p>这一张图概括了我们之前所有的内容。这也是 Q learning 的算法，每次更新我们都用到了 Q 现实和 Q 估计，而且 Q learning 的迷人之处就是在 Q(s1, a2) 现实中，也包含了一个 Q(s2) 的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实，很奇妙吧。最后我们来说说这套算法中一些参数的意义。Epsilon greedy 是用在决策上的一种策略，比如 epsilon = 0.9 时，就说明有90% 的情况我会按照 Q 表的最优值选择行为，10% 的时间使用随机选行为。alpha是学习率，来决定这次的误差有多少是要被学习的，alpha是一个小于1的数。gamma 是对未来 reward 的衰减值。我们可以这样想象。</p>
<h4 id="QLearning-中的-Gamma"><a href="#QLearning-中的-Gamma" class="headerlink" title="QLearning 中的 Gamma"></a>QLearning 中的 Gamma</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q5.png"></p>
<p>我们重写一下 Q(s1) 的公式，将 Q(s2) 拆开，因为Q(s2)可以像 Q(s1)一样，是关于Q(s3) 的，所以可以写成这样。然后以此类推，不停地这样写下去，最后就能写成这样，可以看出Q(s1) 是有关于之后所有的奖励，但这些奖励正在衰减，离 s1 越远的状态衰减越严重。不好理解? 行，我们想象 Qlearning 的机器人天生近视眼，gamma = 1 时，机器人有了一副合适的眼镜，在 s1 看到的 Q 是未来没有任何衰变的奖励，也就是机器人能清清楚楚地看到之后所有步的全部价值；但是当 gamma =0，近视机器人没了眼镜，只能摸到眼前的 reward，同样也就只在乎最近的大奖励；如果 gamma 从 0 变到 1，眼镜的度数由浅变深，对远处的价值看得越清楚，所以机器人渐渐变得有远见，不仅仅只看眼前的利益，也为自己的未来着想。</p>
<h3 id="小例子"><a href="#小例子" class="headerlink" title="小例子"></a>小例子</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>这一次我们会用 tabular Q-learning 的方法实现一个小例子，例子的环境是一个一维世界，在世界的右边有宝藏，探索者只要得到宝藏尝到了甜头，然后以后就记住了得到宝藏的方法，这就是他用强化学习所学习到的行为。</p>
<blockquote>
<p>-o—T</p>
<p>T 就是宝藏的位置, o 是探索者的位置</p>
</blockquote>
<p>Q-learning 是一种记录行为值 (Q value) 的方法，每种在一定状态的行为都会有一个值 <code>Q(s, a)</code>，就是说 行为 <code>a</code> 在 <code>s</code> 状态的值是 <code>Q(s, a)</code>。<code>s</code> 在上面的探索者游戏中，就是 <code>o</code> 所在的地点了。而每一个地点探索者都能做出两个行为 <code>left/right</code>，这就是探索者的所有可行的 <code>a</code> 啦。</p>
<p>如果在某个地点 <code>s1</code>，探索者计算了他能有的两个行为, <code>a1/a2=left/right</code>，计算结果是 <code>Q(s1, a1) &gt; Q(s1, a2)</code>，那么探索者就会选择 <code>left</code> 这个行为。这就是 Q learning 的行为选择简单规则。</p>
<p>==当然我们还会细说更具体的规则。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容，大家大概看看就行，有个大概的 RL 概念就行，知道 RL 的一些关键步骤就行，这节的算法不用仔细研究。==</p>
<h4 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h4><p>这一次需要的模块和参数设置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">N_STATES = <span class="number">6</span>   <span class="comment"># 1维世界的宽度</span></span><br><span class="line">ACTIONS = [<span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;right&#x27;</span>]     <span class="comment"># 探索者的可用动作</span></span><br><span class="line">EPSILON = <span class="number">0.9</span>   <span class="comment"># 贪婪度 greedy</span></span><br><span class="line">ALPHA = <span class="number">0.1</span>     <span class="comment"># 学习率</span></span><br><span class="line">GAMMA = <span class="number">0.9</span>    <span class="comment"># 奖励递减值</span></span><br><span class="line">MAX_EPISODES = <span class="number">13</span>   <span class="comment"># 最大回合数</span></span><br><span class="line">FRESH_TIME = <span class="number">0.3</span>    <span class="comment"># 移动间隔时间</span></span><br></pre></td></tr></table></figure>



<h4 id="Q-表"><a href="#Q-表" class="headerlink" title="Q 表"></a>Q 表</h4><p>对于 tabular Q learning，我们必须将所有的 Q values (行为值) 放在 <code>q_table</code> 中，更新 <code>q_table</code> 也是在更新他的行为准则。<code>q_table</code> 的 index 是所有对应的 <code>state</code> (探索者位置)，columns 是对应的 <code>action</code> (探索者行为)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_q_table</span>(<span class="params">n_states, actions</span>):</span></span><br><span class="line">    table = pd.DataFrame(</span><br><span class="line">        np.zeros((n_states, <span class="built_in">len</span>(actions))),     <span class="comment"># q_table 全 0 初始</span></span><br><span class="line">        columns=actions,    <span class="comment"># columns 对应的是行为名称</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> table</span><br><span class="line"></span><br><span class="line"><span class="comment"># q_table:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   left  right</span></span><br><span class="line"><span class="string">0   0.0    0.0</span></span><br><span class="line"><span class="string">1   0.0    0.0</span></span><br><span class="line"><span class="string">2   0.0    0.0</span></span><br><span class="line"><span class="string">3   0.0    0.0</span></span><br><span class="line"><span class="string">4   0.0    0.0</span></span><br><span class="line"><span class="string">5   0.0    0.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="定义动作"><a href="#定义动作" class="headerlink" title="定义动作"></a>定义动作</h4><p>接着定义探索者是如何挑选行为的。这是我们引入 <code>epsilon greedy</code> 的概念。因为在初始阶段，随机的探索环境，往往比固定的行为模式要好，所以这也是累积经验的阶段，我们希望探索者不会那么贪婪(greedy)。所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。<code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪)，不过在这个例子中，我们就固定成 <code>EPSILON = 0.9</code>，90% 的时间是选择最优策略，10% 的时间来探索。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在某个 state 地点, 选择行为</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">state, q_table</span>):</span></span><br><span class="line">    state_actions = q_table.iloc[state, :]  <span class="comment"># 选出这个 state 的所有 action 值</span></span><br><span class="line">    <span class="keyword">if</span> (np.random.uniform() &gt; EPSILON) <span class="keyword">or</span> (state_actions.<span class="built_in">all</span>() == <span class="number">0</span>):  <span class="comment"># 非贪婪 or 或者这个 state 还没有探索过</span></span><br><span class="line">        action_name = np.random.choice(ACTIONS)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># action_name = state_actions.argmax()不能正常运行</span></span><br><span class="line">        <span class="comment"># argmax()返回的是序列中最大值的int位置</span></span><br><span class="line">        <span class="comment"># idxmax()返回的是最大值的行标签</span></span><br><span class="line">        action_name = state_actions.idxmax()    <span class="comment"># 贪婪模式</span></span><br><span class="line">    <span class="keyword">return</span> action_name</span><br></pre></td></tr></table></figure>



<h4 id="环境反馈-S-R"><a href="#环境反馈-S-R" class="headerlink" title="环境反馈 S_, R"></a>环境反馈 S_, R</h4><p>做出行为后，环境也要给我们的行为一个反馈，反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R)。这里定义的规则就是，只有当 <code>o</code> 移动到了 <code>T</code>，探索者才会得到唯一的一个奖励，奖励值 R=1，其他情况都没有奖励。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_env_feedback</span>(<span class="params">S, A</span>):</span></span><br><span class="line">    <span class="comment"># This is how agent will interact with the environment</span></span><br><span class="line">    <span class="keyword">if</span> A == <span class="string">&#x27;right&#x27;</span>:    <span class="comment"># move right</span></span><br><span class="line">        <span class="keyword">if</span> S == N_STATES - <span class="number">2</span>:   <span class="comment"># terminate</span></span><br><span class="line">            S_ = <span class="string">&#x27;terminal&#x27;</span></span><br><span class="line">            R = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            S_ = S + <span class="number">1</span></span><br><span class="line">            R = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:   <span class="comment"># move left</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> S == <span class="number">0</span>:</span><br><span class="line">            S_ = S  <span class="comment"># reach the wall</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            S_ = S - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> S_, R</span><br></pre></td></tr></table></figure>



<h4 id="环境更新"><a href="#环境更新" class="headerlink" title="环境更新"></a>环境更新</h4><p>接下来就是环境的更新了，不用细看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_env</span>(<span class="params">S, episode, step_counter</span>):</span></span><br><span class="line">    <span class="comment"># This is how environment be updated</span></span><br><span class="line">    env_list = [<span class="string">&#x27;-&#x27;</span>]*(N_STATES-<span class="number">1</span>) + [<span class="string">&#x27;T&#x27;</span>]   <span class="comment"># &#x27;---------T&#x27; our environment</span></span><br><span class="line">    <span class="keyword">if</span> S == <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">        interaction = <span class="string">&#x27;Episode %s: total_steps = %s&#x27;</span> % (episode+<span class="number">1</span>, step_counter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(interaction), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r                                &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        env_list[S] = <span class="string">&#x27;o&#x27;</span></span><br><span class="line">        interaction = <span class="string">&#x27;&#x27;</span>.join(env_list)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(interaction), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        time.sleep(FRESH_TIME)</span><br></pre></td></tr></table></figure>



<h4 id="强化学习主循环"><a href="#强化学习主循环" class="headerlink" title="强化学习主循环"></a>强化学习主循环</h4><p>最重要的地方就在这里。你定义的 RL 方法都在这里体现。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容，大家大概看看就行，这节内容不用仔细研究。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/2-1-1.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rl</span>():</span></span><br><span class="line">    q_table = build_q_table(N_STATES, ACTIONS)  <span class="comment"># 初始 q table</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPISODES):     <span class="comment"># 回合</span></span><br><span class="line">        step_counter = <span class="number">0</span></span><br><span class="line">        S = <span class="number">0</span>   <span class="comment"># 回合初始位置</span></span><br><span class="line">        is_terminated = <span class="literal">False</span>   <span class="comment"># 是否回合结束</span></span><br><span class="line">        update_env(S, episode, step_counter)    <span class="comment"># 环境更新</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_terminated:</span><br><span class="line"></span><br><span class="line">            A = choose_action(S, q_table)   <span class="comment"># 选行为</span></span><br><span class="line">            S_, R = get_env_feedback(S, A)  <span class="comment"># 实施行为并得到环境的反馈</span></span><br><span class="line">            q_predict = q_table.loc[S, A]    <span class="comment"># 估算的(状态-行为)值</span></span><br><span class="line">            <span class="keyword">if</span> S_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">                q_target = R + GAMMA * q_table.iloc[S_, :].<span class="built_in">max</span>()   <span class="comment">#  实际的(状态-行为)值 (回合没结束)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                q_target = R     <span class="comment">#  实际的(状态-行为)值 (回合结束)</span></span><br><span class="line">                is_terminated = <span class="literal">True</span>    <span class="comment"># terminate this episode</span></span><br><span class="line"></span><br><span class="line">            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  <span class="comment">#  q_table 更新</span></span><br><span class="line">            S = S_  <span class="comment"># 探索者移动到下一个 state</span></span><br><span class="line"></span><br><span class="line">            update_env(S, episode, step_counter+<span class="number">1</span>)  <span class="comment"># 环境更新</span></span><br><span class="line"></span><br><span class="line">            step_counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> q_table</span><br></pre></td></tr></table></figure>

<p>写好所有的评估和更新准则后，我们就能开始训练了，把探索者丢到环境中，让它自己去玩吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    q_table = rl()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\r\nQ-table:\n&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(q_table)</span><br></pre></td></tr></table></figure>



<h3 id="Q-learning-算法更新"><a href="#Q-learning-算法更新" class="headerlink" title="Q-learning 算法更新"></a>Q-learning 算法更新</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>上次我们知道了 RL 之中的 Q-learning 方法是在做什么事，今天我们就来说说一个更具体的例子。让探索者学会走迷宫。黄色的是天堂 (reward 1)，黑色的地狱 (reward -1)。大多数 RL 是由 reward 导向的，所以定义 reward 是 RL 中比较重要的一点。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20q.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>




<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/2-1-1.png"></p>
<p>整个算法就是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。Qlearning 是一个 off-policy 的算法，因为里面的 <code>max</code> action 让 Q table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验，甚至是学习他人的经验)。不过这一次的例子，我们没有运用到 off-policy，而是把 Qlearning 用在了 on-policy 上，也就是现学现卖，将现在经历的直接当场学习并运用。On-policy 和 off-policy 的差别我们会在之后的 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">Deep Q network (off-policy)</a> 学习中见识到。而之后的教程也会讲到一个 on-policy (Sarsa) 的形式，我们之后再对比。</p>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>首先我们先 import 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py">这里下载</a>，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。我也有<a href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的。而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> QLearningTable</span><br></pre></td></tr></table></figure>

<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Qlearning 最重要的迭代更新部分啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>



<h3 id="Q-learning-思维决策"><a href="#Q-learning-思维决策" class="headerlink" title="Q-learning 思维决策"></a>Q-learning 思维决策</h3><p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>QLearningTable</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>与上回不一样的地方是，我们将要以一个 class 形式定义 Q learning，并把这种 tabular q learning 方法叫做 <code>QLearningTable</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="预设值-1"><a href="#预设值-1" class="headerlink" title="预设值"></a>预设值</h4><p>初始的参数意义不会在这里提及了，请参考这个快速了解通道 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/tabular-q2/#">机器学习系列-Q learning</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        self.actions = actions  <span class="comment"># a list</span></span><br><span class="line">        self.lr = learning_rate <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = reward_decay   <span class="comment"># 奖励衰减</span></span><br><span class="line">        self.epsilon = e_greedy     <span class="comment"># 贪婪度</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   <span class="comment"># 初始 q_table</span></span><br></pre></td></tr></table></figure>



<h4 id="决定行为"><a href="#决定行为" class="headerlink" title="决定行为"></a>决定行为</h4><p>这里是定义如何根据所在的 state，或者是在这个 state 上的 观测值 (observation) 来决策。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        self.check_state_exist(observation) <span class="comment"># 检测本 state 是否在 q_table 中存在(见后面标题内容)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选择 action</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:  <span class="comment"># 选择 Q value 最高的 action</span></span><br><span class="line">            state_action = self.q_table.loc[observation, :]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.<span class="built_in">max</span>(state_action)].index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 随机选择 action</span></span><br><span class="line">            action = np.random.choice(self.actions)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>同<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/general-rl">上一个简单的 q learning 例子</a>一样，我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更行 <code>q_table</code>。更新的方式是不是很熟悉呢:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update &#x3D; self.lr * (q_target - q_predict)</span><br></pre></td></tr></table></figure>

<p>这可以理解成神经网络中的更新方式，学习率 * (真实值 - 预测值)。将判断误差传递回去，有着和神经网络更新的异曲同工之处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)  <span class="comment"># 检测 q_table 中是否存在 s_ (见后面标题内容)</span></span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()  <span class="comment"># 下个 state 不是 终止符</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># 下个 state 是终止符</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># 更新对应的 state-action 值</span></span><br></pre></td></tr></table></figure>



<h4 id="检测-state-是否存在"><a href="#检测-state-是否存在" class="headerlink" title="检测 state 是否存在"></a>检测 state 是否存在</h4><p>这个功能就是检测 <code>q_table</code> 中有没有当前 state 的步骤了，如果还没有当前 state，那我我们就插入一组全 0 数据，当做这个 state 的所有 action 初始 values。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            self.q_table = self.q_table.append(</span><br><span class="line">                pd.Series(</span><br><span class="line">                    [<span class="number">0</span>]*<span class="built_in">len</span>(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>

<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/1_command_line_reinforcement_learning/treasure_on_right.py">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/2_Q_Learning_maze">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">什么是 Q Learning 短视频</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（Sarsa）</title>
    <url>/YingYingMonstre.github.io/2021/11/10/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88Sarsa%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-Sarsa"><a href="#什么是-Sarsa" class="headerlink" title="什么是 Sarsa"></a>什么是 Sarsa</h3><p>今天我们会来说说强化学习中一个和 Q learning 类似的算法，叫做 Sarsa。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<p><img src="https://static.mofanpy.com/results/ML-intro/s1.png"></p>
<p>在强化学习中 Sarsa 和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 及其类似，这节内容会基于之前我们所讲的 Q learning。所以还不熟悉 Q learning 的朋友们，请前往我制作的 Q learning 简介 (知乎专栏)。我们会对比 Q learning，来看看 Sarsa 是特殊在哪些方面。和上次一样，我们还是使用写作业和看电视这个例子。没写完作业去看电视被打，写完了作业有糖吃。</p>
<h4 id="Sarsa-决策"><a href="#Sarsa-决策" class="headerlink" title="Sarsa 决策"></a>Sarsa 决策</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s2.png"></p>
<p>Sarsa 的决策部分和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩。但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<h4 id="Sarsa-更新行为准则"><a href="#Sarsa-更新行为准则" class="headerlink" title="Sarsa 更新行为准则"></a>Sarsa 更新行为准则</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s3.png"></p>
<p>同样，我们会经历正在写作业的状态 s1，然后再挑选一个带来最大潜在奖励的动作 a2，这样我们就到达了 继续写作业状态 s2，而在这一步，如果你用的是 Q learning，你会观看一下在 s2 上选取哪一个动作会带来最大的奖励，但是在真正要做决定时，却不一定会选取到那个带来最大奖励的动作，Q-learning 在这一步只是估计了一下接下来的动作值。而 Sarsa 是实践派，他说到做到，在 s2 这一步估算的动作也是接下来要做的动作。所以 Q(s1, a2) 现实的计算值，我们也会稍稍改动，去掉maxQ，取而代之的是在 s2 上我们实实在在选取的 a2 的 Q 值。最后像 Q learning 一样，求出现实和估计的差距 并更新 Q 表里的 Q(s1, a2)。</p>
<h4 id="对比-Sarsa-和-Qlearning-算法"><a href="#对比-Sarsa-和-Qlearning-算法" class="headerlink" title="对比 Sarsa 和 Qlearning 算法"></a>对比 Sarsa 和 Qlearning 算法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s4.png"></p>
<p>从算法来看，这就是他们两最大的不同之处了。因为 Sarsa 是说到做到型，所以我们也叫他 on-policy、在线学习，学着自己在做的事情。而 Q learning 是说到但并不一定做到，所以它也叫作 Off-policy、离线学习。而因为有了 maxQ，Q-learning 也是一个特别勇敢的算法。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/s5.png"></p>
<p>为什么说他勇敢呢，因为 Q learning 机器人永远都会选择最近的一条通往成功的道路，不管这条路会有多危险。而 Sarsa 则是相当保守，他会选择离危险远远的，拿到宝藏是次要的，保住自己的小命才是王道。这就是使用 Sarsa 方法的不同之处。</p>
<h3 id="Sarsa-算法更新"><a href="#Sarsa-算法更新" class="headerlink" title="Sarsa 算法更新"></a>Sarsa 算法更新</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>这次我们用同样的迷宫例子来实现 RL 中另一种和 Qlearning 类似的算法，叫做 Sarsa (state-action-reward-state*-action*)。我们从这一个简称可以了解到，Sarsa 的整个循环都将是在一个路径上，也就是 on-policy，下一个 state，和下一个 <em>action</em> 将会变成他真正采取的 action 和 state。和 Qlearning 的不同之处就在这。Qlearning 的下个一个 state_ action_ 在算法更新的时候都还是不确定的 (off-policy)。而 Sarsa 的 state，<em>action</em> 在这次算法更新的时候已经确定好了 (on-policy)。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-1-1.png"></p>
<p>整个算法还是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。不过于 Qlearning 不同之处：</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的 <code>action</code>，而且想好了下一个 <code>state_</code> 和下一个 <code>action_</code> (Qlearning 还没有想好下一个 <code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code> (Qlearning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<p>这种不同之处使得 Sarsa 相对于 Qlearning 更加的胆小。因为 Qlearning 永远都是想着 <code>maxQ</code> 最大化，因为这个 <code>maxQ</code> 而变得贪婪，不考虑其他非 <code>maxQ</code> 的结果。我们可以理解成 Qlearning 是一种贪婪、大胆、勇敢的算法，对于错误、死亡并不在乎。而 Sarsa 是一种保守的算法，他在乎每一步决策，对于错误和死亡比较铭感。这一点我们会在可视化的部分看出他们的不同。两种算法都有他们的好处，比如在实际中，你比较在乎机器的损害，用一种保守的算法，在训练时就能减少损坏的次数。</p>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>首先我们先 import 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/3_Sarsa_maze/maze_env.py">这里下载</a>。<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。我也有<a href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的。而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> SarsaTable</span><br></pre></td></tr></table></figure>

<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Sarsa 最重要的迭代更新部分啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sarsa 根据 state 观测选择行为</span></span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 根据下一个 state (obervation_) 选取下一个 action_</span></span><br><span class="line">            action_ = RL.choose_action(<span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个当成下一步的 state (observation) and action</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 终止时跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 大循环完毕</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = SarsaTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>

<p>下一节我们会来讲解 <code>SarsaTable</code> 这种算法具体要怎么编。</p>
<h3 id="Sarsa思维决策"><a href="#Sarsa思维决策" class="headerlink" title="Sarsa思维决策"></a>Sarsa思维决策</h3><h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>和之前定义 Qlearning 中的 <code>QLearningTable</code> 一样，因为使用 tabular 方式的 <code>Sarsa</code> 和 <code>Qlearning</code> 的相似度极高，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>

<p>我们甚至可以定义一个主class <code>RL</code>，然后将 <code>QLearningTable</code> 和 <code>SarsaTable</code> 作为 主class <code>RL</code> 的衍生，这个主 <code>RL</code> 可以这样定义。所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、<code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RL</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, action_space, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># 每种的都有点不同, 所以用 pass</span></span><br></pre></td></tr></table></figure>

<p>如果是这样定义父类的 <code>RL</code> class，通过继承关系，那之子类 <code>QLearningTable</code> class 就能简化成这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>(<span class="params">RL</span>):</span>   <span class="comment"># 继承了父类 RL</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    <span class="comment"># 表示继承关系</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span>   <span class="comment"># learn 的方法在每种类型中有不一样, 需重新定义</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r</span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>(<span class="params">RL</span>):</span>   <span class="comment"># 继承 RL class</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    <span class="comment"># 表示继承关系</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="comment"># q_target 基于选好的 a_ 而不是 Q(s_) 的最大值</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># 如果 s_ 是终止符</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># 更新 q_table</span></span><br></pre></td></tr></table></figure>



<h3 id="什么是-Sarsa-lambda"><a href="#什么是-Sarsa-lambda" class="headerlink" title="什么是 Sarsa(lambda)"></a>什么是 Sarsa(lambda)</h3><p>今天我们会来说说强化学习中基于 Sarsa 的一种提速方法，叫做 sarsa-lambda。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<h4 id="Sarsa-n"><a href="#Sarsa-n" class="headerlink" title="Sarsa(n)"></a>Sarsa(n)</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl1.png"></p>
<p>通过上个视频的介绍，我们知道这个 [Sarsa]/tutorials/machine-learning/reinforcement-learning/intro-sarsa)) 的算法是一种在线学习法，on-policy。但是这个 lambda 到底是什么。其实吧，Sarsa 是一种单步更新法，在环境中每走一步，更新一次自己的行为准则，我们可以在这样的 Sarsa 后面打一个括号，说他是 Sarsa(0)，因为他等走完这一步以后直接更新行为准则。如果延续这种想法，走完这步，再走一步，然后再更新，我们可以叫他 Sarsa(1)。同理，如果等待回合完毕我们一次性再更新呢，比如这回合我们走了 n 步，那我们就叫 Sarsa(n)。为了统一这样的流程，我们就有了一个 lambda 值来代替我们想要选择的步数，这也就是 Sarsa(lambda) 的由来。我们看看最极端的两个例子，对比单步更新和回合更新，看看回合更新的优势在哪里。</p>
<h4 id="单步更新-and-回合更新"><a href="#单步更新-and-回合更新" class="headerlink" title="单步更新 and 回合更新"></a>单步更新 and 回合更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl2.png"></p>
<p>虽然我们每一步都在更新，但是在没有获取宝藏的时候，我们现在站着的这一步也没有得到任何更新，也就是直到获取宝藏时，我们才为获取到宝藏的上一步更新为：这一步很好，和获取宝藏是有关联的，而之前为了获取宝藏所走的所有步都被认为和获取宝藏没关系。回合更新虽然我要等到这回合结束，才开始对本回合所经历的所有步都添加更新，但是这所有的步都是和宝藏有关系的，都是为了得到宝藏需要学习的步，所以每一个脚印在下回合被选则的几率又高了一些。在这种角度来看，回合更新似乎会有效率一些。</p>
<h4 id="有时迷茫"><a href="#有时迷茫" class="headerlink" title="有时迷茫"></a>有时迷茫</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl3.png"></p>
<p>我们看看这种情况，还是使用单步更新的方法在每一步都进行更新，但是同时记下之前的寻宝之路。你可以想像，每走一步，插上一个小旗子，这样我们就能清楚的知道除了最近的一步，找到宝物时还需要更新哪些步了。不过，有时候情况可能没有这么乐观。开始的几次，因为完全没有头绪，我可能在原地打转了很久，然后才找到宝藏，那些重复的脚步真的对我拿到宝藏很有必要吗？答案我们都知道。所以Sarsa(lambda)就来拯救你啦。</p>
<h4 id="Lambda-含义"><a href="#Lambda-含义" class="headerlink" title="Lambda 含义"></a>Lambda 含义</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl4.png"></p>
<p>其实 lambda 就是一个衰变值，他可以让你知道离奖励越远的步可能并不是让你最快拿到奖励的步，所以我们想象我们站在宝藏的位置，回头看看我们走过的寻宝之路，离宝藏越近的脚印越看得清，远处的脚印太渺小，我们都很难看清，那我们就索性记下离宝藏越近的脚印越重要，越需要被好好的更新。和之前我们提到过的奖励衰减值 gamma 一样，lambda 是脚步衰减值，都是一个在 0 和 1 之间的数。</p>
<h4 id="Lambda-取值"><a href="#Lambda-取值" class="headerlink" title="Lambda 取值"></a>Lambda 取值</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl5.png"></p>
<p>当 lambda 取0，就变成了 Sarsa 的单步更新，当 lambda 取 1，就变成了回合更新，对所有步更新的力度都是一样。当 lambda 在 0 和 1 之间，取值越大，离宝藏越近的步更新力度越大。这样我们就不用受限于单步更新的每次只能更新最近的一步，我们可以更有效率的更新所有相关步了。</p>
<h3 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa-lambda"></a>Sarsa-lambda</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>Sarsa-lambda 是基于 Sarsa 方法的升级版，他能更有效率地学习到怎么样获得好的 reward。如果说 Sarsa 和 Qlearning 都是每次获取到 reward，只更新获取到 reward 的前一步。那 Sarsa-lambda 就是更新获取到 reward 的前 lambda 步。lambda 是在 [0, 1] 之间取值，</p>
<p>如果 lambda = 0，Sarsa-lambda 就是 Sarsa，只更新获取到 reward 前经历的最后一步。</p>
<p>如果 lambda = 1，Sarsa-lambda 更新的是获取到 reward 前所有经历的步。</p>
<p>这样解释起来有点抽象，还是建议大家观看我制作的 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda">什么是 Sarsa-lambda 短视频</a>, 用动画展示具体的区别。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa_lambda.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="代码主结构-1"><a href="#代码主结构-1" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>使用 <code>SarsaLambdaTable</code> 在算法更新迭代的部分，是和之前的 <code>SarsaTable</code> 一样的，所以这一节，我们没有算法更新部分, 直接变成思维决策部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>

<p>同样, 我们选择继承的方式，将 <code>SarsaLambdaTable</code> 继承到 <code>RL</code>，所以我们将之前的 <code>__init__</code>，<code>check_state_exist</code>，<code>choose_action</code>，<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<p>算法的相应更改请参考这个：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-3-1.png"></p>
<h4 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h4><p>在预设值当中，我们添加了 <code>trace_decay=0.9</code> 这个就是 <code>lambda</code> 的值了。这个值将会使得拿到 reward 前的每一步都有价值。如果还不太明白其他预设值的意思，请查看我的 <a href="https://mofanpy.com/tutorials/machine-learning/ML-intro">关于强化学习的短视频列表</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 后向观测算法, eligibility trace.</span></span><br><span class="line">        self.lambda_ = trace_decay</span><br><span class="line">        self.eligibility_trace = self.q_table.copy()    <span class="comment"># 空的 eligibility trace 表</span></span><br></pre></td></tr></table></figure>



<h4 id="检测-state-是否存在"><a href="#检测-state-是否存在" class="headerlink" title="检测 state 是否存在"></a>检测 state 是否存在</h4><p><code>check_state_exist</code> 和之前的是高度相似的。唯一不同的地方是我们考虑了 <code>eligibility_trace</code>，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            to_be_append = pd.Series(</span><br><span class="line">                    [<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            self.q_table = self.q_table.append(to_be_append)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># also update eligibility trace</span></span><br><span class="line">            self.eligibility_trace = self.eligibility_trace.append(to_be_append)</span><br></pre></td></tr></table></figure>



<h4 id="学习-1"><a href="#学习-1" class="headerlink" title="学习"></a>学习</h4><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaLambdaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaLambdaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        <span class="comment"># 这部分和 Sarsa 一样</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.ix[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.ix[s_, a_]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r</span><br><span class="line">        error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里开始不同:</span></span><br><span class="line">        <span class="comment"># 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环</span></span><br><span class="line">        self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q table 更新</span></span><br><span class="line">        self.q_table += self.lr * error * self.eligibility_trace</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随着时间衰减 eligibility trace 的值, 离获取 reward 越远的步, 他的&quot;不可或缺性&quot;越小</span></span><br><span class="line">        self.eligibility_trace *= self.gamma*self.lambda_</span><br></pre></td></tr></table></figure>

<p>除了图中和上面代码这种更新方式，还有一种会更加有效率。我们可以将上面的这一步替换成下面这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上面代码中的方式:</span></span><br><span class="line">self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更有效的方式:</span></span><br><span class="line">self.eligibility_trace.ix[s, :] *= <span class="number">0</span></span><br><span class="line">self.eligibility_trace.ix[s, a] = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>他们两的不同之处可以用这张图来概括:</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-3-2.png"></p>
<p>这是针对于一个 state-action 值按经历次数的变化。最上面是经历 state-action 的时间点，第二张图是使用这种方式所带来的 不可或缺性值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.eligibility_trace.ix[s, a] +&#x3D; 1</span><br></pre></td></tr></table></figure>

<p>下面图是使用这种方法带来的不可或缺性值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.eligibility_trace.ix[s, :] *&#x3D; 0; self.eligibility_trace.ix[s, a] &#x3D; 1</span><br></pre></td></tr></table></figure>

<p>实验证明选择下面这种方法会有更好的效果。大家也可以自己玩一玩，试试两种方法的不同表现。</p>
<p>最后不要忘了，eligibility trace 只是记录每个回合的每一步，新回合开始的时候需要将 Trace 清零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 新回合, 清零</span></span><br><span class="line">    RL.eligibility_trace *= <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>: <span class="comment"># 开始回合</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/3_Sarsa_maze">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p>学习资料:</p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/3_Sarsa_maze">全部代码</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q-learning 简介视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">什么是 Sarsa 短视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda">什么是 Sarsa-lambda 短视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/tabular-sarsa1">Sarsa Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
</search>
