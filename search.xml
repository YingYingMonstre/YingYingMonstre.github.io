<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MacBook Air M1安装gym问题汇总</title>
    <url>/YingYingMonstre.github.io/2021/11/03/MacBook%20Air%20M1%E5%AE%89%E8%A3%85gym%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>安装gym花了一天时间，中途出了很多问题，特此记录。</p>
<ol start="0">
<li>关于本机</li>
</ol>
<ul>
<li>macOS Monterey版本12.0.1</li>
<li>MacBook Air（M1，2020）</li>
<li>芯片 Apple M1</li>
</ul>
<ol>
<li>环境相关</li>
</ol>
<p>创建pycharm环境参考文章：<a href="https://zhuanlan.zhihu.com/p/410961551">https://zhuanlan.zhihu.com/p/410961551</a></p>
<ol start="2">
<li>下载gym</li>
</ol>
<p>可以直接在终端上pip install gym，但是在pycharm中使用时会报错缺一些组件，比如No module named ‘pyglet’。本人使用的是下边的方法：</p>
<p>git clone <a href="https://github.com/openai/gym.git">https://github.com/openai/gym.git</a></p>
<p>cd gym</p>
<p>pip install -e ‘.[all]’</p>
<ol start="3">
<li>No matching distribution found for ale-py~=0.7.1 (from gym==0.21.0)</li>
</ol>
<p>问题状况：在pip install -e ‘.[all]’时报错。</p>
<p>解决方法：直接pip install ale-py，如果不能下载（具体报错的原因忘了），尝试conda update pip，我是在更新完pip后可以下载的。</p>
<ol start="4">
<li>unable to execute ‘swig’: No such file or directory</li>
</ol>
<p>问题状况：解决了第3步的问题后，在pip install -e ‘.[all]’过程中出现。</p>
<p>解决方法：brew install swig</p>
<ol start="5">
<li>zsh: command not found: brew</li>
</ol>
<p>问题状况：在第4步输入brew install swig后报错。</p>
<p>解决方法：mac安装homebrew，</p>
<p>用以下命令安装，序列号选择中科大（1）的</p>
<p>/bin/zsh -c “$(curl -fsSL <a href="https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;">https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;</a></p>
<p>原文地址：<a href="https://links.jianshu.com/go?to=https://blog.csdn.net/wangyun71/article/details/108560873">https://blog.csdn.net/wangyun71/article/details/108560873</a></p>
<ol start="6">
<li>from . import multiarray等</li>
</ol>
<p>问题状况：在解决了第4、5步的问题后继续运行pip install -e ‘.[all]’，成功安装。但是在测试用例代码时报的错误。提示检查python和numpy的版本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></span><br></pre></td></tr></table></figure>

<p>解决方法：重新安装了numpy之后就可以用了。</p>
<p>pip uninstall numpy</p>
<p>pip install numpy</p>
<ol start="7">
<li>zsh:killed</li>
</ol>
<p>问题状况：偶然遇到的，在conda activate环境后不能用clear、pip等命令，不知道是什么原因。</p>
<p>解决方法：删除环境重新搭建环境就可以了</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ModbusTCP协议学习</title>
    <url>/YingYingMonstre.github.io/2021/09/17/ModbusTCP%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Modbus由MODICON公司于1979年开发，是一种<strong>工业现场总线</strong>协议标准。1996年<strong>施耐德</strong>公司推出基于<strong>以太网TCP/IP</strong>的Modbus协议：<strong>Modbus TCP</strong>。</p>
<p>Modbus协议是一项应用层报文传输协议，包括ASCII、RTU、<strong>TCP</strong>三种报文类型。</p>
<p>标准的Modbus协议物理层接口有RS232、RS422、RS485和<strong>以太网</strong>接口，采用<strong>master/slave</strong>方式通信。</p>
<h2 id="Modbus-TCP数据帧"><a href="#Modbus-TCP数据帧" class="headerlink" title="Modbus TCP数据帧"></a>Modbus TCP数据帧</h2><p>Modbus TCP的数据帧可分为两部分：<strong>MBAP</strong>+<strong>PDU</strong>。其协议特征如图所示。</p>
<p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fwww.51wendang.com%2Fpic%2Fe5675f007044fd1446490edf%2F2-537-png_6_0_0_464_608_341_194_892.8_1262.699-947-0-526-947.jpg&refer=http%3A%2F%2Fwww.51wendang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1634438115&t=92f4357504d2885188a29589d7de4e7f"></p>
<center>Modbus TCP协议特征</center>

<h3 id="报文头MBAP"><a href="#报文头MBAP" class="headerlink" title="报文头MBAP"></a>报文头MBAP</h3><p>MBAP为报文头，长度为7字节，组成如下：</p>
<table>
<thead>
<tr>
<th align="left">事务处理标识</th>
<th align="left">协议标识</th>
<th align="left">长度</th>
<th align="left">单元标识符</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2字节</td>
<td align="left">2字节</td>
<td align="left">2字节</td>
<td align="left">1字节</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td><strong>事务处理标识</strong></td>
<td>可以理解为报文的序列号，一般每次通信之后就要加1以区别不同的通信数据报文。</td>
</tr>
<tr>
<td><strong>协议标识符</strong></td>
<td>00 00表示Modbus TCP协议。</td>
</tr>
<tr>
<td><strong>长度</strong></td>
<td>表示接下来的数据长度，单位为字节。</td>
</tr>
<tr>
<td><strong>单元标识符</strong></td>
<td>可以理解为设备地址。</td>
</tr>
</tbody></table>
<h3 id="帧结构PDU"><a href="#帧结构PDU" class="headerlink" title="帧结构PDU"></a>帧结构PDU</h3><p>PDU由<strong>功能码+数据</strong>组成。功能码为1字节，数据长度不定，由具体功能决定。</p>
<p><strong>功能码</strong></p>
<p>Modbus的操作对象有四种：线圈、离散输入、保持寄存器、输入寄存器。</p>
<table>
<thead>
<tr>
<th align="center">对象</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线圈</td>
<td align="center">PLC的输出位，开关量，在Modbus中可读可写</td>
</tr>
<tr>
<td align="center">离散量</td>
<td align="center">PLC的输入位，开关量，在Modbus中只读</td>
</tr>
<tr>
<td align="center">输入寄存器</td>
<td align="center">PLC中只能从模拟量输入端改变的寄存器，在Modbus中只读</td>
</tr>
<tr>
<td align="center">保持寄存器</td>
<td align="center">PLC中用于输出模拟量信号的寄存器，在Modbus中可读可写</td>
</tr>
</tbody></table>
<p>根据对象的不同，Modbus的功能码有：</p>
<table>
<thead>
<tr>
<th align="center">功能码</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0x01</td>
<td align="center">读线圈</td>
</tr>
<tr>
<td align="center">0x05</td>
<td align="center">写单个线圈</td>
</tr>
<tr>
<td align="center">0x0F</td>
<td align="center">写多个线圈</td>
</tr>
<tr>
<td align="center">0x02</td>
<td align="center">读离散量输入</td>
</tr>
<tr>
<td align="center">0x04</td>
<td align="center">读输入寄存器</td>
</tr>
<tr>
<td align="center">0x03</td>
<td align="center">读保持寄存器</td>
</tr>
<tr>
<td align="center">0x06</td>
<td align="center">写单个保持寄存器</td>
</tr>
<tr>
<td align="center">0x10</td>
<td align="center">写多个保持寄存器</td>
</tr>
</tbody></table>
<p>说明更详细的表</p>
<table>
<thead>
<tr>
<th align="center">代码</th>
<th align="center">中文名称</th>
<th align="center">英文名</th>
<th align="center">位操作/字操作</th>
<th align="center">操作数量</th>
</tr>
</thead>
<tbody><tr>
<td align="center">01</td>
<td align="center">读线圈状态</td>
<td align="center">READ COIL STATUS</td>
<td align="center">位操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">02</td>
<td align="center">读离散输入状态</td>
<td align="center">READ INPUT STATUS</td>
<td align="center">位操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">03</td>
<td align="center">读保持寄存器</td>
<td align="center">READ HOLDING REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">04</td>
<td align="center">读输入寄存器</td>
<td align="center">READ INPUT REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">05</td>
<td align="center">写线圈状态</td>
<td align="center">WRITE SINGLE COIL</td>
<td align="center">位操作</td>
<td align="center">单个</td>
</tr>
<tr>
<td align="center">06</td>
<td align="center">写单个保持寄存器</td>
<td align="center">WRITE SINGLE REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个</td>
</tr>
<tr>
<td align="center">15</td>
<td align="center">写多个线圈</td>
<td align="center">WRITE MULTIPLE COIL</td>
<td align="center">位操作</td>
<td align="center">多个</td>
</tr>
<tr>
<td align="center">16</td>
<td align="center">写多个保持寄存器</td>
<td align="center">WRITE MULTIPLE REGISTER</td>
<td align="center">字操作</td>
<td align="center">多个</td>
</tr>
</tbody></table>
<h2 id="PDU详细结构"><a href="#PDU详细结构" class="headerlink" title="PDU详细结构"></a>PDU详细结构</h2><p>测试软件：mod_RSsim5.3</p>
<table>
<thead>
<tr>
<th align="center">模式</th>
<th align="center">对应</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线圈</td>
<td align="center">Coil Outputs</td>
</tr>
<tr>
<td align="center">离散量</td>
<td align="center">Digital Inputs</td>
</tr>
<tr>
<td align="center">输入寄存器</td>
<td align="center">Analogue Inputs</td>
</tr>
<tr>
<td align="center">保持寄存器</td>
<td align="center">Holding Registers</td>
</tr>
</tbody></table>
<p><strong>0x01：读线圈</strong></p>
<p>在从站中读1~2000个连续线圈状态，ON=1,OFF=0</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 数量H 数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 数据（一个地址的数据为1位）</li>
<li>如：在从站0x01中，读取开始地址为0x0002的线圈数据，读0x0008位<br>00 01 00 00 00 06 01 01 00 02 00 08</li>
<li>回：数据长度为0x01个字节，数据为0x01，第一个线圈为ON，其余为OFF<br>00 01 00 00 00 04 01 01 01 01</li>
</ul>
<p><strong>0x05：写单个线圈</strong></p>
<p>将从站中的一个输出写成ON或OFF，0xFF00请求输出为ON,0x000请求输出为OFF</p>
<ul>
<li>请求：MBAP 功能码 输出地址H 输出地址L 输出值H 输出值L（共12字节）</li>
<li>响应：MBAP 功能码 输出地址H 输出地址L 输出值H 输出值L（共12字节）</li>
<li>如：将地址为0x0003的线圈设为ON<br>00 01 00 00 00 06 01 05 00 03 FF 00</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 05 00 03 FF 00</li>
</ul>
<p><strong>0x0F：写多个线圈</strong></p>
<p>将一个从站中的一个线圈序列的每个线圈都强制为ON或OFF，数据域中置1的位请求相应输出位ON，置0的位请求响应输出为OFF</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 输出数量H 输出数量L 字节长度 输出值H 输出值L</li>
<li>响应：MBAP 功能码 起始地址H 起始地址L 输出数量H 输出数量L</li>
</ul>
<p><strong>0x02：读离散量输入</strong></p>
<p>从一个从站中读1~2000个连续的离散量输入状态</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 数量H 数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 数据（长度：9+ceil（数量/8））</li>
<li>如：从地址0x0000开始读0x0012个离散量输入<br>00 01 00 00 00 06 01 02 00 00 00 12</li>
<li>回：数据长度为0x03个字节，数据为0x01 04 00，表示第一个离散量输入和第11个离散量输入为ON，其余为OFF<br>00 01 00 00 00 06 01 02 03 01 04 00</li>
</ul>
<p><strong>0x04：读输入寄存器</strong></p>
<p>从一个远程设备中读1~2000个连续输入寄存器</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 寄存器数据(长度：9+寄存器数量×2)</li>
<li>如：读起始地址为0x0002，数量为0x0005的寄存器数据<br>00 01 00 00 00 06 01 04 00 02 00 05</li>
<li>回：数据长度为0x0A，第一个寄存器的数据为0x0c，其余为0x00<br>00 01 00 00 00 0D 01 04 0A 00 0C 00 00 00 00 00 00 00 00</li>
</ul>
<p><strong>0x03：读保持寄存器</strong></p>
<p>从远程设备中读保持寄存器连续块的内容</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 寄存器数据(长度：9+寄存器数量×2)</li>
<li>如：起始地址是0x0000，寄存器数量是 0x0003<br>00 01 00 00 00 06 01 03 00 00 00 03</li>
<li>回：数据长度为0x06，第一个寄存器的数据为0x21，其余为0x00<br>00 01 00 00 00 09 01 03 06 00 21 00 00 00 00</li>
</ul>
<p><strong>0x06：写单个保持寄存器</strong></p>
<p>在一个远程设备中写一个保持寄存器</p>
<ul>
<li>请求：MBAP 功能码 寄存器地址H 寄存器地址L 寄存器值H 寄存器值L（共12字节）</li>
<li>响应：MBAP 功能码 寄存器地址H 寄存器地址L 寄存器值H 寄存器值L（共12字节）</li>
<li>如：向地址是0x0000的寄存器写入数据0x000A<br>00 01 00 00 00 06 01 06 00 00 00 0A</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 06 00 00 00 0A</li>
</ul>
<p><strong>0x10：写多个保持寄存器</strong></p>
<p>在一个远程设备中写连续寄存器块（1~123个寄存器）</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L 字节长度 寄存器值（13+寄存器数量×2）</li>
<li>响应：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>如：向起始地址为0x0000，数量为0x0001的寄存器写入数据，数据长度为0x02，数据为0x000F<br>00 01 00 00 00 09 01 10 00 00 00 01 02 00 0F</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 10 00 00 00 01</li>
</ul>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    HOST = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    TCP_IP = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    TCP_PORT = <span class="number">502</span></span><br><span class="line">    MaxBytes = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建socket连接</span></span><br><span class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s.connect((TCP_IP, TCP_PORT))</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;error&#x27;</span>, e)</span><br><span class="line">        s.close()</span><br><span class="line">        sys.exit()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 连接成功</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;have connected with server&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 十进制、十六进制都可以</span></span><br><span class="line">    <span class="comment"># 示例为写一个保持寄存器</span></span><br><span class="line">    arr = [<span class="number">00</span>, <span class="number">1</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">0x0A</span>]</span><br><span class="line">    data = struct.pack(<span class="string">&quot;%dB&quot;</span> % (<span class="built_in">len</span>(arr)), *arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        s.settimeout(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># s.sendall(data) 发送数据包</span></span><br><span class="line">        sendBytes = s.send(data)</span><br><span class="line">        <span class="keyword">if</span> sendBytes &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接受响应信息</span></span><br><span class="line">        recvData = s.recv(MaxBytes)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> recvData:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;接收数据为空，我要退出了&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        localTime = time.asctime(time.localtime(time.time()))</span><br><span class="line">        <span class="built_in">print</span>(localTime, <span class="string">&#x27; 接收到数据字节数:&#x27;</span>, <span class="built_in">len</span>(recvData))</span><br><span class="line">        <span class="built_in">print</span>(struct.unpack(<span class="string">&quot;%dB&quot;</span> % (<span class="built_in">len</span>(recvData)), recvData))</span><br><span class="line"></span><br><span class="line">        localTime = time.asctime(time.localtime(time.time()))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;我已经退出了，后会无期&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="Modbus-TCP-示例报文"><a href="#Modbus-TCP-示例报文" class="headerlink" title="Modbus TCP 示例报文"></a>Modbus TCP 示例报文</h2><p>ModBusTcp与串行链路Modbus的数据域是一致的，具体数据域可以参考串行Modbus。这里给出几个ModbusTcp的链路解析说明，辅助新人分析报文。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b0fa067f61a600643c84f36ea69c49bb.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2a92c75438fa7336652d27ecd6081a08.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/616f2f4ff46da56aacec9bf7266043db.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c62b523c4e499d641ae06973b6b29c95.png"></p>
<blockquote>
<p>功能码 0x10：写多个保持寄存器。上面图片3和图片4都写错了。</p>
</blockquote>
<h2 id="ModbusTCP通信"><a href="#ModbusTCP通信" class="headerlink" title="ModbusTCP通信"></a>ModbusTCP通信</h2><p><strong>通信方式</strong></p>
<p>Modbus设备可分为主站(poll)和从站(slave)。主站只有一个，从站有多个，主站向各从站发送请求帧，从站给予响应。在使用TCP通信时，主站为client端，主动建立连接；从站为server端，等待连接。</p>
<ul>
<li>主站请求：功能码+数据</li>
<li>从站正常响应：请求功能码+响应数据</li>
<li>从站异常响应：异常功能码+异常码，其中异常功能码即将请求功能码的最高有效位置1，异常码指示差错类型</li>
<li><strong>注意：需要超时管理机制，避免无期限的等待可能不出现的应答</strong></li>
</ul>
<p>IANA（Internet Assigned Numbers Authority，互联网编号分配管理机构）给Modbus协议赋予TCP端口号为<strong>502</strong>，这是目前在仪表与自动化行业中唯一分配到的端口号。</p>
<p><strong>通信过程</strong></p>
<ol>
<li>connect 建立TCP连接</li>
<li>准备Modbus报文</li>
<li>使用send命令发送报文</li>
<li>在同一连接下等待应答</li>
<li>使用recv命令读取报文，完成一次数据交换</li>
<li>通信任务结束时，关闭TCP连接</li>
</ol>
<h2 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h2><p>在工业自动化控制中，经常会遇到开关量，数字量，模拟量，离散量，脉冲量等各种概念，而人们在实际应用中，对于这些概念又很容易混淆。现将各种概念罗列如下：</p>
<p><strong>1.开关量：</strong></p>
<p>一般指的是触点的“开”与“关”的状态，一般在计算机设备中也会用“0”或“1”来表示开关量的状态。开关量分为有源开关量信号和无源开关量信号，有源开关量信号指的是“开”与“关”的状态是带电源的信号，专业叫法为跃阶信号，可以理解为脉冲量，一般的都有220VAC, 110VAC,24VDC,12VDC等信号，无源开关量信号指的是“开”和“关”的状态时不带电源的信号，一般又称之为干接点。电阻测试法为电阻0或无穷大。</p>
<p><strong>2.数字量：</strong></p>
<p>很多人会将数字量与开关量混淆，也将其与模拟量混淆。数字量在时间和数量上都是离散的物理量，其表示的信号则为数字信号。数字量是由0和1组成的信号，经过编码形成有规律的信号，量化后的模拟量就是数字量。</p>
<p><strong>3.模拟量：</strong></p>
<p>模拟量的概念与数字量相对应，但是经过量化之后又可以转化为数字量。模拟量是在时间和数量上都是连续的物理量，其表示的信号则为模拟信号。模拟量在连续的变化过程中任何一个取值都是一个具体有意义的物理量，如温度，电压，电流等。</p>
<p><strong>4.离散量：</strong></p>
<p>离散量是将模拟量离散化之后得到的物理量。即任何仪器设备对于模拟量都不可能有个完全精确的表示，因为他们都有一个采样周期，在该采样周期内，其物理量的数值都是不变的，而实际上的模拟量则是变化的。这样就将模拟量离散化，成为了离散量。</p>
<p><strong>5.脉冲量：</strong></p>
<p>脉冲量就是瞬间电压或电流由某一值跃变到另一值的信号量。在量化后，其变化持续有规律就是数字量，如果其由0变成某一固定值并保持不变，其就是开关量。</p>
<blockquote>
<p>综上所述，模拟量就是在某个过程中时间和数量连续变化的物理量，由于在实际的应用中，所有的仪器设备对于外界数据的采集都有一个采样周期，其采集的数据只有在下一个采样周期开始时才有变动，采样周期内其数值并不随模拟量的变化而变动。</p>
<p>这样就将模拟量离散化了，例如：某设备的采样周期为1秒，其在第五秒的时间采集的温度为35度，而第六秒的温度为36度，该设备就只能标称第五秒时间温度35度，第六秒时间温度36度，而第五点五秒的时间其标称也只是35度，但是其实际的模拟量是35.5度。这样就将模拟信号离散化。其采集的数据就是离散化了，不再是连续的模拟量信号。</p>
<p>由于计算机只识别0和1两个信号，即开关量信号，用其来表示数值都是使用数字串来表示，由于计算能力的问题，其数字串不能无限长，即其表达的精度也是有限的，同样的以温度为例，由于数字串限制，其表达温度的精度只能达到0.1度，小于该单位的数值则不能被标称，这样就必须将离散量进行量化，将其变为数字量。即35.68度的温度则表示为35.6度。</p>
</blockquote>
<p>参考文章：<a href="https://www.cnblogs.com/ioufev/articles/10830028.html">ModbusTCP协议 - ioufev - 博客园 (cnblogs.com)</a></p>
]]></content>
      <categories>
        <category>protocol</category>
      </categories>
      <tags>
        <tag>Modbus</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch神经网络基础</title>
    <url>/YingYingMonstre.github.io/2021/12/07/PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h3 id="Torch-或-Numpy"><a href="#Torch-或-Numpy" class="headerlink" title="Torch 或 Numpy"></a>Torch 或 Numpy</h3><h4 id="用-Numpy-还是-Torch"><a href="#用-Numpy-还是-Torch" class="headerlink" title="用 Numpy 还是 Torch"></a>用 Numpy 还是 Torch</h4><p>Torch 自称为神经网络界的 Numpy，因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU)，就像 Numpy 会把 array 放在 CPU 中加速运算。所以神经网络的话，当然是用 Torch 的 tensor 形式数据最好咯。就像 Tensorflow 当中的 tensor 一样。</p>
<p>当然，我们对 Numpy 还是爱不释手的，因为我们太习惯 numpy 的形式了。不过 torch 看出来我们的喜爱，他把 torch 做的和 numpy 能很好的兼容。比如这样就能自由地转换 numpy array 和 torch tensor 了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nnumpy array:&#x27;</span>, np_data,          <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch tensor:&#x27;</span>, torch_data,      <span class="comment">#  0  1  2 \n 3  4  5    [torch.LongTensor of size 2x3]</span></span><br><span class="line">    <span class="string">&#x27;\ntensor to array:&#x27;</span>, tensor2array, <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h4 id="Torch-中的数学运算"><a href="#Torch-中的数学运算" class="headerlink" title="Torch 中的数学运算"></a>Torch 中的数学运算</h4><p>其实 torch 中 tensor 的运算和 numpy array 的如出一辙，我们就以对比的形式来看。如果想了解 torch 中其它更多有用的运算符，<a href="http://pytorch.org/docs/torch.html#math-operations">API就是你要去的地方</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># abs 绝对值计算</span></span><br><span class="line">data = [-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nabs&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.<span class="built_in">abs</span>(data),          <span class="comment"># [1 2 1 2]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.<span class="built_in">abs</span>(tensor)      <span class="comment"># [1 2 1 2]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sin   三角函数 sin</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nsin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.sin(data),      <span class="comment"># [-0.84147098 -0.90929743  0.84147098  0.90929743]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.sin(tensor)  <span class="comment"># [-0.8415 -0.9093  0.8415  0.9093]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean  均值</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmean&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.mean(data),         <span class="comment"># 0.0</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.mean(tensor)     <span class="comment"># 0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>除了简单的计算，矩阵运算才是神经网络中最重要的部分。所以我们展示下矩阵的乘法。注意一下包含了一个 numpy 中可行，但是 torch 中不可行的方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication 矩阵点乘</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="comment"># correct method</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmatrix multiplication (matmul)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.matmul(data, data),     <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.mm(tensor, tensor)   <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!  下面是错误的方法 !!!!</span></span><br><span class="line">data = np.array(data)</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmatrix multiplication (dot)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, data.dot(data),        <span class="comment"># [[7, 10], [15, 22]] 在numpy 中可行</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, tensor.dot(tensor)     <span class="comment"># torch 会转换成 [1,2,3,4].dot(&lt;a href=&quot;http://pytorch.org/docs/master/torch.html&quot; target=&#x27;_blank&#x27; &gt;1,2,3,4) = 30.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>新版本中(&gt;=0.3.0)，关于 <code>tensor.dot()</code> 有了新的改变，它[只能针对于一维的数组。所以上面的有所改变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor.dot(tensor)     <span class="comment"># torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 变为</span></span><br><span class="line">torch.dot(tensor.dot(tensor))</span><br></pre></td></tr></table></figure>



<h3 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量 (Variable)"></a>变量 (Variable)</h3><h4 id="什么是-Variable"><a href="#什么是-Variable" class="headerlink" title="什么是 Variable"></a>什么是 Variable</h4><p>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置。里面的值会不停的变化。就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那谁是里面的鸡蛋呢，自然就是 Torch 的 Tensor 咯。如果用一个 Variable 进行计算，那返回的也是一个同类型的 Variable。</p>
<p>我们定义一个 Variable：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="comment"># torch 中 Variable 模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先生鸡蛋</span></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="Variable-计算-梯度"><a href="#Variable-计算-梯度" class="headerlink" title="Variable 计算, 梯度"></a>Variable 计算, 梯度</h4><p>我们再对比一下 tensor 的计算和 variable 的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line"><span class="built_in">print</span>(t_out)</span><br><span class="line"><span class="built_in">print</span>(v_out)    <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>

<p>到目前为止，我们看不出什么不同，<strong>但是时刻记住, Variable 计算时，它在背景幕布后面一步步默默地搭建着一个庞大的系统，叫做计算图，computational graph。这个图是用来干嘛的？原来是将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候，一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来，而 tensor 就没有这个能力啦。</strong></p>
<p><code>v_out = torch.mean(variable*variable)</code> 就是在计算图中添加的一个计算步骤，计算误差反向传递的时候有他一份功劳，我们就来举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v_out.backward()    <span class="comment"># 模拟 v_out 的误差反向传递</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span></span><br><span class="line"><span class="comment"># 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.grad)    <span class="comment"># 初始 Variable 的梯度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="获取-Variable-里面的数据"><a href="#获取-Variable-里面的数据" class="headerlink" title="获取 Variable 里面的数据"></a>获取 Variable 里面的数据</h4><p>直接<code>print(variable)</code>只会输出 Variable 形式的数据，在很多时候是用不了的(比如想要用 plt 画图)，所以我们要转换一下，将它变成 tensor 形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(variable)     <span class="comment">#  Variable 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.data)    <span class="comment"># tensor 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.data.numpy())    <span class="comment"># numpy 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="什么是激励函数-Activation-Function"><a href="#什么是激励函数-Activation-Function" class="headerlink" title="什么是激励函数 (Activation Function)"></a>什么是激励函数 (Activation Function)</h3><p>今天我们会来聊聊现代神经网络中必不可少的一个组成部分，激励函数，activation function。</p>
<h4 id="非线性方程"><a href="#非线性方程" class="headerlink" title="非线性方程"></a>非线性方程</h4><p>我们为什么要使用激励函数？用简单的语句来概括。就是因为，现实并没有我们想象的那么美好，它是残酷多变的。哈哈，开个玩笑，不过激励函数也就是为了解决我们日常生活中 不能用线性方程所概括的问题。好了，我知道你的问题来了。什么是线性方程 (linear function)？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/active1.png"></p>
<p>说到线性方程，我们不得不提到另外一种方程，非线性方程 (nonlinear function)。我们假设，女生长得越漂亮，越多男生爱。这就可以被当做一个线性问题。但是如果我们假设这个场景是发生在校园里。校园里的男生数是有限的，女生再漂亮，也不可能会有无穷多的男生喜欢她。所以这就变成了一个非线性问题。再说…女生也不可能是无穷漂亮的。这个问题我们以后有时间私下讨论。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/active2.png"></p>
<p>然后我们就可以来讨论如何在神经网络中达成我们描述非线性的任务了。我们可以把整个网络简化成这样一个式子。Y = Wx，W 就是我们要求的参数，y 是预测值，x 是输入值。用这个式子，我们很容易就能描述刚刚的那个线性问题，因为 W 求出来可以是一个固定的数。不过这似乎并不能让这条直线变得扭起来，激励函数见状，拔刀相助，站出来说道: “让我来掰弯它!”。</p>
<h4 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/active3.png"></p>
<p>这里的 AF 就是指的激励函数。激励函数拿出自己最擅长的”掰弯利器”，套在了原函数上用力一扭，原来的 Wx 结果就被扭弯了。</p>
<p>其实这个 AF, 掰弯利器，也不是什么触不可及的东西。它其实就是另外一个非线性函数。比如说relu、sigmoid、tanh。将这些掰弯利器嵌套在原有的结果之上，强行把原有的线性结果给扭曲了。使得输出结果 y 也有了非线性的特征。举个例子，比如我使用了 relu 这个掰弯利器，如果此时 Wx 的结果是1，y 还将是1，不过 Wx 为-1的时候，y 不再是-1，而会是0。</p>
<p>你甚至可以创造自己的激励函数来处理自己的问题，不过要确保的是这些激励函数必须是可以微分的，因为在 backpropagation 误差反向传递的时候，只有这些可微分的激励函数才能把误差传递回去。</p>
<h4 id="常用选择"><a href="#常用选择" class="headerlink" title="常用选择"></a>常用选择</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/active4.png"></p>
<p>想要恰当使用这些激励函数，还是有窍门的。比如当你的神经网络层只有两三层，不是很多的时候，对于隐藏层，使用任意的激励函数，随便掰弯是可以的，不会有特别大的影响。不过，当你使用特别多层的神经网络，在掰弯的时候，玩玩不得随意选择利器。因为这会涉及到梯度爆炸，梯度消失的问题。因为时间的关系，我们可能会在以后来具体谈谈这个问题。</p>
<p>最后我们说说，在具体的例子中，我们默认首选的激励函数是哪些。在少量层结构中，我们可以尝试很多种不同的激励函数。在卷积神经网络 Convolutional neural networks 的卷积层中，推荐的激励函数是 relu。在循环神经网络中 recurrent neural networks，推荐的是 tanh 或者是 relu (这个具体怎么选, 我会在以后循环神经网络的介绍中在详细讲解)。</p>
<h3 id="激励函数-Activation"><a href="#激励函数-Activation" class="headerlink" title="激励函数 (Activation)"></a>激励函数 (Activation)</h3><h4 id="什么是-Activation"><a href="#什么是-Activation" class="headerlink" title="什么是 Activation"></a>什么是 Activation</h4><p>一句话概括 Activation：就是让神经网络可以描述非线性问题的步骤，是神经网络变得更强大。如果还不是特别了解，我有制作一个<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">动画短片</a>，浅显易懂的阐述了激励函数的作用。包懂。</p>
<h4 id="Torch-中的激励函数"><a href="#Torch-中的激励函数" class="headerlink" title="Torch 中的激励函数"></a>Torch 中的激励函数</h4><p>Torch 中的激励函数有很多，不过我们平时要用到的就这几个。<code>relu</code>，<code>sigmoid</code>，<code>tanh</code>，<code>softplus</code>。那我们就看看他们各自长什么样啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做一些假数据来观看图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">200</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">x = Variable(x)</span><br></pre></td></tr></table></figure>

<p>接着就是做生成不同的激励函数数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_np = x.data.numpy()   <span class="comment"># 换成 numpy array, 出图时用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 几种常用的 激励函数</span></span><br><span class="line">y_relu = F.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = F.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = F.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"><span class="comment"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</span></span><br></pre></td></tr></table></figure>

<p>接着我们开始画图, 画图的代码也在下面：</p>
<p><img src="https://static.mofanpy.com/results/torch/2-3-1.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># python 的可视化模块, 我有教程 (https://mofanpy.com/tutorials/data-manipulation/plt/)</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x_np, y_relu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">0.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x_np, y_tanh, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x_np, y_softplus, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;softplus&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">0.2</span>, <span class="number">6</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents">本章的全部代码</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
<li><a href="http://pytorch.org/docs/torch.html#math-operations">PyTorch 中的常用数学计算</a></li>
<li>Theano 激励函数 <a href="https://mofanpy.com/tutorials/machine-learning/theano/activation">教程</a></li>
<li>Tensorflow 激励函数 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/activation">教程</a></li>
<li>PyTorch 激励函数 <a href="https://mofanpy.com/tutorials/machine-learning/torch/activation">教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">我制作的 激励函数 动画简介</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>XCTF-Web基础篇</title>
    <url>/YingYingMonstre.github.io/2021/09/26/XCTF-Web%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
    <content><![CDATA[<h2 id="view-source"><a href="#view-source" class="headerlink" title="view_source"></a>view_source</h2><p>方法一：不能用右键审查元素，则按F12打开开发者工具查看，找到flag</p>
<p>方法二：在url中通过view-source:的方法来访问源码，在url中提交view-source:+url</p>
<p>方法三：通过Burpsuite抓包查看源代码</p>
<h2 id="robots"><a href="#robots" class="headerlink" title="robots"></a>robots</h2><p>[原理]</p>
<p>robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。</p>
<p>[步骤]</p>
<p>1.根据提示robots,可以直接想到robots.txt</p>
<p>2.或通过扫目录也可以扫到: <code>python dirsearch.py -u http://10.10.10.175:32793/ -e *</code>（这个脚本在cmd中可以用，git bash不行）</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/robots/1.png" alt="img"></p>
<p>3.访问<code>http://111.198.29.45:33982/robots.txt</code>发现<code>f1ag_1s_h3re.php</code></p>
<p>4.访问<code>http://111.198.29.45:33982/f1ag_1s_h3re.php</code>得到flag</p>
<h2 id="backup"><a href="#backup" class="headerlink" title="backup"></a>backup</h2><p><strong>[目标]</strong></p>
<p>掌握有关备份文件的知识</p>
<p>常见的备份文件后缀名有: <code>.git .svn .swp .svn .~ .bak .bash_history</code></p>
<p><strong>[环境]</strong></p>
<p>无</p>
<p><strong>[工具]</strong></p>
<p>扫目录脚本dirsearch(项目地址：<code>https://github.com/maurosoria/dirsearch</code>(<code>https://github.com/maurosoria/dirsearch</code>))</p>
<p><strong>[步骤]</strong></p>
<p>1.可以手动猜测,也可以使用扫目录脚本/软件,扫一下,这里使用的是github上的脚本dirsearch,命令行下: <code>py python3 dirsearch.py -u http://10.10.10.175:32770 -e *</code></p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/backup/1.png" alt="img"></p>
<p>2.看到存在备份文件index.php.bak访问 <code>http://10.10.10.175:32770/index.php.bak</code></p>
<p>3.保存到本地打开，即可看到flag</p>
<h2 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h2><p>[原理]</p>
<p> Cookie是当主机访问Web服务器时，由 Web 服务器创建的，将信息存储在用户计算机上的文件。一般网络用户习惯用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 Session 跟踪而存储在用户本地终端上的数据，而这些数据通常会经过加密处理。</p>
<p><strong>[目的]</strong></p>
<p>掌握有关cookie的知识，了解cookie所在位置</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.浏览器按下F12键打开开发者工具，刷新后，在存储一栏，可看到名为look-here的cookie的值为cookie.php</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/1.png" alt="img"></p>
<p>2.访问<code>http://111.198.29.45:47911/cookie.php</code>，提示查看http响应包，在网络一栏，可看到访问cookie.php的数据包</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/2.png" alt="img"></p>
<p>3.点击查看数据包，在消息头内可发现flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/3.png" alt="img"></p>
<p>用dirsearch可以找到cookie.php文件，再用burpsuite抓包也可行。</p>
<h2 id="disabled-button"><a href="#disabled-button" class="headerlink" title="disabled_button"></a>disabled_button</h2><p><strong>[目标]</strong></p>
<p>初步了解前端知识</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开开发者工具（得用火狐浏览器），在查看器窗口审查元素，发现存在disabled=””字段，</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/1.png" alt="img"></p>
<p>2.将<code>disabled=&quot;&quot;</code>删除后，按钮可按，按下后得到flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/2.png" alt="img"></p>
<p>3.或审计from表单代码，使用hackbar（不能用，现在收费了），用post方式传递auth=flag，同样可以获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/3.png" alt="img"></p>
<h2 id="weak-auth"><a href="#weak-auth" class="headerlink" title="weak_auth"></a>weak_auth</h2><p><strong>[目标]</strong></p>
<p>了解弱口令，掌握爆破方法</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<ul>
<li>burpsuite</li>
<li>字典<code>&lt;https://github.com/rootphantomer/Blasting_dictionary/blob/master/%E5%B8%B8%E7%94%A8%E5%AF%86%E7%A0%81.txt&gt;</code></li>
</ul>
<p><strong>[步骤]</strong></p>
<p>1.随便输入下用户名和密码,提示要用admin用户登入,然后跳转到了check.php,查看下源代码提示要用字典。</p>
<p>2.用burpsuite截下登录的数据包,把数据包发送到intruder爆破</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/1.png" alt="img"></p>
<p>2.设置爆破点为password</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/2.png" alt="img"></p>
<p>3.加载字典</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/3.png" alt="img"></p>
<p>4.开始攻击，查看响应包列表，发现密码为123456时，响应包的长度和别的不一样.</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/4.png" alt="img"></p>
<p>5.点进去查看响应包，获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/5.png" alt="img"></p>
<h2 id="simple-php"><a href="#simple-php" class="headerlink" title="simple_php"></a>simple_php</h2><p><strong>[原理]</strong></p>
<p>php中有两种比较符号</p>
<p>=== 会同时比较字符串的值和类型</p>
<p>== 会先将字符串换成相同类型（先类型转换），再作比较，属于弱类型比较</p>
<p><strong>[目地]</strong></p>
<p>掌握php的弱类型比较</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开页面，进行代码审计，发现同时满足 $a==0 和 $a 时，显示flag1。</p>
<p>2.php中的弱类型比较会使’abc’ == 0为真，所以输入a=abc时，可得到flag1，如图所示。（abc可换成任意字符）。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_php/1.png" alt="img"></p>
<p>3.is_numeric() 函数会判断如果是数字和数字字符串则返回 TRUE，否则返回 FALSE,且php中弱类型比较时，会使(‘1234a’ == 1234)为真，所以当输入a=abc&amp;b=1235a，可得到flag2，如图所示。==（数字和字符混合的字符串转换为整数后只保留数字）==</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_php/2.png" alt="img"></p>
<p> php类型比较表<a href="https://www.php.net/manual/zh/types.comparisons.php%E3%80%82">https://www.php.net/manual/zh/types.comparisons.php。</a></p>
<h2 id="get-post"><a href="#get-post" class="headerlink" title="get_post"></a>get_post</h2><p><strong>[原理]</strong></p>
<p>HTTP工作原理</p>
<p><strong>[目的]</strong></p>
<p>掌握常用http请求方式</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox和cmd</p>
<p><strong>[步骤]</strong></p>
<p>GET请求在url后加?a=1即可</p>
<p>POST请求需要用curl POST -d “b=2” <a href="http://111.200.241.244:65197/?a=1">http://111.200.241.244:65197/?a=1</a></p>
<p>返回如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl: (6) Could not resolve host: POST</span><br><span class="line">﻿&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang&#x3D;&quot;en&quot;&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;title&gt;POST&amp;GET&lt;&#x2F;title&gt;</span><br><span class="line">    &lt;link href&#x3D;&quot;http:&#x2F;&#x2F;libs.baidu.com&#x2F;bootstrap&#x2F;3.0.3&#x2F;css&#x2F;bootstrap.min.css&quot; rel&#x3D;&quot;stylesheet&quot; &#x2F;&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;请用GET方式提交一个名为a,值为1的变量&lt;&#x2F;h1&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;请再以POST方式随便提交一个名为b,值为2的变量&lt;&#x2F;h1&gt;&lt;h1&gt;cyberpeace&#123;51117d20e10dd8646f8d2eed10942e46&#125;&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>

<p>或者用burpsuite实现GET、POST的抓包：</p>
<p>GET的实现：在GET / HTTP/1.1中加入?a=1得到GET /?a=1 HTTP/1.1</p>
<p>POST的实现：GET / HTTP/1.1改为POST /?a=1 HTTP/1.1，在最下面（正文）加b=2，报文头加上Content-Type: application/x-www-form-urlencoded。</p>
<h2 id="xff-referer"><a href="#xff-referer" class="headerlink" title="xff_referer"></a>xff_referer</h2><p>[原理]</p>
<p>X-Forwarded-For:简称XFF头，它代表客户端，也就是HTTP的请求端真实的IP，只有在通过了HTTP 代理或者负载均衡服务器时才会添加该项</p>
<p>HTTP Referer是header的一部分，当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的</p>
<p><strong>[目的]</strong></p>
<p>掌握有关X-Forwarded-For和Referer的知识</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox、burpsuite</p>
<p><strong>[步骤]</strong></p>
<p>1.打开firefox和burp，使用burp对firefox进行代理拦截，在请求头添加<code>X-Forwarded-For: 123.123.123.123</code>，然后放包</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/xff_referer/1.png" alt="img"></p>
<p>2.接着继续在请求头内添加<code>Referer: https://www.google.com</code>，可获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/xff_referer/2.png" alt="img"></p>
<h2 id="webshell"><a href="#webshell" class="headerlink" title="webshell"></a>webshell</h2><p><strong>[目标]</strong></p>
<p>了解php一句话木马、如何使用webshell</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox、hackbar</p>
<p>蚁剑下载地址<code>https://github.com/AntSwordProject/antSword/releases</code>(<code>https://github.com/AntSwordProject/antSword/releases</code>)</p>
<p><strong>[步骤]</strong></p>
<p>1.直接提示给了php一句话，可以用菜刀或蚁剑连接,此处用蚁剑链接:</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/1.png" alt="img"></p>
<p>2.连接后在网站目录下发现了flag.txt文件，查看文件可获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/2.png" alt="img"></p>
<p>3.也可以使用hackbar，使用post方式传递shell=system(‘cat flag.txt’); 获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/3.png" alt="img"></p>
<h2 id="command-execution"><a href="#command-execution" class="headerlink" title="command_execution"></a>command_execution</h2><p><strong>[原理]</strong></p>
<p>| 的作用为将前一个命令的结果传递给后一个命令作为输入</p>
<p>&amp;&amp;的作用是前一条命令执行成功时，才执行后一条命令</p>
<p><strong>[目地]</strong></p>
<p>掌握命令拼接的方法</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开浏览器，在文本框内输入127.0.0.1 |  find / -name “flag.txt” （将 | 替换成 &amp; 或 &amp;&amp; 都可以）,查找flag所在位置，如图所示。</p>
<p>127.0.0.1 | ls ../../../可以找到一个home文件夹，里面有flag.txt文件。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/command_execution/1.png" alt="img"></p>
<p>2.在文本框内输入 127.0.0.1 |  cat /home/flag.txt 可得到flag，如图所示。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/command_execution/2.png" alt="img"></p>
<h2 id="simple-js"><a href="#simple-js" class="headerlink" title="simple_js"></a>simple_js</h2><p><strong>[原理]</strong></p>
<p>javascript的代码审计</p>
<p><strong>[目地]</strong></p>
<p>掌握简单的javascript函数</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开页面，查看源代码，可以发现js代码，如图所示。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_js/1.png" alt="img"></p>
<p>2.进行代码审计，发现不论输入什么都会跳到假密码，真密码位于 fromCharCode 。</p>
<p>3.先将字符串用python处理一下，得到数组[55,56,54,79,115,69,114,116,107,49,50]，exp如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="string">&quot;\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30&quot;</span></span><br><span class="line"><span class="built_in">print</span> (s)</span><br></pre></td></tr></table></figure>

<p>4.将得到的数字分别进行ascii处理，可得到字符串786OsErtk12，exp如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a = [55,56,54,79,115,69,114,116,107,49,50]</span></span><br><span class="line"><span class="string">c = &quot;&quot;</span></span><br><span class="line"><span class="string">for i in a:</span></span><br><span class="line"><span class="string">    b = chr(i)</span></span><br><span class="line"><span class="string">    c = c + b</span></span><br><span class="line"><span class="string">print(c)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">string = <span class="string">&quot;\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37&quot;</span> \</span><br><span class="line">         <span class="string">&quot;\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31&quot;</span> \</span><br><span class="line">         <span class="string">&quot;\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30&quot;</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">label = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> string:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&quot;,&quot;</span>:</span><br><span class="line">        label += <span class="built_in">chr</span>(count)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        count = count * <span class="number">10</span> + <span class="built_in">int</span>(i)</span><br><span class="line"></span><br><span class="line">label += <span class="built_in">chr</span>(count)</span><br><span class="line"><span class="built_in">print</span>(label)</span><br></pre></td></tr></table></figure>

<p>5.规范flag格式，可得到Cyberpeace{786OsErtk12}</p>
<h2 id="baby-web"><a href="#baby-web" class="headerlink" title="baby_web"></a>baby_web</h2><p><strong>【实验原理】</strong></p>
<p>web请求头中的location作用</p>
<p><strong>【实验目的】</strong></p>
<p>掌握web响应包头部常见参数</p>
<p><strong>【实验环境】</strong></p>
<p>Windows</p>
<p><strong>【实验工具】</strong></p>
<p>firefox</p>
<p><strong>【实验步骤】</strong></p>
<p>1.根据提示，在url中输入index.php,发现打开的仍然还是1.php</p>
<p>2.打开火狐浏览器的开发者模式，选择网络模块，再次请求index.php,查看返回包，可以看到location参数被设置了1.php，并且得到flag。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/baby_web/1.png" alt="img"></p>
<p>我的：burpsuite抓包，看到有flag。</p>
]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>XCTF</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（简介）</title>
    <url>/YingYingMonstre.github.io/2021/10/31/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是强化学习"><a href="#什么是强化学习" class="headerlink" title="什么是强化学习"></a>什么是强化学习</h3><h4 id="从无到有"><a href="#从无到有" class="headerlink" title="从无到有"></a>从无到有</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL1.png"></p>
<p>强化学习是一类算法，是让计算机实现从一开始什么都不懂，脑袋里没有一点想法，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程。实际中的强化学习例子有很多。比如近期最有名的 Alpha go，机器头一次在围棋场上战胜人类高手；让计算机自己学着玩经典游戏 Atari，这些都是让计算机在不断的尝试中更新自己的行为准则，从而一步步学会如何下好围棋、如何操控游戏得到高分。既然要让计算机自己学，那计算机通过什么来学习呢？</p>
<h4 id="虚拟老师"><a href="#虚拟老师" class="headerlink" title="虚拟老师"></a>虚拟老师</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL2.png"></p>
<p>原来计算机也需要一位虚拟的老师，这个老师比较吝啬，他不会告诉你如何移动、如何做决定，他为你做的事只有给你的行为打分，那我们应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，我只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为。</p>
<p>比如老师会根据我的开心程度来打分——我开心时，可以得到高分，我不开心时得到低分。有了这些被打分的经验，我就能判断为了拿到高分，我应该选择一张开心的脸， 避免选到伤心的脸。这也是强化学习的核心思想。可以看出在强化学习中，一种行为的分数是十分重要的。所以强化学习具有分数导向性。我们换一个角度来思考。这种分数导向性好比我们在监督学习中的正确标签。</p>
<h4 id="对比监督学习"><a href="#对比监督学习" class="headerlink" title="对比监督学习"></a>对比监督学习</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL3.png"></p>
<p>我们知道监督学习，是已经有了数据和数据对应的正确标签，比如这样。监督学习就能学习出那些脸对应哪种标签。不过强化学习还要更进一步，一开始它并没有数据和标签。</p>
<p>他要通过一次次在环境中的尝试，获取这些数据和标签，然后再学习通过哪些数据能够对应哪些标签，通过学习到的这些规律，尽可能地选择带来高分的行为 (比如这里的开心脸)。这也就证明了在强化学习中，分数标签就是他的老师，他和监督学习中的老师也差不多。</p>
<h4 id="RL-算法们"><a href="#RL-算法们" class="headerlink" title="RL 算法们"></a>RL 算法们</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL4.png"></p>
<p>强化学习是一个大家族，他包含了很多种算法，我们也会一一提到之中一些比较有名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的Q learning、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习 等等。</p>
<h3 id="强化学习方法汇总"><a href="#强化学习方法汇总" class="headerlink" title="强化学习方法汇总"></a>强化学习方法汇总</h3><p>了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法。比如说比较知名的控制方法 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>，<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a>，还有基于对环境的理解的 model-based RL 等等。接下来我们通过分类的方式来了解他们的区别。</p>
<h4 id="Model-free-和-Model-based"><a href="#Model-free-和-Model-based" class="headerlink" title="Model-free 和 Model-based"></a>Model-free 和 Model-based</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd1.png"></p>
<p>我们可以将所有强化学习的方法分为理不理解所处环境，如果我们不尝试去理解环境，环境给了我们什么就是什么。我们就把这种方法叫做 model-free，这里的 model 就是用模型来表示环境，那理解了环境也就是学会了用一个模型来代表环境，所以这种就是 model-based 方法。我们想象：现在环境就是我们的世界，我们的机器人正在这个世界里玩耍，他不理解这个世界是怎样构成的，也不理解世界对于他的行为会怎么样反馈。举个例子，他决定丢颗原子弹去真实的世界，结果把自己给炸死了，所有结果都是那么现实。不过如果采取的是 model-based RL，机器人会通过过往的经验，先理解真实世界是怎样的，并建立一个模型来模拟现实世界的反馈，最后他不仅可以在现实世界中玩耍，也能在模拟的世界中玩耍，这样就没必要去炸真实世界，连自己也炸死了，他可以像玩游戏一样炸炸游戏里的世界，也保住了自己的小命。那我们就来说说这两种方式的强化学习各用那些方法吧。</p>
<p>Model-free 的方法有很多，像 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">Sarsa</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a> 都是从环境中得到反馈然后从中学习。而 model-based RL 只是多了一道程序——为真实世界建模，也可以说他们都是 model-free 的强化学习，只是 model-based 多出了一个虚拟环境，我们不仅可以像 model-free 那样在现实中玩耍，还能在游戏中玩耍，而玩耍的方式也都是 model-free 中那些玩耍方式，最终 model-based 还有一个杀手锏是 model-free 超级羡慕的，那就是想象力。</p>
<p>Model-free 中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 model-based，他能通过想象来预判断接下来将要发生的所有情况，然后选择这些想象情况中最好的那种。并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来，我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p>
<h4 id="基于概率-和-基于价值"><a href="#基于概率-和-基于价值" class="headerlink" title="基于概率 和 基于价值"></a>基于概率 和 基于价值</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd2.png"></p>
<p>基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，我们会根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为铁定、毫不留情，就选价值最高的；而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他。</p>
<p>我们现在说的动作都是一个一个不连续的动作，而对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个概率分布在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢?</p>
<p>比如在基于概率这边，有 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a>，在基于价值这边有 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">Sarsa</a> 等。而且我们还能结合这两类方法的优势之处，创造更牛逼的一种方法，叫做 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-AC">Actor-Critic</a>，actor 会基于概率做出动作，而 critic 会对做出的动作给出动作的价值，这样就在原有的 policy gradients 上加速了学习过程。</p>
<h4 id="回合更新-和-单步更新"><a href="#回合更新-和-单步更新" class="headerlink" title="回合更新 和 单步更新"></a>回合更新 和 单步更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd3.png"></p>
<p>强化学习还能用另外一种方式分类，回合更新和单步更新，想象强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，我们要等待游戏结束，然后再总结这一回合中的所有转折点，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束，这样我们就能边玩边学习了。</p>
<p>再来说说方法，Monte-carlo learning 和基础版的 policy gradients 等 都是回合更新制，Q learning、Sarsa、升级版的 policy gradients 等都是单步更新制。因为单步更新更有效率，所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</p>
<h4 id="在线学习-和-离线学习"><a href="#在线学习-和-离线学习" class="headerlink" title="在线学习 和 离线学习"></a>在线学习 和 离线学习</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd4.png"></p>
<p>这个视频的最后一种分类方式是 在线学习和离线学习。所谓在线学习，就是指我必须本人在场，并且一定是本人边玩边学习；而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。或者我也不必要边玩边学习，我可以白天先存储下来玩耍时的记忆，然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢?</p>
<p>最典型的在线学习就是 Sarsa 了，还有一种优化 Sarsa 的算法，叫做 Sarsa lambda，最典型的离线学习就是 Q learning，后来人也根据离线学习的属性，开发了更强大的算法，比如让计算机学会玩电动的 Deep-Q-Network。</p>
<p>这就是我们从各种不同的角度来对比了强化学习中的多种算法。</p>
<h3 id="为什么用强化学习-Why"><a href="#为什么用强化学习-Why" class="headerlink" title="为什么用强化学习 Why?"></a>为什么用强化学习 Why?</h3><h4 id="强化学习介绍"><a href="#强化学习介绍" class="headerlink" title="强化学习介绍"></a>强化学习介绍</h4><p>强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支，由于近些年来的技术突破，和深度学习 (Deep Learning) 的整合，使得强化学习有了进一步的运用。比如让计算机学着玩游戏，AlphaGo 挑战世界围棋高手，都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生，成长为一个在环境中游刃有余的高手。</p>
<p>这些教程的教学，不依赖于任何强化学习的 python 模块。因为强化学习的复杂性、多样性，到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p>
<h4 id="模拟程序提前看"><a href="#模拟程序提前看" class="headerlink" title="模拟程序提前看"></a>模拟程序提前看</h4><p>以下是我们将要在后续的课程中实现的牛逼的自学程序。</p>
<p>Youtube 的模拟视频都在这里:</p>
<p><a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O">https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O</a>_.</p>
<p>优酷的模拟视频在这里:</p>
<p><a href="http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1">http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1</a></p>
<p>下面是其中一些模拟视频:</p>
<ul>
<li>Maze</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa_lambda.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>


<ul>
<li>Cartpole</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/cartpole%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>


<ul>
<li>Mountain car</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h3 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h3><h4 id="教程必备模块"><a href="#教程必备模块" class="headerlink" title="教程必备模块"></a>教程必备模块</h4><p>强化学习有一些现成的模块可以使用，但是那些模块并不全面，而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学习，可能 RL 的代码就不同。所以我们要抱着以不变应万变的心态，用基础的模块，从基础学起。懂了原理，再复杂的环境也不在话下。</p>
<p>所以用到的模块和对应的教程:</p>
<ul>
<li><a href="https://mofanpy.com/tutorials/data-manipulation/np-pd/">Numpy, Pandas</a> (必学), 用于学习的数据处理</li>
<li><a href="https://mofanpy.com/tutorials/data-manipulation/plt/">Matplotlib</a> (可学), 偶尔会用来呈现误差曲线什么的</li>
<li><a href="https://mofanpy.com/tutorials/python-basic/tkinter/">Tkinter</a> (可学), 你可以自己用它来编写模拟环境</li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow</a> (可学), 后面实现神经网络与强化学习结合的时候用到</li>
<li><a href="https://gym.openai.com/">OpenAI gym</a> (可学), 提供了很多现成的模拟环境</li>
</ul>
<h4 id="快速了解强化学习"><a href="#快速了解强化学习" class="headerlink" title="快速了解强化学习"></a>快速了解强化学习</h4><p>我也会制作每种强化学习对应的简介视频 (在这个学习列表里: <a href="https://mofanpy.com/tutorials/machine-learning/ML-intro/">有趣的机器学习</a>)，大家可以只花很少的时间来观看了解这些学习方法的不同之处。有了一定概念和基础，我们在这套教材里实现起来就容易多了。而且不懂的时候也能只花很少的时间回顾就行。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch简介</title>
    <url>/YingYingMonstre.github.io/2021/12/06/PyTorch%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h3 id="科普-人工神经网络-VS-生物神经网络"><a href="#科普-人工神经网络-VS-生物神经网络" class="headerlink" title="科普: 人工神经网络 VS 生物神经网络"></a>科普: 人工神经网络 VS 生物神经网络</h3><h4 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann1.png"></p>
<p>2-30年前，一想到神经网络，我们就会想到生物神经系统中数以万计的细胞联结，将感官和反射器联系在一起的系统。但是今天，你可能的第一反应却是…电脑和电脑程序当中的人工神经网络。昔日复杂的动神经网络系统居然神奇地放入了计算机？而且人类正在将这种人工神经网络系统推向更高的境界。今天的世界早已布满了人工神经网络的身影。</p>
<p>比如 Google 的搜索引擎。股票价格预测、机器人学习、围棋、家庭助手，等等等等。从金融到仿生样样都能运用。看起来人工神经网络的确很强大。但，是不是有这么一个问题一直在你脑海中环绕，没有答案。“计算机领域的神经网络和我们自己身体里的神经网络究竟是一样的吗?” 科学家们通过长久的探索，想让计算机像人一样思考，所以研发了人工神经网络，究竟和我们的神经网络有多像？那我们就先来看看人的神经网络到底是什么。</p>
<h4 id="你的神经网络系统"><a href="#你的神经网络系统" class="headerlink" title="你的神经网络系统"></a>你的神经网络系统</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann2.png"></p>
<p>9百亿神经细胞组成了我们复杂的神经网络系统，这个数量甚至可以和宇宙中的星球数相比较。如果仅仅靠单个的神经元，是永远没有办法让我们像今天一样，完成各种任务，处理各种复杂的问题。那我们是如何靠这些神经元来解决问题的呢？首先需要知道的是我们的记忆是如何产生的。想象我们还是婴儿，</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/ann3.png"></p>
<p>包着尿布的我们什么都不知道，神经元并没有形成系统和网络。可能只是一些分散的细胞而已，一端连着嘴巴的味觉感受器，一端连着手部的肌肉。小时候，世界上有一种神奇的东西叫做——糖果，当我们第一次品尝它的时候，美妙的感觉，让我们发现活着是多么有意义的事情。这时候神经元开始产生联结，记忆形成，但是形成的新联结怎么样变成记忆，仍然是科学界的一个迷。不过现在，我们的手和嘴产生了某种特定的搭配。每次发现有糖果的时候，某种生物信号就会从我们的嘴，通过之前形成的神经联结，传递到手上，让手的动作变得有意义，比如这样，然后爸妈就会再给我们一颗糖果啦~ 哈哈，吃糖的目的达成。现在我们来看看人工神经网络要怎样达到这个目的。</p>
<h4 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann4.png"></p>
<p>首先，替代掉生物神经网络的，就是已经成体系的人工神经网络。所有神经元之间的连接都是固定不可更换的，这也就是说，在人工神经网络里，没有凭空产生新联结这回事。人工神经网络典型的一种学习方式就是，我已经知道吃到糖果时，手会如何动，但是我想让神经网络学着帮我做这件动动手的事情。所以我预先准备好非常多吃糖的学习数据，然后将这些数据一次次放入这套人工神经网络系统中，糖的信号会通过这套系统传递到手。然后通过对比这次信号传递后，手的动作是不是”讨糖”动作，来修改人工神经网络当中的神经元强度。这种修改在专业术语中叫做”误差反向传递”，也可以看作是再一次将传过来的信号传回去，看看这个负责传递信号神经元对于”讨糖”的动作到底有没有贡献，让它好好反思与改正，争取下次做出更好的贡献。这样看来，人工神经网络和生物神经网络的确不是一回事。</p>
<h4 id="两者区别总结"><a href="#两者区别总结" class="headerlink" title="两者区别总结"></a>两者区别总结</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann5.png"></p>
<p>人工神经网络靠的是正向和反向传播来更新神经元，从而形成一个好的神经系统，本质上，这是一个能让计算机处理和优化的数学模型。而生物神经网络是通过刺激，产生新的联结，让信号能够通过新的联结传递而形成反馈。虽然现在的计算机技术越来越高超，不过我们身体里的神经系统经过了数千万年的进化，还是独一无二的，迄今为止，再复杂、再庞大的人工神经网络系统也不能替代我们的小脑袋。我们应该感到自豪，也应该珍惜上天的这份礼物。</p>
<h3 id="什么是神经网络-Neural-Network"><a href="#什么是神经网络-Neural-Network" class="headerlink" title="什么是神经网络 (Neural Network)"></a>什么是神经网络 (Neural Network)</h3><h4 id="内容简介"><a href="#内容简介" class="headerlink" title="内容简介"></a>内容简介</h4><p>这里提到的是人工神经网络，是存在于计算机里的神经系统。人工神经网络和自然神经网络的区别、神经网络是什么、它是怎么工作的、都会在影片里一一提到。</p>
<h3 id="神经网络-梯度下降"><a href="#神经网络-梯度下降" class="headerlink" title="神经网络 梯度下降"></a>神经网络 梯度下降</h3><p>欢迎观看有趣的机器学习系列视频，神经网络是当今为止最流行的一种深度学习框架，它的基本原理也很简单，就是一种梯度下降机制。我们今天就来看看这神奇的优化模式吧。</p>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p><img src="https://static.mofanpy.com/results/ML-intro/gd2.png"></p>
<p>学习机器学习的同学们常会遇到这样的图像，我了个天，看上去好复杂，哈哈，不过还挺好看的。这些和我们说的梯度下降又有什么关系呢？原来这些图片展示出来了一个家族的历史，这个家族的名字就是——”optimization” (优化问题)。优化能力是人类历史上的重大突破，他解决了很多实际生活中的问题。从而渐渐演化成了一个庞大的家族。</p>
<p>比如说牛顿法 (Newton’s method)、最小二乘法(Least Squares method)、梯度下降法 (Gradient Descent) 等等。而我们的神经网络就是属于梯度下降法这个分支中的一个。提到梯度下降，我们不得不说说大学里面学习过的求导求微分。因为这就是传说中”梯度下降”里面的”梯度” (gradient)啦。听到求导微分可别后怕，因为这个短视频只是让你有一个直观上的理解，并不会涉及太过复杂的东西。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gd3.png"></p>
<p>初学神经网络的时候，我们通常会遇到这样一个方程，叫做误差方程 (Cost Function)。用来计算预测出来的和我们实际中的值有多大差别。在预测数值的问题中，我们常用平方差 (Mean Squared Error) 来代替。我们简化一下这个方程，W是我们神经网络中的参数，x、y 都是我们的数据，因为 xy 都是实实在在的数据点，在这个假设情况中，是多少都无所谓，然后我们再厚颜无耻地像这样继续简化一下，(注意, 这个过程在在数学中并不正确, 不过我们只是为了看效果)，所以现在误差值曲线就成了这样。假设我们初始化的 W 在这个位置。而这个位置的斜率是这条线，这也就是梯度下降中的梯度啦。我们从图中可以看出，Cost 误差最小的时候正是这条 cost 曲线最低的地方，不过在蓝点的 W 却不知道这件事情，他目前所知道的就是梯度线为自己在这个位置指出的一个下降方向，我们就要朝着这个蓝色梯度的方向下降一点点。在做一条切线，发现我还能下降，那我就朝着梯度的方向继续下降，这时，再展示出现在的梯度，因为梯度线已经躺平了，我们已经指不出哪边是下降的方向了，所以这时我们就找到了 W 参数的最理想值。简而言之，就是找到梯度线躺平的点。可是神经网络的梯度下降可没这么简单。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/gd4.png"></p>
<p>神经网络中的 W 可不止一个，如果只有一个 W，我们就能画出之前那样的误差曲线，如果有两个 W 也简单，我们可以用一个3D 图来展示，可是超过3个 W，我们可就没办法很好的可视化出来啦。这可不是最要命的。在通常的神经网络中，误差曲线可没这么优雅。</p>
<h4 id="全局-and-局部最优"><a href="#全局-and-局部最优" class="headerlink" title="全局 and 局部最优"></a>全局 and 局部最优</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gd5.png"></p>
<p>在简化版的误差曲线中，我们只要找到梯度线躺平的地方，就能能迅速找到误差最小时的 W。可是很多情况是这样的，误差曲线并不只有一个沟，而且梯度躺平的点也不止一个。不同的 W 初始化的位置，将会带来不同的下降区域。不同的下降区域，又会带来不同的 W 解。在这个图像当中，W 的全局最优解(Global minima)在这个位置，而其它的解都是局部最优(Local minima)。全局最优固然是最好，但是很多时候，你手中的都是一个局部最优解，这也是无可避免的。不过你可以不必担心，因为虽然不是全局最优，但是神经网络也能让你的局部最优足够优秀，以至于即使拿着一个局部最优也能出色的完成手中的任务。</p>
<h3 id="科普-神经网络的黑盒不黑"><a href="#科普-神经网络的黑盒不黑" class="headerlink" title="科普: 神经网络的黑盒不黑"></a>科普: 神经网络的黑盒不黑</h3><p>今天我们来说说为了理解神经网络在做什么，对神经网络这个黑盒的正确打开方式。</p>
<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation1.png"></p>
<p>当然，这可不是人类的神经网络，因为至今我们都还没彻底弄懂人类复杂神经网络的运行方式。今天只来说说计算机中的人工神经网络。我们都听说过，神经网络是一个黑盒。</p>
<h4 id="黑盒"><a href="#黑盒" class="headerlink" title="黑盒"></a>黑盒</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation2.png"></p>
<p>呀，咋一看，的确挺黑的。我们还知道，如果你丢一个东西进这个黑盒，他会给你丢出来另一个东西。具体在黑盒里偷偷摸摸做了什么，我们不得而知。但丢出来的东西和丢进去的东西有着某些联系。这是为什么呢？这个黑盒里究竟又发生了什么呢？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation3.png"></p>
<p>正好我手边有一个手电筒，我们打开黑盒好好照亮看看。一般来说，神经网络是一连串神经层所组成的把输入进行加工再输出的系统。中间的加工过程就是我们所谓的黑盒。想把黑盒打开，就是把神经网络给拆开。按正常的逻辑，我们能将神经网络分成三部分。</p>
<h4 id="神经网络分区"><a href="#神经网络分区" class="headerlink" title="神经网络分区"></a>神经网络分区</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation4.png"></p>
<p>输入端，黑盒，输出端。输入端是我们能理解的物体，一个宝宝，输出端也是一个我们能理解的物体，一个奶瓶。对于神经网络，传统的理解就是，中间的这两层神经层在对输入信息进行加工，好让自己的输出信息和奶瓶吻合。但是我们如果换一个角度来想想。此时，我们将左边的红线移动一下</p>
<p>现在的输入端增加了一层，原本我们认定为黑盒的一部分被照亮，变成了一个已知部分。我们将最左边两层的神经层共同看成输入端。貌似怪怪的，你可能会问：可是这时的输入端不再是我们知道的”宝宝”了呀，为什么可以这样看？想得没错，它的确已经不是我们认识的宝宝啦，但是”宝宝”这个人类定义的形象通过了一层神经网络加工，变成了另外一种宝宝的形象，可能这种形象我们用肉眼看起来并不像宝宝，不过计算机却能理解，这是它所能识别的”宝宝”形象。在专业术语中，我们将宝宝当做特征(features)，将神经网络第一层加工后的宝宝叫做代表特征(feature representation)。如果再次移动红线，我们的黑盒就消失了，这时原本在黑盒里的所有神经层都被照亮。原本的代表特征再次被加工，变成了另一种代表特征，同样，再次加工形成的代表特征通常只有计算机自己看得懂，能够理解。所以，与其说黑盒是在加工处理，还不如说是在将一种代表特征转换成另一种代表特征，一次次特征之间的转换，也就是一次次的更有深度的理解。比如神经网络如果接收人类手写数字的图片。</p>
<h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation5.png"></p>
<p>然后我们将这个神经网络的输出层给拆掉，只留下前三层，那第3层输出的信息就是我们这些数字的3个最重要的代表特征，换句话说，就是用3个信息来代表整张手写数字图片的所有像素点。我们如果把这3个信息展示出来，我们就能很清楚的看到，计算机是如何用3个点来代表不同的数字内容，比如神经网络认为 1 和 0 是完全不同的，所以他们应该被放在空间里不同的地方。输出层就更好理解了，</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation6.png"></p>
<p>有了用3个点表示的数字代表特征，我们就能整理整理，将落在相同区域的数字分为一类，如果落在了那些1所在的区域，我们就认定张手写图片就是1，如果是2的区域，就认定为2。这就是神经网络的黑盒并不黑的原因啦，只是因为有时候代表特征太多了，我们人类没有办法看懂他们代表的是什么，然而计算机却能看清楚它所学到的规律，所以我们才觉得神经网络就是个黑盒。这种代表特征的理解方式其实非常有用，以至于人们拿着它来研究更高级的神经网络玩法。比如迁移学习(Transfer Learning)。我们举一个例子。</p>
<h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation7.png"></p>
<p>对于一个有分类能力的神经网络，有时候我们只需要这套神经网络的理解能力，并拿这种能力去处理其他问题。所以我们保留它的代表特征转换能力。因为有了这种能力，就能将复杂的图片像素信息转换成更少量，但更精辟的信息，比如刚刚我们说将手写数字变成的3个点信息。然后我们需要干点坏事，将这个神经网络的输出层给拆掉。套上另外一个神经网络，用这种移植的方式再进行训练，让它处理不同的问题，比如，预测照片里事物的价值。现在看来，这黑盒里开个灯，其实还挺有用的嘛。当你看不懂神经网络的时候，这样想想，是不是更好理解啦。</p>
<h3 id="Why-Pytorch"><a href="#Why-Pytorch" class="headerlink" title="Why Pytorch?"></a>Why Pytorch?</h3><h4 id="为什么用-PyTorch"><a href="#为什么用-PyTorch" class="headerlink" title="为什么用 PyTorch"></a>为什么用 PyTorch</h4><p><a href="http://pytorch.org/">PyTorch</a> 是 <a href="http://pytorch.org/">PyTorch</a> 在 Python 上的衍生。因为 <a href="http://pytorch.org/">PyTorch</a> 是一个使用 <a href="http://pytorch.org/">PyTorch</a> 语言的神经网络库，Torch 很好用，但是 Lua 又不是特别流行，所有开发团队将 Lua 的 Torch 移植到了更流行的语言 Python 上。是的 PyTorch 一出生就引来了剧烈的反响。为什么呢？</p>
<p>很简单，我们就看看有谁在用 PyTorch 吧。</p>
<p><img src="https://static.mofanpy.com/results-small/torch/1-1-1.png"></p>
<p>可见，著名的 Facebook、twitter 等都在使用它，这就说明 PyTorch 的确是好用的，而且是值得推广。</p>
<p>而且如果你知道 <a href="http://www.numpy.org/">Numpy</a>，PyTorch 说他就是在神经网络领域可以用来替换 numpy 的模块。</p>
<h4 id="神经网络在做什么"><a href="#神经网络在做什么" class="headerlink" title="神经网络在做什么"></a>神经网络在做什么</h4><p>神经网络在学习拟合线条(回归)：</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-2.gif"></p>
<p>神经网络在学习区分数据(分类)：</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-3.gif"></p>
<h4 id="PyTorch-和-Tensorflow"><a href="#PyTorch-和-Tensorflow" class="headerlink" title="PyTorch 和 Tensorflow"></a>PyTorch 和 Tensorflow</h4><p>据 PyTorch 自己介绍，他们家的最大优点就是建立的神经网络是动态的，对比静态的 Tensorflow，他能更有效地处理一些问题，比如说 RNN 变化时间长度的输出。而我认为，各家有各家的优势和劣势，所以我们要以中立的态度。两者都是大公司，Tensorflow 自己说自己在分布式训练上下了很大的功夫，那我就默认 Tensorflow 在这一点上要超出 PyTorch，但是 Tensorflow 的静态计算图使得他在 RNN 上有一点点被动 (虽然它用其他途径解决了)，不过用 PyTorch 的时候，你会对这种动态的 RNN 有更好的理解。</p>
<p>而且 Tensorflow 的高度工业化，它的底层代码…你是看不懂的。PyTorch 好那么一点点，如果你深入 API，你至少能比看 Tensorflow 多看懂一点点 PyTorch 的底层在干嘛。</p>
<p>最后我的建议就是：</p>
<ul>
<li>如果你是学生，随便选一个学，或者稍稍偏向 PyTorch，因为写代码的时候应该更好理解。懂了一个模块，转换 Tensorflow 或者其他的模块都好说。</li>
<li>如果是上班了，跟着你公司来，公司用什么，你就用什么，不要脱群。</li>
</ul>
<h4 id="Pytorch-安装"><a href="#Pytorch-安装" class="headerlink" title="Pytorch 安装"></a>Pytorch 安装</h4><p>对于MacBook，安装推荐参考：<a href="https://zhuanlan.zhihu.com/p/410961551">Mac M1安装tensorflow和pytorch</a></p>
<h4 id="支持的系统"><a href="#支持的系统" class="headerlink" title="支持的系统"></a>支持的系统</h4><p>PyTorch 暂时只支持 MacOS、Linux。暂不支持 Windows！(可怜的 Windows 同学们.. 又被抛弃了)。不过说不定像 Tensorflow 一样，因为 Windows 用户的强烈要求，他们在某天就突然支持了。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>PyTorch 安装起来很简单，<a href="http://pytorch.org/">它自家网页</a>上就有很方便的选择方式 (网页升级改版后可能和下图有点不同)：</p>
<p><img src="https://static.mofanpy.com/results-small/torch/1-2-1.png"></p>
<p>所以根据你的情况选择适合你的安装方法，我已自己为例，我使用的是 MacOS，想用 pip 安装，我的 Python 是 3.5 版的，我没有 GPU 加速，那我就按上面的选：</p>
<p>然后根据上面的提示，我只需要在我的 Terminal 当中输入以下指令就好了：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install http://download.pytorch.org/whl/torch-0.1.11.post5-cp35-cp35m-macosx_10_7_x86_64.whl</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install torchvision</span></span><br></pre></td></tr></table></figure>

<p>注意，我安装的是0.1.11版本的 torch，你需要去他们网站上看是否有新版本的。安装 PyTorch 会安装两个模块，一个是 torch，一个 torchvision。torch 是主模块，用来搭建神经网络的，torchvision 是辅模块，有数据库，还有一些已经训练好的神经网络等着你直接用，比如 (<a href="http://pytorch.org/docs/torchvision/models.html">VGG, AlexNet, ResNet</a>)。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li>Python 做神经网络 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow 教程</a></li>
<li>Python 做神经网络 <a href="https://mofanpy.com/tutorials/machine-learning/torch/">Pytorch 教程</a></li>
<li>有网友根据我的 Tensorflow 系列做了一个很好的<a href="http://www.jianshu.com/p/e112012a4b2d">文字笔记</a>, 推荐阅读.</li>
<li>Tensorflow <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">学习目录</a></li>
<li>PyTorch <a href="https://mofanpy.com/tutorials/machine-learning/torch/">学习目录</a></li>
<li>Theano <a href="https://mofanpy.com/tutorials/machine-learning/theano/">学习目录</a></li>
<li>Keras <a href="https://mofanpy.com/tutorials/machine-learning/keras/">学习目录</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-NN">什么是神经网络 短视频</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>XCTF-逆向基础篇</title>
    <url>/YingYingMonstre.github.io/2021/09/26/XCTF-%E9%80%86%E5%90%91%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
    <content><![CDATA[<h2 id="insanity"><a href="#insanity" class="headerlink" title="insanity"></a>insanity</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/1.png" alt="img"></p>
<p>发现这不是PE文件,是ELF文件，将程序在Linux环境下运行</p>
<p>0x02,查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/2.png" alt="img"></p>
<p>发现是32位的程序</p>
<p>0x03.IDA </p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/3.png" alt="img"></p>
<p>F5查看伪函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/4.png" alt="img"></p>
<p>发现一个关键的字符串，&amp;strs,发现是取这个字符串输出，然后，跟进strs</p>
<p>(shift + f12 字符串窗口)</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/5.png" alt="img"></p>
<p>发现有一个明显的提示：This_is_a_flag</p>
<p>猜测9447{This_is_a_flag}是最后的falg</p>
<p>或者直接用记事本打开，仔细找也能找到。</p>
<h2 id="python-trade"><a href="#python-trade" class="headerlink" title="python-trade"></a>python-trade</h2><p><strong>[工具]</strong></p>
<p>在线python反编译</p>
<p><strong>[分析过程]</strong></p>
<p>0x01.下载附件</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/1.png" alt="img"></p>
<p>注：</p>
<p>pyc文件是py文件编译后生成的字节码文件</p>
<p>0x02.在线Python反编译</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/2.png" alt="img"></p>
<p>这是生成的py文件</p>
<p>然后，对这个文件的运算逻辑进行逆向</p>
<p>0x03.写EXP</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line">correct = <span class="string">&quot;XlNkVmtUI1MgXWBZXCFeKY+AaXNt&quot;</span></span><br><span class="line">s = base64.b64decode(correct)</span><br><span class="line">flag = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">    flag += <span class="built_in">chr</span>((i-<span class="number">16</span>) ^ <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(flag)</span><br></pre></td></tr></table></figure>

<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/3.png" alt="img"></p>
<p>先对字符串进行b64decode,然后，再进行xor运算得到最后的flag:nctf{d3c0mpil1n9_PyC}</p>
<p>0x04.运行脚本</p>
<p>nctf{d3c0mpil1n9_PyC}</p>
<h2 id="re1"><a href="#re1" class="headerlink" title="re1"></a>re1</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>参考</p>
<p>0x01.运行程序</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/1.png" alt="img"></p>
<p>可以看到需要输入正确的flag</p>
<p>那么现在，我们需要判断程序是多少位的，有没有加壳</p>
<p>0x02.exeinfope查详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/2.png" alt="img"></p>
<p>可以看到程序是32位的，是Microsoft Visual c++编译的，并且没有加壳</p>
<p>注：查壳工具还有PEID，EID，但是推荐EID或者exeinfope，因为，PEID查壳的时候有时候不准确</p>
<p>那么，我们可以用静态分析神器 IDA 打开，进一步分析了</p>
<p>0x03.</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/3.png" alt="img"></p>
<p>然后，查找主函数main,可以看到右侧的是反汇编的汇编代码，这时候，我们可以直接分析汇编语言，但是，汇编语言看起来太多，费劲。这个时候就可以是有IDA是最强大的功能F5了，它能够直接将汇编代码生成C语言代码，虽然和这个程序的源码不完全一样，但是逻辑关系是一样的</p>
<p>F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/4.png" alt="img"></p>
<p>这是整个main函数的运算逻辑</p>
<p>可以看到一个关键的字符串，print(aFlag)，那么证明这就是输入正确flag，然后，会输出aFlag证明你的flag正确，然后，继续往上分析，可以看到v3的值，是由strcmp()决定的，比较v5和输入的字符串，如果一样就会进入后面的if判断，所以,我们继续往上分析，看看哪里又涉及v5，可以看到开头的_mm_storeu_si128(），对其进行分析发现它类似于memset(),将xmmword_413E34的值赋值给v5，所以，我们可以得到正确的flag应该在xmmword_413E34中，然后，我们双击413E34进行跟进</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/5.png" alt="img"></p>
<p>可以看到一堆十六进制的数</p>
<p>这时，我们使用IDA的另一个功能 R ，能够将十六进制的数转换为字符串。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/6.png" alt="img"></p>
<p>这就是我们最后的flag了</p>
<p>注：这里要跟大家普及一个知识了，及大端与小端</p>
<p>假设一个十六进制数0x12345678</p>
<p>大端的存储方式是：12,34,56,78，然后读取的时候也是从前往后读</p>
<p>小端的存储方式是：78,56,34,12，然后读取的时候是从后往前读取</p>
<p>所以，最后的flag应该是：DUTCTF{We1c0met0DUTCTF}</p>
<p>0x04.运行程序输入正确的flag</p>
<p>方法二：记事本打开，也能找到。</p>
<p>方法三：OD–》插件–》中文搜索引擎–》ASCII也能搜到。</p>
<h2 id="game"><a href="#game" class="headerlink" title="game"></a>game</h2><p>我的做法（暴力破解）：先用exeinfo pe查壳，发现是32位未加壳。用OD打开，用插件里的中文搜索ASCII，能找到done flag is的字符串。然后找到关键CALL（00B301BB和00B30359），修改跳转（从%d那里直接改为CALL xxxx跳到done那里）。运行到那里之后就可以得到flag了。</p>
<p>直接玩游戏：从1输到8。</p>
<p>爆破方法二：找到F5后伪代码最后那部分判断的代码（空格切换到图形视图，对着最后的那部分再空格切换回来），patch修改，正好有8个JNZ，改5个为jz，然后Edit–》Patch program–》Apply patches to input file，点OK，再回去运行就可以得到了。</p>
<p>IDA分析代码逻辑：先是判断是输入的是否是1-8，然后进入后面的if判断然后进行循环，这个时候应该就是程序的亮暗的显示，然后，如果byte_532E28每一位都是1，那么，就会进入sub_457AB4,然后我们猜测这里应该就是最后的flag的地方。然后跟进 sub_457AB4。（注：这里说明一下，如果IDA不能正确的获得自定义函数的名字，那么IDA会用sub__加上自定义函数的起始地址来定义函数的名字）</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/8.png" alt="img"></p>
<p>这里只截取了后面的部分，发现函数进行了两次xor运算，xor的逆运算也是xor，那么我们就可以根据这个运算来写脚本得到最后的flag。</p>
<p>这里看到v2和v59这就证明了这是两个数组的运算，所以我们应该将上面的字符串分成两个数组，分别从v2和v59开始</p>
<p>0x05.写EXP</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/9.png" alt="img"></p>
<p>这里先是通过循环，将a和b数组的值进行xor运算，然后再将数组a的值与0x13xor运算</p>
<p>chr()：是将十六进制转换为字符串</p>
<p>0x05.运行脚本</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/10.png" alt="img"></p>
<p>得到最后的flag: zsctf{T9is_tOpic_1s_v5ry_int7resting_b6t_others_are_n0t}。</p>
<p>广度优先搜索法：未证。</p>
<h2 id="Hello，CTF"><a href="#Hello，CTF" class="headerlink" title="Hello，CTF"></a>Hello，CTF</h2><p>查壳，32位、无壳。</p>
<p>IDA从main开始分析，F5查看伪代码。首先，可以看到先是将字符串复制到v13的位置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">strcpy(&amp;v13, &quot;437261636b4d654a757374466f7246756e&quot;);</span><br></pre></td></tr></table></figure>

<p>然后，后面对输入进行了判断，输入的字符串不能大于17接着，将字符串以十六进制输出，然后，再将得到的十六进制字符添加到v10最后，进行比较，看输入的字符串是否和v10的字符串相等，如果相等，则得到真确的flag。最后将字符串转换为十六进制。</p>
<h2 id="open-source"><a href="#open-source" class="headerlink" title="open-source"></a>open-source</h2><p>代码审计：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">first = <span class="number">0xcafe</span></span><br><span class="line">flag = first * <span class="number">31337</span> + (second % <span class="number">17</span>) * <span class="number">11</span> + <span class="built_in">len</span>(<span class="string">&quot;h4cky0u&quot;</span>) - <span class="number">1615810207</span></span><br></pre></td></tr></table></figure>

<p>关键在于second的取值。观察代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> second = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">if</span> (second % <span class="number">5</span> == <span class="number">3</span> || second % <span class="number">17</span> != <span class="number">8</span>) &#123;</span><br><span class="line">    	<span class="built_in">printf</span>(<span class="string">&quot;ha, you won&#x27;t get it!\n&quot;</span>);</span><br><span class="line">    	<span class="built_in">exit</span>(<span class="number">3</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>这里想到要get it就要将if里的逻辑取反，即second % 5 != 3 &amp;&amp; second % 17 == 8。</p>
<p>故(second % 17) * 11=8*11=88。得到flag=12648430–化为16进制–》c0ffee。</p>
<h2 id="simple-unpack"><a href="#simple-unpack" class="headerlink" title="simple_unpack"></a>simple_unpack</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, upx </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/1.png" alt="img"></p>
<p>发现有upx壳。</p>
<p>注：windows下的文件是PE文件，Linux/Unix下的文件是ELF文件</p>
<p>0x02.UPX 脱壳</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/2.png" alt="img"></p>
<p>upx -d 即可对upx壳进行脱壳</p>
<p>0x03.载入IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/3.png" alt="img"></p>
<p>还是从main函数开始分析，结果我们再右侧发现了意外惊喜</p>
<p>运行程序，输入我们看到的flag:flag{Upx_1s_n0t_a_d3liv3r_c0mp4ny}</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/4.png" alt="img"></p>
<h2 id="logmein"><a href="#logmein" class="headerlink" title="logmein"></a>logmein</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/1.png" alt="img"></p>
<p>发现程序是一个ELF文件，将其放入Linux环境中进行分析</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/2.png" alt="img"></p>
<p>发现程序是64位的，使用静态分析工具IDA进行分析</p>
<p>0x02.IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/3.png" alt="img"></p>
<p>从main函数开始分析，使用F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/4.png" alt="img"></p>
<p>发现main函数的整个运算逻辑</p>
<p>先是，将指定字符串复制到v8</p>
<p>s是用户输入的字符串，先进行比较长度，如果长度比v8小，则进入sub_4007c0函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/5.png" alt="img"></p>
<p>可以看出输出字符串Incorrect password,然后，退出</p>
<p>如果长度大于或等与v8则进入下面的循环</p>
<p>看到判断如果输入的字符串和经过运算后的后字符串不等，则进入sub_4007c0,输出Incorrect password,</p>
<p>如果想得，则进入sub_4007f0函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/6.png" alt="img"></p>
<p>证明输入的字符串就是flag</p>
<p>接下来写脚本</p>
<p>0x03.Write EXP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">v7 &#x3D; &quot;harambe&quot;</span><br><span class="line">v6 &#x3D; 7</span><br><span class="line">v8 &#x3D; &quot;:\&quot;AL_RT^L*.?+6&#x2F;46&quot;</span><br><span class="line">flag &#x3D; &quot;&quot;</span><br><span class="line">for i in range(0, len(v8)):</span><br><span class="line">    flag +&#x3D; chr(ord((v7[i % 7])) ^ ord(v8[i]))</span><br><span class="line">print(flag)</span><br></pre></td></tr></table></figure>

<p>由于程序是小段的存储方式，所以，ebmarah就得变成harambe（C语言数据在内存中是小端存储，一开始v7是一个数据，so）</p>
<p>ord():是将字符串转换为ascii格式，为了方便运算</p>
<p>chr():是将ascii转换为字符串</p>
<p>运行脚本得到最后的flag:RC3-2016-XORISGUD</p>
<p>（直接用C语言更方便些）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#define BYTE unsigned char</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void main() &#123;</span><br><span class="line">	int i;</span><br><span class="line">	int v6 &#x3D; 7;</span><br><span class="line">	__int64 v7 &#x3D; 28537194573619560LL;</span><br><span class="line">	char v8[18] &#x3D; &quot;:\&quot;AL_RT^L*.?+6&#x2F;46&quot;;</span><br><span class="line">	char s[18] &#x3D; &quot;&quot;;</span><br><span class="line">	for ( i &#x3D; 0; i &lt; strlen(v8); ++i ) &#123;</span><br><span class="line">		s[i] +&#x3D; (char)(*((BYTE*)&amp;v7 + i % v6) ^ v8[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	printf(&quot;%s\n&quot;,s);</span><br><span class="line"></span><br><span class="line">	system(&quot;pause&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="no-strings-attached"><a href="#no-strings-attached" class="headerlink" title="no-strings-attached"></a>no-strings-attached</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, GDB</p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/1.png" alt="img"></p>
<p>说明程序是ELF文件，32位</p>
<p>0x02.使用静态分析工具IDA进行分析</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/2.png" alt="img"></p>
<p>然后对main函数使用F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/3.png" alt="img"></p>
<p>然后，对每个函数进行跟进，最后发现authenricate(),符合获得flag的函数，对其进行跟进</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/4.png" alt="img"></p>
<p>然后我们发现一个特殊的函数decrypt,根据字面的意思是加密，那么我们可以大概的猜测是一个对dword_8048A90所对应的字符串进行加密，</p>
<p>加密得到的就应该是我们需要的flag，后面的判断应该就是将字符串输出。</p>
<p>这里我们有两种思维方式:</p>
<p>第一种就是跟进decrypt然后分析它的运算逻辑，然后，自己写脚本，得到最后的flag</p>
<p>第二种就涉及逆向的另一种调试方式，及动态调试，这里我就用动态调试了，之前的一直是静态调试</p>
<p>0x03.GDB动态调试</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/5.png" alt="img"></p>
<p>gdb ./no_strings_attached 将文件加载到GDB中</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/6.png" alt="img"></p>
<p>既然是动态调试，那么如果让它一直不停，那我不就相当于运行了嘛，所以，我们就需要下断点，断点就是让程序运行到断点处就停止</p>
<p>之前通过IDA，我们知道关键函数是decrypt,所以我们把断点设置在decrypt处，b在GDB中就是下断点的意思，及在decrypt处下断点</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/7.png" alt="img"></p>
<p>r就是运行的意思，这里运行到了我们之前下的断点处，停止。</p>
<p>我们要的是经过decrypt函数，生成的字符串，所以我们这里就需要运行一步，GDB中用n来表示运行一步</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/8.png" alt="img"></p>
<p>然后我们就需要去查看内存了，去查找最后生成的字符串</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/9.png" alt="img"></p>
<p>通过IDA生成的汇编指令，我们可以看出进过decrypt函数后，生成的字符串保存在EAX寄存器中，所以，我们在GDB就去查看eax寄存器的值</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/10.png" alt="img"></p>
<p>x:就是用来查看内存中数值的，后面的200代表查看多少个，wx代表是以word字节查看看，$eax代表的eax寄存器中的值</p>
<p>在这里我们看到0x00000000，这就证明这个字符串结束了，因为，在C中，代表字符串结尾的就是”\0”,那么前面的就是经过decrypt函数生成的falg</p>
<p>那我们就需要将这些转换为字符串的形式</p>
<p>0x04.Write EXP</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/11.png" alt="img"></p>
<p>首先将寄存器中的值提取出来，然后利用Python的decode函数，通过”hex”的方式转化为字符串，然后输出</p>
<p>0x05.运行脚本</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/12.png" alt="img"></p>
<p>得到最后的flag: 9447{you_are_an_international_mystery}</p>
<p>IDA分析就分析它的decrypt函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; [0x3a, 0x36, 0x37, 0x3b, 0x80, 0x7a, 0x71, 0x78,</span><br><span class="line">     0x63, 0x66, 0x73, 0x67, 0x62, 0x65, 0x73, 0x60,</span><br><span class="line">     0x6b, 0x71, 0x78, 0x6a, 0x73, 0x70, 0x64, 0x78,</span><br><span class="line">     0x6e, 0x70, 0x70, 0x64, 0x70, 0x64, 0x6e, 0x7b,</span><br><span class="line">     0x76, 0x78, 0x6a, 0x73, 0x7b, 0x80]</span><br><span class="line">v6 &#x3D; len(s)</span><br><span class="line">a2 &#x3D; [1, 2, 3, 4, 5]</span><br><span class="line">v7 &#x3D; len(a2)</span><br><span class="line">v2 &#x3D; v6</span><br><span class="line">dest &#x3D; s</span><br><span class="line">v4 &#x3D; 0</span><br><span class="line">while v4 &lt; v6:</span><br><span class="line">        dest[v4] -&#x3D; a2[v4 % 5]</span><br><span class="line">        v4 +&#x3D; 1</span><br><span class="line"></span><br><span class="line">flag &#x3D; &#39;&#39;</span><br><span class="line">for j in dest:</span><br><span class="line">    flag +&#x3D; chr(j)</span><br><span class="line">print(flag)</span><br></pre></td></tr></table></figure>



<h2 id="getit"><a href="#getit" class="headerlink" title="getit"></a>getit</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, GDB  </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/1.png" alt="img"></p>
<p>可以看出这是一个ELF文件，64位</p>
<p>0x02.IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/2.png" alt="img"></p>
<p>对main函数进行F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/3.png" alt="img"></p>
<p>可以看到先判断v5是否大于s存储字符串的长度，然后通过运算，最后将得到的flag写入文件。</p>
<p>但是有意思的地方在flag.txt文件所在的位置是/tmp目录，这个目录是Linux下的临时文件夹，程序运行完，生成flag的txt文件被清理了，所以我们找不到文件</p>
<p>我们这时候通过IDA查看汇编代码，按空格键可以生成所有的汇编文件</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/4.png" alt="img"></p>
<p>然后我们向下追踪，追踪到for循环的位置，因为，flag是在这里存入文件的，所以，我们可以在内存中找到正要存储的字符串</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/5.png" alt="img"></p>
<p>我们将鼠标指向strlen(),在下面可以看到汇编所在的地址，然后我们根据大概的地址去看汇编代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/6.png" alt="img"></p>
<p>可以看到这是调用strlen()函数的汇编指令</p>
<p>我们通过上一个图片，可以知道经过for()的判断条件后，还要进行一步fseek函数，所以，根据汇编代码，可以确定jnb loc_4008B5就是fseek()函数，那么，mov eax,[rbp+var_3C]肯定就是最后要得到的flag了</p>
<p>0x04.GDB</p>
<p>这里我们用linux下的动态调试工具gdb进行动态调试，这里介绍一下，对gdb进行强化的两个工具peda和pwndbg，这两个工具可以强化视觉效果，可以更加清楚的显示堆栈，内存，寄存机的情况</p>
<p>先加载程序</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/7.png" alt="img"></p>
<p>然后，用b 下断点</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/8.png" alt="img"></p>
<p>然后，运行 R</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/9.png" alt="img"></p>
<p>这里我们可以看出，程序停止在0x400832的位置，然后，要被移动的字符串在RDX的位置</p>
<p>注：</p>
<p>这里介绍一下一下RDX，RDX存的是i/0指针，0x6010e0,这个位置存的字符串是最后的flag:SharifCTF{b70c59275fcfa8aebf2d5911223c6589}</p>
<p>以为这里涉及的是程序读写函数，所以涉及的就是i/o指针</p>
<p>所以我们能得到最后的flag: SharifCTF{b70c59275fcfa8aebf2d5911223c6589}</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; &quot;c61b68366edeb7bdce3c6820314b7498&quot;</span><br><span class="line">v3 &#x3D; 0</span><br><span class="line">v5 &#x3D; 0</span><br><span class="line">t &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">while v5 &lt; len(s):</span><br><span class="line">    if v5 &amp; 1:</span><br><span class="line">        v3 &#x3D; 1</span><br><span class="line">    else:</span><br><span class="line">        v3 &#x3D; -1</span><br><span class="line">    t +&#x3D; chr(ord(s[v5]) + v3)</span><br><span class="line">    v5 +&#x3D; 1</span><br><span class="line"></span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>



<h2 id="csaw2013reversing2"><a href="#csaw2013reversing2" class="headerlink" title="csaw2013reversing2"></a>csaw2013reversing2</h2><p><a href="https://blog.csdn.net/weixin_43784056/article/details/103655968">XCTF-csaw2013reversing2_臭nana的博客-CSDN博客</a></p>
<p>默认if条件不成立，跳过了sub_401000的解码，输出一堆乱码。</p>
<h2 id="maze"><a href="#maze" class="headerlink" title="maze"></a>maze</h2><p>迷宫问题</p>
<p><img src="https://www.pianshen.com/images/271/7d91ebf895b8ed648ed3ab80de463137.JPEG" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>XCTF</tag>
        <tag>逆向</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（QLearning）</title>
    <url>/YingYingMonstre.github.io/2021/11/08/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88QLearning%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-Q-Learning"><a href="#什么是-Q-Learning" class="headerlink" title="什么是 Q Learning"></a>什么是 Q Learning</h3><p>今天我们会来说说强化学习中一个很有名的算法——Q-learning。</p>
<h4 id="行为准则"><a href="#行为准则" class="headerlink" title="行为准则"></a>行为准则</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q1.png"></p>
<p>我们做事情都会有一个自己的行为准则，比如小时候爸妈常说”不写完作业就不准看电视”。所以我们在写作业的这种状态下，好的行为就是继续写作业，直到写完它，我们还可以得到奖励。不好的行为就是没写完就跑去看电视了，被爸妈发现，后果很严重。小时候这种事情做多了，也就变成我们不可磨灭的记忆。这和我们要提到的Q learning 有什么关系呢?原来 Q learning 也是一个决策过程，和小时候的这种情况差不多。我们举例说明。</p>
<p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视，所以现在我们有两种选择：1.继续写作业，2. 跑去看电视。因为以前没有被罚过，所以我选看电视，然后现在的状态变成了看电视，我又选了继续看电视，接着我还是看电视。最后爸妈回家，发现我没写完作业就去看电视了，狠狠地惩罚了我一次，我也深刻地记下了这一次经历，并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为，我们在看看 Q learning 根据很多这样的经历是如何来决策的吧。</p>
<h4 id="QLearning-决策"><a href="#QLearning-决策" class="headerlink" title="QLearning 决策"></a>QLearning 决策</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q2.png"></p>
<p>假设我们的行为准则已经学习好了，现在我们处于状态s1，我在写作业，我有两个行为 a1、a2，分别是看电视和写作业，根据我的经验，在这种 s1 状态下, a2 写作业带来的潜在奖励要比 a1 看电视高，这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替，在我的记忆Q表格中, Q(s1, a1)=-2 要小于 Q(s1, a2)=1，所以我们判断要选择 a2 作为下一个行为。现在我们的状态更新成 s2，我们还是有两个同样的选择，重复上面的过程，在行为准则Q表中寻找 Q(s2, a1) Q(s2, a2) 的值, 并比较他们的大小，选取较大的一个。接着根据 a2 我们到达 s3 并在此重复上面的决策过程。Q learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改，提升的。</p>
<h4 id="QLearning-更新"><a href="#QLearning-更新" class="headerlink" title="QLearning 更新"></a>QLearning 更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q3.png"></p>
<p>所以我们回到之前的流程，根据 Q 表的估计，因为在 s1 中, a2 的值比较大，通过之前的决策方法，我们在 s1 采取了 a2，并到达 s2，这时我们开始更新用于决策的 Q 表，接着我们并没有在实际中采取任何行为，而是想象自己在 s2 上采取了每种行为，分别看看两种行为哪一个的 Q 值大。比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0)，因为会获取实实在在的奖励 R，我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值。所以有了现实和估计值，我们就能更新Q(s1, a2)，根据估计与现实的差距，将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值 变成新的值。但时刻记住，我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做。这就是 off-policy 的 Q learning 是如何决策和学习优化决策的过程。</p>
<h4 id="QLearning-整体算法"><a href="#QLearning-整体算法" class="headerlink" title="QLearning 整体算法"></a>QLearning 整体算法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q4.png"></p>
<p>这一张图概括了我们之前所有的内容。这也是 Q learning 的算法，每次更新我们都用到了 Q 现实和 Q 估计，而且 Q learning 的迷人之处就是在 Q(s1, a2) 现实中，也包含了一个 Q(s2) 的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实，很奇妙吧。最后我们来说说这套算法中一些参数的意义。Epsilon greedy 是用在决策上的一种策略，比如 epsilon = 0.9 时，就说明有90% 的情况我会按照 Q 表的最优值选择行为，10% 的时间使用随机选行为。alpha是学习率，来决定这次的误差有多少是要被学习的，alpha是一个小于1的数。gamma 是对未来 reward 的衰减值。我们可以这样想象。</p>
<h4 id="QLearning-中的-Gamma"><a href="#QLearning-中的-Gamma" class="headerlink" title="QLearning 中的 Gamma"></a>QLearning 中的 Gamma</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q5.png"></p>
<p>我们重写一下 Q(s1) 的公式，将 Q(s2) 拆开，因为Q(s2)可以像 Q(s1)一样，是关于Q(s3) 的，所以可以写成这样。然后以此类推，不停地这样写下去，最后就能写成这样，可以看出Q(s1) 是有关于之后所有的奖励，但这些奖励正在衰减，离 s1 越远的状态衰减越严重。不好理解? 行，我们想象 Qlearning 的机器人天生近视眼，gamma = 1 时，机器人有了一副合适的眼镜，在 s1 看到的 Q 是未来没有任何衰变的奖励，也就是机器人能清清楚楚地看到之后所有步的全部价值；但是当 gamma =0，近视机器人没了眼镜，只能摸到眼前的 reward，同样也就只在乎最近的大奖励；如果 gamma 从 0 变到 1，眼镜的度数由浅变深，对远处的价值看得越清楚，所以机器人渐渐变得有远见，不仅仅只看眼前的利益，也为自己的未来着想。</p>
<h3 id="小例子"><a href="#小例子" class="headerlink" title="小例子"></a>小例子</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>这一次我们会用 tabular Q-learning 的方法实现一个小例子，例子的环境是一个一维世界，在世界的右边有宝藏，探索者只要得到宝藏尝到了甜头，然后以后就记住了得到宝藏的方法，这就是他用强化学习所学习到的行为。</p>
<blockquote>
<p>-o—T</p>
<p>T 就是宝藏的位置, o 是探索者的位置</p>
</blockquote>
<p>Q-learning 是一种记录行为值 (Q value) 的方法，每种在一定状态的行为都会有一个值 <code>Q(s, a)</code>，就是说 行为 <code>a</code> 在 <code>s</code> 状态的值是 <code>Q(s, a)</code>。<code>s</code> 在上面的探索者游戏中，就是 <code>o</code> 所在的地点了。而每一个地点探索者都能做出两个行为 <code>left/right</code>，这就是探索者的所有可行的 <code>a</code> 啦。</p>
<p>如果在某个地点 <code>s1</code>，探索者计算了他能有的两个行为, <code>a1/a2=left/right</code>，计算结果是 <code>Q(s1, a1) &gt; Q(s1, a2)</code>，那么探索者就会选择 <code>left</code> 这个行为。这就是 Q learning 的行为选择简单规则。</p>
<p>==当然我们还会细说更具体的规则。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容，大家大概看看就行，有个大概的 RL 概念就行，知道 RL 的一些关键步骤就行，这节的算法不用仔细研究。==</p>
<h4 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h4><p>这一次需要的模块和参数设置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">N_STATES = <span class="number">6</span>   <span class="comment"># 1维世界的宽度</span></span><br><span class="line">ACTIONS = [<span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;right&#x27;</span>]     <span class="comment"># 探索者的可用动作</span></span><br><span class="line">EPSILON = <span class="number">0.9</span>   <span class="comment"># 贪婪度 greedy</span></span><br><span class="line">ALPHA = <span class="number">0.1</span>     <span class="comment"># 学习率</span></span><br><span class="line">GAMMA = <span class="number">0.9</span>    <span class="comment"># 奖励递减值</span></span><br><span class="line">MAX_EPISODES = <span class="number">13</span>   <span class="comment"># 最大回合数</span></span><br><span class="line">FRESH_TIME = <span class="number">0.3</span>    <span class="comment"># 移动间隔时间</span></span><br></pre></td></tr></table></figure>



<h4 id="Q-表"><a href="#Q-表" class="headerlink" title="Q 表"></a>Q 表</h4><p>对于 tabular Q learning，我们必须将所有的 Q values (行为值) 放在 <code>q_table</code> 中，更新 <code>q_table</code> 也是在更新他的行为准则。<code>q_table</code> 的 index 是所有对应的 <code>state</code> (探索者位置)，columns 是对应的 <code>action</code> (探索者行为)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_q_table</span>(<span class="params">n_states, actions</span>):</span></span><br><span class="line">    table = pd.DataFrame(</span><br><span class="line">        np.zeros((n_states, <span class="built_in">len</span>(actions))),     <span class="comment"># q_table 全 0 初始</span></span><br><span class="line">        columns=actions,    <span class="comment"># columns 对应的是行为名称</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> table</span><br><span class="line"></span><br><span class="line"><span class="comment"># q_table:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   left  right</span></span><br><span class="line"><span class="string">0   0.0    0.0</span></span><br><span class="line"><span class="string">1   0.0    0.0</span></span><br><span class="line"><span class="string">2   0.0    0.0</span></span><br><span class="line"><span class="string">3   0.0    0.0</span></span><br><span class="line"><span class="string">4   0.0    0.0</span></span><br><span class="line"><span class="string">5   0.0    0.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="定义动作"><a href="#定义动作" class="headerlink" title="定义动作"></a>定义动作</h4><p>接着定义探索者是如何挑选行为的。这是我们引入 <code>epsilon greedy</code> 的概念。因为在初始阶段，随机的探索环境，往往比固定的行为模式要好，所以这也是累积经验的阶段，我们希望探索者不会那么贪婪(greedy)。所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。<code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪)，不过在这个例子中，我们就固定成 <code>EPSILON = 0.9</code>，90% 的时间是选择最优策略，10% 的时间来探索。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在某个 state 地点, 选择行为</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">state, q_table</span>):</span></span><br><span class="line">    state_actions = q_table.iloc[state, :]  <span class="comment"># 选出这个 state 的所有 action 值</span></span><br><span class="line">    <span class="keyword">if</span> (np.random.uniform() &gt; EPSILON) <span class="keyword">or</span> (state_actions.<span class="built_in">all</span>() == <span class="number">0</span>):  <span class="comment"># 非贪婪 or 或者这个 state 还没有探索过</span></span><br><span class="line">        action_name = np.random.choice(ACTIONS)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># action_name = state_actions.argmax()不能正常运行</span></span><br><span class="line">        <span class="comment"># argmax()返回的是序列中最大值的int位置</span></span><br><span class="line">        <span class="comment"># idxmax()返回的是最大值的行标签</span></span><br><span class="line">        action_name = state_actions.idxmax()    <span class="comment"># 贪婪模式</span></span><br><span class="line">    <span class="keyword">return</span> action_name</span><br></pre></td></tr></table></figure>



<h4 id="环境反馈-S-R"><a href="#环境反馈-S-R" class="headerlink" title="环境反馈 S_, R"></a>环境反馈 S_, R</h4><p>做出行为后，环境也要给我们的行为一个反馈，反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R)。这里定义的规则就是，只有当 <code>o</code> 移动到了 <code>T</code>，探索者才会得到唯一的一个奖励，奖励值 R=1，其他情况都没有奖励。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_env_feedback</span>(<span class="params">S, A</span>):</span></span><br><span class="line">    <span class="comment"># This is how agent will interact with the environment</span></span><br><span class="line">    <span class="keyword">if</span> A == <span class="string">&#x27;right&#x27;</span>:    <span class="comment"># move right</span></span><br><span class="line">        <span class="keyword">if</span> S == N_STATES - <span class="number">2</span>:   <span class="comment"># terminate</span></span><br><span class="line">            S_ = <span class="string">&#x27;terminal&#x27;</span></span><br><span class="line">            R = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            S_ = S + <span class="number">1</span></span><br><span class="line">            R = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:   <span class="comment"># move left</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> S == <span class="number">0</span>:</span><br><span class="line">            S_ = S  <span class="comment"># reach the wall</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            S_ = S - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> S_, R</span><br></pre></td></tr></table></figure>



<h4 id="环境更新"><a href="#环境更新" class="headerlink" title="环境更新"></a>环境更新</h4><p>接下来就是环境的更新了，不用细看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_env</span>(<span class="params">S, episode, step_counter</span>):</span></span><br><span class="line">    <span class="comment"># This is how environment be updated</span></span><br><span class="line">    env_list = [<span class="string">&#x27;-&#x27;</span>]*(N_STATES-<span class="number">1</span>) + [<span class="string">&#x27;T&#x27;</span>]   <span class="comment"># &#x27;---------T&#x27; our environment</span></span><br><span class="line">    <span class="keyword">if</span> S == <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">        interaction = <span class="string">&#x27;Episode %s: total_steps = %s&#x27;</span> % (episode+<span class="number">1</span>, step_counter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(interaction), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r                                &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        env_list[S] = <span class="string">&#x27;o&#x27;</span></span><br><span class="line">        interaction = <span class="string">&#x27;&#x27;</span>.join(env_list)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(interaction), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        time.sleep(FRESH_TIME)</span><br></pre></td></tr></table></figure>



<h4 id="强化学习主循环"><a href="#强化学习主循环" class="headerlink" title="强化学习主循环"></a>强化学习主循环</h4><p>最重要的地方就在这里。你定义的 RL 方法都在这里体现。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容，大家大概看看就行，这节内容不用仔细研究。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/2-1-1.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rl</span>():</span></span><br><span class="line">    q_table = build_q_table(N_STATES, ACTIONS)  <span class="comment"># 初始 q table</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPISODES):     <span class="comment"># 回合</span></span><br><span class="line">        step_counter = <span class="number">0</span></span><br><span class="line">        S = <span class="number">0</span>   <span class="comment"># 回合初始位置</span></span><br><span class="line">        is_terminated = <span class="literal">False</span>   <span class="comment"># 是否回合结束</span></span><br><span class="line">        update_env(S, episode, step_counter)    <span class="comment"># 环境更新</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_terminated:</span><br><span class="line"></span><br><span class="line">            A = choose_action(S, q_table)   <span class="comment"># 选行为</span></span><br><span class="line">            S_, R = get_env_feedback(S, A)  <span class="comment"># 实施行为并得到环境的反馈</span></span><br><span class="line">            q_predict = q_table.loc[S, A]    <span class="comment"># 估算的(状态-行为)值</span></span><br><span class="line">            <span class="keyword">if</span> S_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">                q_target = R + GAMMA * q_table.iloc[S_, :].<span class="built_in">max</span>()   <span class="comment">#  实际的(状态-行为)值 (回合没结束)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                q_target = R     <span class="comment">#  实际的(状态-行为)值 (回合结束)</span></span><br><span class="line">                is_terminated = <span class="literal">True</span>    <span class="comment"># terminate this episode</span></span><br><span class="line"></span><br><span class="line">            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  <span class="comment">#  q_table 更新</span></span><br><span class="line">            S = S_  <span class="comment"># 探索者移动到下一个 state</span></span><br><span class="line"></span><br><span class="line">            update_env(S, episode, step_counter+<span class="number">1</span>)  <span class="comment"># 环境更新</span></span><br><span class="line"></span><br><span class="line">            step_counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> q_table</span><br></pre></td></tr></table></figure>

<p>写好所有的评估和更新准则后，我们就能开始训练了，把探索者丢到环境中，让它自己去玩吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    q_table = rl()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\r\nQ-table:\n&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(q_table)</span><br></pre></td></tr></table></figure>



<h3 id="Q-learning-算法更新"><a href="#Q-learning-算法更新" class="headerlink" title="Q-learning 算法更新"></a>Q-learning 算法更新</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>上次我们知道了 RL 之中的 Q-learning 方法是在做什么事，今天我们就来说说一个更具体的例子。让探索者学会走迷宫。黄色的是天堂 (reward 1)，黑色的地狱 (reward -1)。大多数 RL 是由 reward 导向的，所以定义 reward 是 RL 中比较重要的一点。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20q.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>




<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/2-1-1.png"></p>
<p>整个算法就是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。Qlearning 是一个 off-policy 的算法，因为里面的 <code>max</code> action 让 Q table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验，甚至是学习他人的经验)。不过这一次的例子，我们没有运用到 off-policy，而是把 Qlearning 用在了 on-policy 上，也就是现学现卖，将现在经历的直接当场学习并运用。On-policy 和 off-policy 的差别我们会在之后的 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">Deep Q network (off-policy)</a> 学习中见识到。而之后的教程也会讲到一个 on-policy (Sarsa) 的形式，我们之后再对比。</p>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>首先我们先 import 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py">这里下载</a>，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。我也有<a href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的。而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> QLearningTable</span><br></pre></td></tr></table></figure>

<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Qlearning 最重要的迭代更新部分啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>



<h3 id="Q-learning-思维决策"><a href="#Q-learning-思维决策" class="headerlink" title="Q-learning 思维决策"></a>Q-learning 思维决策</h3><p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>QLearningTable</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>与上回不一样的地方是，我们将要以一个 class 形式定义 Q learning，并把这种 tabular q learning 方法叫做 <code>QLearningTable</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="预设值-1"><a href="#预设值-1" class="headerlink" title="预设值"></a>预设值</h4><p>初始的参数意义不会在这里提及了，请参考这个快速了解通道 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/tabular-q2/#">机器学习系列-Q learning</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        self.actions = actions  <span class="comment"># a list</span></span><br><span class="line">        self.lr = learning_rate <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = reward_decay   <span class="comment"># 奖励衰减</span></span><br><span class="line">        self.epsilon = e_greedy     <span class="comment"># 贪婪度</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   <span class="comment"># 初始 q_table</span></span><br></pre></td></tr></table></figure>



<h4 id="决定行为"><a href="#决定行为" class="headerlink" title="决定行为"></a>决定行为</h4><p>这里是定义如何根据所在的 state，或者是在这个 state 上的 观测值 (observation) 来决策。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        self.check_state_exist(observation) <span class="comment"># 检测本 state 是否在 q_table 中存在(见后面标题内容)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选择 action</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:  <span class="comment"># 选择 Q value 最高的 action</span></span><br><span class="line">            state_action = self.q_table.loc[observation, :]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.<span class="built_in">max</span>(state_action)].index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 随机选择 action</span></span><br><span class="line">            action = np.random.choice(self.actions)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>同<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/general-rl">上一个简单的 q learning 例子</a>一样，我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更行 <code>q_table</code>。更新的方式是不是很熟悉呢:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update &#x3D; self.lr * (q_target - q_predict)</span><br></pre></td></tr></table></figure>

<p>这可以理解成神经网络中的更新方式，学习率 * (真实值 - 预测值)。将判断误差传递回去，有着和神经网络更新的异曲同工之处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)  <span class="comment"># 检测 q_table 中是否存在 s_ (见后面标题内容)</span></span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()  <span class="comment"># 下个 state 不是 终止符</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># 下个 state 是终止符</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># 更新对应的 state-action 值</span></span><br></pre></td></tr></table></figure>



<h4 id="检测-state-是否存在"><a href="#检测-state-是否存在" class="headerlink" title="检测 state 是否存在"></a>检测 state 是否存在</h4><p>这个功能就是检测 <code>q_table</code> 中有没有当前 state 的步骤了，如果还没有当前 state，那我我们就插入一组全 0 数据，当做这个 state 的所有 action 初始 values。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            self.q_table = self.q_table.append(</span><br><span class="line">                pd.Series(</span><br><span class="line">                    [<span class="number">0</span>]*<span class="built_in">len</span>(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>

<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/1_command_line_reinforcement_learning/treasure_on_right.py">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/2_Q_Learning_maze">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">什么是 Q Learning 短视频</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（DQN上）</title>
    <url>/YingYingMonstre.github.io/2021/11/30/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DQN%E4%B8%8A%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-DQN"><a href="#什么是-DQN" class="headerlink" title="什么是 DQN"></a>什么是 DQN</h3><p>今天我们会来说说强化学习中的一种强大武器，Deep Q Network 简称为 DQN。Google Deep mind 团队就是靠着这 DQN 使计算机玩电动玩得比我们还厉害。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<h4 id="强化学习与神经网络"><a href="#强化学习与神经网络" class="headerlink" title="强化学习与神经网络"></a>强化学习与神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN1.png"></p>
<p>之前我们所谈论到的强化学习方法都是比较传统的方式，而如今，随着机器学习在日常生活中的各种应用，各种机器学习方法也在融汇、合并、升级。而我们今天所要探讨的强化学习则是这么一种融合了神经网络和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 的方法，名字叫做 Deep Q Network。这种新型结构是为什么被提出来呢？原来，传统的表格形式的强化学习有这样一个瓶颈。</p>
<h4 id="神经网络的作用"><a href="#神经网络的作用" class="headerlink" title="神经网络的作用"></a>神经网络的作用</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN2.png"></p>
<p>我们使用表格来存储每一个状态 state，和在这个 state 每个行为 action 所拥有的 Q 值。而当今问题是在太复杂，状态可以多到比天上的星星还多(比如下围棋)。如果全用表格来存储它们，恐怕我们的计算机有再大的内存都不够，而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事。不过，在机器学习中，有一种方法对这种事情很在行，那就是神经网络。我们可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样我们就没必要在表格中记录 Q 值，而是直接使用神经网络生成 Q 值。还有一种形式的是这样，我们也能只输入状态值，输出所有的动作值，然后按照 Q learning 的原则，直接选择拥有最大值的动作当做下一步要做的动作。我们可以想象，神经网络接受外部的信息，相当于眼睛鼻子耳朵收集信息，然后通过大脑加工输出每种动作的值，最后通过强化学习的方式选择动作。</p>
<h4 id="更新神经网络"><a href="#更新神经网络" class="headerlink" title="更新神经网络"></a>更新神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN3.png"></p>
<p>接下来我们基于第二种神经网络来分析，我们知道，神经网络是要被训练才能预测出准确的值。那在强化学习中，神经网络是如何被训练的呢？首先，我们需要 a1、a2 正确的Q值，这个 Q 值我们就用之前在 Q learning 中的 Q 现实来代替。同样我们还需要一个 Q 估计来实现神经网络的更新。所以神经网络的的参数就是老的 NN 参数加学习率 alpha 乘以 Q 现实和 Q 估计的差距。我们整理一下。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/DQN4.png"></p>
<p>我们通过 NN 预测出Q(s2, a1) 和 Q(s2,a2) 的值，这就是 Q 估计。然后我们选取 Q 估计中最大值的动作来换取环境中的奖励 reward。而 Q 现实中也包含从神经网络分析出来的两个 Q 估计值，不过这个 Q 估计是针对于下一步在 s’ 的估计。最后再通过刚刚所说的算法更新神经网络中的参数。但是这并不是 DQN 会玩电动的根本原因。还有两大因素支撑着 DQN 使得它变得无比强大。这两大因素就是 Experience replay 和 Fixed Q-targets。</p>
<h4 id="DQN-两大利器"><a href="#DQN-两大利器" class="headerlink" title="DQN 两大利器"></a>DQN 两大利器</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN5.png"></p>
<p>简单来说，DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过，Q learning 是一种 off-policy 离线学习法，它能学习当前经历着的，也能学习过去经历过的，甚至是学习别人的经历。所以每次 DQN 更新的时候，我们都可以随机抽取一些之前的经历进行学习。随机抽取这种做法打乱了经历之间的相关性，也使得神经网络更新更有效率。Fixed Q-targets 也是一种打乱相关性的机理，如果使用 fixed Q-targets，我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络，预测 Q 估计的神经网络具备最新的参数，而预测 Q 现实的神经网络使用的参数则是很久以前的。有了这两种提升手段，DQN 才能在一些游戏中超越人类。</p>
<h3 id="DQN-算法更新"><a href="#DQN-算法更新" class="headerlink" title="DQN 算法更新"></a>DQN 算法更新</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>Deep Q Network 的简称叫 DQN，是将 Q learning 的优势 和 Neural networks 结合了。如果我们使用 tabular Q learning，对于每一个 state，action 我们都需要存放在一张 q_table 的表中。如果像显示生活中，情况可就比那个迷宫的状况复杂多了，我们有千千万万个 state，如果将这千万个 state 的值都放在表中，受限于我们计算机硬件，这样从表中获取数据，更新数据是没有效率的。这就是 DQN 产生的原因了。我们可以使用神经网络来 估算这个 state 的值，这样就不需要一张表了。</p>
<p>这次的教程我们还是基于熟悉的迷宫环境，重点在实现 DQN 算法，之后我们再拿着做好的 DQN 算法去跑其他更有意思的环境。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-1-1.jpg"></p>
<p>整个算法乍看起来很复杂，不过我们拆分一下，就变简单了。也就是个 Q learning 主框架上加了些装饰。</p>
<p>这些装饰包括：</p>
<ul>
<li>记忆库 (用于重复学习)</li>
<li>神经网络计算 Q 值</li>
<li>暂时冻结 <code>q_target</code> 参数 (切断相关性)</li>
</ul>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>接下来我们对应上面的算法，来实现主循环。首先 import 所需模块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DeepQNetwork</span><br></pre></td></tr></table></figure>

<p>下面的代码，就是 DQN 于环境交互最重要的部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_maze</span>():</span></span><br><span class="line">    step = <span class="number">0</span>    <span class="comment"># 用来控制什么时候学习</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据行为给出下一个 state, reward, 是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 存储记忆</span></span><br><span class="line">            RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 控制学习起始时间和频率 (先累积一些记忆再开始学习)</span></span><br><span class="line">            <span class="keyword">if</span> (step &gt; <span class="number">200</span>) <span class="keyword">and</span> (step % <span class="number">5</span> == <span class="number">0</span>):</span><br><span class="line">                RL.learn()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state_ 变为 下次循环的 state</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            step += <span class="number">1</span>   <span class="comment"># 总步数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># end of game</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = DeepQNetwork(env.n_actions, env.n_features,</span><br><span class="line">                      learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                      reward_decay=<span class="number">0.9</span>,</span><br><span class="line">                      e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                      replace_target_iter=<span class="number">200</span>,  <span class="comment"># 每 200 步替换一次 target_net 的参数</span></span><br><span class="line">                      memory_size=<span class="number">2000</span>, <span class="comment"># 记忆上限</span></span><br><span class="line">                      <span class="comment"># output_graph=True   # 是否输出 tensorboard 文件</span></span><br><span class="line">                      )</span><br><span class="line">    env.after(<span class="number">100</span>, run_maze)</span><br><span class="line">    env.mainloop()</span><br><span class="line">    RL.plot_cost()  <span class="comment"># 观看神经网络的误差曲线</span></span><br></pre></td></tr></table></figure>

<p>下一节我们会来讲解 <code>DeepQNetwork</code> 这种算法具体要怎么编。</p>
<h3 id="DQN-神经网络"><a href="#DQN-神经网络" class="headerlink" title="DQN 神经网络"></a>DQN 神经网络</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>接着上节内容，这节我们使用 Tensorflow (如果还不了解 Tensorflow，这里去往 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow">经典的 Tensorflow 视频教程</a>) 来搭建 DQN 当中的神经网络部分 (用来预测 Q 值)。</p>
<h4 id="两个神经网络"><a href="#两个神经网络" class="headerlink" title="两个神经网络"></a>两个神经网络</h4><p>为了使用 Tensorflow 来实现 DQN，比较推荐的方式是搭建两个神经网络，<code>target_net</code> 用于预测 <code>q_target</code> 值，它不会及时更新参数。<code>eval_net</code> 用于预测 <code>q_eval</code>，这个神经网络拥有最新的神经网络参数。不过这两个神经网络结构是完全一样的，只是里面的参数不一样。在<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">这个短视频里</a>，能找到我们为什么要建立两个不同参数的神经网络。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-2-1.png"></p>
<h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>因为 DQN 的结构相比之前所讲的内容都不一样，所以我们不使用继承来实现这次的功能。这次我们创建一个 <code>DeepQNetwork</code> 的 class，以及他神经网络部分的功能。下次再说强化学习的更新部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="comment"># 建立神经网络</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="创建两个网络"><a href="#创建两个网络" class="headerlink" title="创建两个网络"></a>创建两个网络</h4><p>两个神经网络是为了固定住一个神经网络 (<code>target_net</code>) 的参数，<code>target_net</code> 是 <code>eval_net</code> 的一个历史版本，拥有 <code>eval_net</code> 很久之前的一组参数，而且这组参数被固定一段时间，然后再被 <code>eval_net</code> 的新参数所替换。而 <code>eval_net</code> 是不断在被提升的，所以是一个可以被训练的网络 <code>trainable=True</code>。而 <code>target_net</code> 的 <code>trainable=False</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># -------------- 创建 eval 神经网络, 及时提升参数 --------------</span></span><br><span class="line">        self.s = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s&#x27;</span>)  <span class="comment"># 用来接收 observation</span></span><br><span class="line">        self.q_target = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_actions], name=<span class="string">&#x27;Q_target&#x27;</span>) <span class="comment"># 用来接收 q_target 的值, 这个之后会通过计算得到</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;eval_net&#x27;</span>):</span><br><span class="line">            <span class="comment"># c_names(collections_names) 是在更新 target_net 参数时会用到</span></span><br><span class="line">            c_names, n_l1, w_initializer, b_initializer = \</span><br><span class="line">                [<span class="string">&#x27;eval_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES], <span class="number">10</span>, \</span><br><span class="line">                tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>), tf.constant_initializer(<span class="number">0.1</span>)  <span class="comment"># config of layers</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># eval_net 的第一层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># eval_net 的第二层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l2&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                self.q_eval = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;loss&#x27;</span>): <span class="comment"># 求误差</span></span><br><span class="line">            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;train&#x27;</span>):    <span class="comment"># 梯度下降</span></span><br><span class="line">            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------------- 创建 target 神经网络, 提供 target Q ---------------------</span></span><br><span class="line">        self.s_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s_&#x27;</span>)    <span class="comment"># 接收下个 observation</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;target_net&#x27;</span>):</span><br><span class="line">            <span class="comment"># c_names(collections_names) 是在更新 target_net 参数时会用到</span></span><br><span class="line">            c_names = [<span class="string">&#x27;target_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># target_net 的第一层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># target_net 的第二层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l2&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                self.q_next = tf.matmul(l1, w2) + b2</span><br></pre></td></tr></table></figure>



<h3 id="DQN-思维决策"><a href="#DQN-思维决策" class="headerlink" title="DQN 思维决策"></a>DQN 思维决策</h3><p>接着上节内容，我们来定义 <code>DeepQNetwork</code> 的决策和思考部分。</p>
<h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>定义完上次的神经网络部分以后，这次我们来定义其他部分。包括：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="comment"># 上次的内容</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这次的内容:</span></span><br><span class="line">    <span class="comment"># 初始值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储记忆</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 看看学习效果 (可选)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="初始值"><a href="#初始值" class="headerlink" title="初始值"></a>初始值</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            n_actions,</span></span></span><br><span class="line"><span class="function"><span class="params">            n_features,</span></span></span><br><span class="line"><span class="function"><span class="params">            learning_rate=<span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            reward_decay=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            e_greedy=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            replace_target_iter=<span class="number">300</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            memory_size=<span class="number">500</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            batch_size=<span class="number">32</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            e_greedy_increment=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            output_graph=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon_max = e_greedy     <span class="comment"># epsilon 的最大值</span></span><br><span class="line">        self.replace_target_iter = replace_target_iter  <span class="comment"># 更换 target_net 的步数</span></span><br><span class="line">        self.memory_size = memory_size  <span class="comment"># 记忆上限</span></span><br><span class="line">        self.batch_size = batch_size    <span class="comment"># 每次更新时从 memory 里面取多少记忆出来</span></span><br><span class="line">        self.epsilon_increment = e_greedy_increment <span class="comment"># epsilon 的增量</span></span><br><span class="line">        self.epsilon = <span class="number">0</span> <span class="keyword">if</span> e_greedy_increment <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.epsilon_max <span class="comment"># 是否开启探索模式, 并逐步减少探索次数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录学习次数 (用于判断是否更换 target_net 参数)</span></span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化全 0 记忆 [s, a, r, s_]</span></span><br><span class="line">        self.memory = np.zeros((self.memory_size, n_features*<span class="number">2</span>+<span class="number">2</span>)) <span class="comment"># 和视频中不同, 因为 pandas 运算比较慢, 这里改为直接用 numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建 [target_net, evaluate_net]</span></span><br><span class="line">        self._build_net()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 替换 target net 的参数</span></span><br><span class="line">        t_params = tf.get_collection(<span class="string">&#x27;target_net_params&#x27;</span>)  <span class="comment"># 提取 target_net 的参数</span></span><br><span class="line">        e_params = tf.get_collection(<span class="string">&#x27;eval_net_params&#x27;</span>)   <span class="comment"># 提取  eval_net 的参数</span></span><br><span class="line">        self.replace_target_op = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(t_params, e_params)] <span class="comment"># 更新 target_net 参数</span></span><br><span class="line"></span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出 tensorboard 文件</span></span><br><span class="line">        <span class="keyword">if</span> output_graph:</span><br><span class="line">            <span class="comment"># $ tensorboard --logdir=logs</span></span><br><span class="line">            tf.summary.FileWriter(<span class="string">&quot;logs/&quot;</span>, self.sess.graph)</span><br><span class="line"></span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        self.cost_his = []  <span class="comment"># 记录所有 cost 变化, 用于最后 plot 出来观看</span></span><br></pre></td></tr></table></figure>



<h4 id="存储记忆"><a href="#存储记忆" class="headerlink" title="存储记忆"></a>存储记忆</h4><p>DQN 的精髓部分之一：记录下所有经历过的步，这些步可以进行反复的学习，所以是一种 off-policy 方法，你甚至可以自己玩，然后记录下自己玩的经历，让这个 DQN 学习你是如何通关的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;memory_counter&#x27;</span>):</span><br><span class="line">            self.memory_counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录一条 [s, a, r, s_] 记录</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换</span></span><br><span class="line">        index = self.memory_counter % self.memory_size</span><br><span class="line">        self.memory[index, :] = transition <span class="comment"># 替换过程</span></span><br><span class="line"></span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h4 id="选行为"><a href="#选行为" class="headerlink" title="选行为"></a>选行为</h4><p>和之前的 <code>QLearningTable</code>，<code>SarsaTable</code> 等一样，都需要一个选行为的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="comment"># 统一 observation 的 shape (1, size_of_observation)</span></span><br><span class="line">        observation = observation[np.newaxis, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="comment"># 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action</span></span><br><span class="line">            actions_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: observation&#125;)</span><br><span class="line">            action = np.argmax(actions_value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)   <span class="comment"># 随机选择</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>最重要的一步来了，就是在 <code>DeepQNetwork</code> 中，是如何学习，更新参数的。这里涉及了 <code>target_net</code> 和 <code>eval_net</code> 的交互使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_replace_target_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 检查是否替换 target_net 参数</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.sess.run(self.replace_target_op)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\ntarget_params_replaced\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 memory 中随机抽取 batch_size 这么多记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        batch_memory = self.memory[sample_index, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取 q_next (target_net 产生了 q) 和 q_eval(eval_net 产生的 q)</span></span><br><span class="line">        q_next, q_eval = self.sess.run(</span><br><span class="line">            [self.q_next, self.q_eval],</span><br><span class="line">            feed_dict=&#123;</span><br><span class="line">                self.s_: batch_memory[:, -self.n_features:],</span><br><span class="line">                self.s: batch_memory[:, :self.n_features]</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面这几步十分重要. q_next, q_eval 包含所有 action 的值,</span></span><br><span class="line">        <span class="comment"># 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.</span></span><br><span class="line">        <span class="comment"># 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.</span></span><br><span class="line">        <span class="comment"># 这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]</span></span><br><span class="line">        <span class="comment"># q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.</span></span><br><span class="line">        <span class="comment"># q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,</span></span><br><span class="line">        <span class="comment"># 我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.</span></span><br><span class="line">        <span class="comment"># 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,</span></span><br><span class="line">        <span class="comment"># 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.</span></span><br><span class="line">        <span class="comment"># 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.</span></span><br><span class="line">        <span class="comment"># 具体在下面还有一个举例说明.</span></span><br><span class="line"></span><br><span class="line">        q_target = q_eval.copy()</span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>)</span><br><span class="line">        reward = batch_memory[:, self.n_features + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        q_target[batch_index, eval_act_index] = reward + self.gamma * np.<span class="built_in">max</span>(q_next, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:</span></span><br><span class="line"><span class="string">        q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        q_target = q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:</span></span><br><span class="line"><span class="string">        比如在:</span></span><br><span class="line"><span class="string">            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;</span></span><br><span class="line"><span class="string">            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:</span></span><br><span class="line"><span class="string">        q_target =</span></span><br><span class="line"><span class="string">        [[-1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, -2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        所以 (q_target - q_eval) 就变成了:</span></span><br><span class="line"><span class="string">        [[(-1)-(1), 0, 0],</span></span><br><span class="line"><span class="string">         [0, 0, (-2)-(6)]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.</span></span><br><span class="line"><span class="string">        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.</span></span><br><span class="line"><span class="string">        我们只反向传递之前选择的 action 的值,</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练 eval_net</span></span><br><span class="line">        _, self.cost = self.sess.run([self._train_op, self.loss],</span><br><span class="line">                                     feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                self.q_target: q_target&#125;)</span><br><span class="line">        self.cost_his.append(self.cost) <span class="comment"># 记录 cost 误差</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐渐增加 epsilon, 降低行为的随机性</span></span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h4 id="看学习效果"><a href="#看学习效果" class="headerlink" title="看学习效果"></a>看学习效果</h4><p>为了看看学习效果，我们在最后输出学习过程中的 <code>cost</code> 变化曲线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_replace_target_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-3-1.png"></p>
<p>可以看出曲线并不是平滑下降的，这是因为 DQN 中的 input 数据是一步步改变的，而且会根据学习情况，获取到不同的数据。所以这并不像一般的监督学习，DQN 的 cost 曲线就有所不同了。</p>
<h4 id="修改版的-DQN"><a href="#修改版的-DQN" class="headerlink" title="修改版的 DQN"></a>修改版的 DQN</h4><p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-3-2.png"></p>
<p>最后提供一种修改版的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py">DQN 代码</a>，这是录制完视频以后做的，这是将 <code>q_target</code> 的计算也加在了 Tensorflow 的 graph 里面。这种结构还是有好处的，作为学习样本的话，计算结构全部在 tensorboard 上，就更好理解，代码结构也更好理解。</p>
<p>我只在原本的 DQN 代码上改了一点点东西，大家应该可以很容易辨别。修改版的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py">代码在这</a>。</p>
<p>如果想一次性看到全部代码, 请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">DQN Tensorflow Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/DQN">DQN PyTorch Python 教程</a></li>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">什么是 DQN 短视频</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>论文 <a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（Sarsa）</title>
    <url>/YingYingMonstre.github.io/2021/11/10/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88Sarsa%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-Sarsa"><a href="#什么是-Sarsa" class="headerlink" title="什么是 Sarsa"></a>什么是 Sarsa</h3><p>今天我们会来说说强化学习中一个和 Q learning 类似的算法，叫做 Sarsa。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<p><img src="https://static.mofanpy.com/results/ML-intro/s1.png"></p>
<p>在强化学习中 Sarsa 和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 及其类似，这节内容会基于之前我们所讲的 Q learning。所以还不熟悉 Q learning 的朋友们，请前往我制作的 Q learning 简介 (知乎专栏)。我们会对比 Q learning，来看看 Sarsa 是特殊在哪些方面。和上次一样，我们还是使用写作业和看电视这个例子。没写完作业去看电视被打，写完了作业有糖吃。</p>
<h4 id="Sarsa-决策"><a href="#Sarsa-决策" class="headerlink" title="Sarsa 决策"></a>Sarsa 决策</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s2.png"></p>
<p>Sarsa 的决策部分和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩。但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<h4 id="Sarsa-更新行为准则"><a href="#Sarsa-更新行为准则" class="headerlink" title="Sarsa 更新行为准则"></a>Sarsa 更新行为准则</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s3.png"></p>
<p>同样，我们会经历正在写作业的状态 s1，然后再挑选一个带来最大潜在奖励的动作 a2，这样我们就到达了 继续写作业状态 s2，而在这一步，如果你用的是 Q learning，你会观看一下在 s2 上选取哪一个动作会带来最大的奖励，但是在真正要做决定时，却不一定会选取到那个带来最大奖励的动作，Q-learning 在这一步只是估计了一下接下来的动作值。而 Sarsa 是实践派，他说到做到，在 s2 这一步估算的动作也是接下来要做的动作。所以 Q(s1, a2) 现实的计算值，我们也会稍稍改动，去掉maxQ，取而代之的是在 s2 上我们实实在在选取的 a2 的 Q 值。最后像 Q learning 一样，求出现实和估计的差距 并更新 Q 表里的 Q(s1, a2)。</p>
<h4 id="对比-Sarsa-和-Qlearning-算法"><a href="#对比-Sarsa-和-Qlearning-算法" class="headerlink" title="对比 Sarsa 和 Qlearning 算法"></a>对比 Sarsa 和 Qlearning 算法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s4.png"></p>
<p>从算法来看，这就是他们两最大的不同之处了。因为 Sarsa 是说到做到型，所以我们也叫他 on-policy、在线学习，学着自己在做的事情。而 Q learning 是说到但并不一定做到，所以它也叫作 Off-policy、离线学习。而因为有了 maxQ，Q-learning 也是一个特别勇敢的算法。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/s5.png"></p>
<p>为什么说他勇敢呢，因为 Q learning 机器人永远都会选择最近的一条通往成功的道路，不管这条路会有多危险。而 Sarsa 则是相当保守，他会选择离危险远远的，拿到宝藏是次要的，保住自己的小命才是王道。这就是使用 Sarsa 方法的不同之处。</p>
<h3 id="Sarsa-算法更新"><a href="#Sarsa-算法更新" class="headerlink" title="Sarsa 算法更新"></a>Sarsa 算法更新</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>这次我们用同样的迷宫例子来实现 RL 中另一种和 Qlearning 类似的算法，叫做 Sarsa (state-action-reward-state*-action*)。我们从这一个简称可以了解到，Sarsa 的整个循环都将是在一个路径上，也就是 on-policy，下一个 state，和下一个 <em>action</em> 将会变成他真正采取的 action 和 state。和 Qlearning 的不同之处就在这。Qlearning 的下个一个 state_ action_ 在算法更新的时候都还是不确定的 (off-policy)。而 Sarsa 的 state，<em>action</em> 在这次算法更新的时候已经确定好了 (on-policy)。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-1-1.png"></p>
<p>整个算法还是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。不过于 Qlearning 不同之处：</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的 <code>action</code>，而且想好了下一个 <code>state_</code> 和下一个 <code>action_</code> (Qlearning 还没有想好下一个 <code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code> (Qlearning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<p>这种不同之处使得 Sarsa 相对于 Qlearning 更加的胆小。因为 Qlearning 永远都是想着 <code>maxQ</code> 最大化，因为这个 <code>maxQ</code> 而变得贪婪，不考虑其他非 <code>maxQ</code> 的结果。我们可以理解成 Qlearning 是一种贪婪、大胆、勇敢的算法，对于错误、死亡并不在乎。而 Sarsa 是一种保守的算法，他在乎每一步决策，对于错误和死亡比较铭感。这一点我们会在可视化的部分看出他们的不同。两种算法都有他们的好处，比如在实际中，你比较在乎机器的损害，用一种保守的算法，在训练时就能减少损坏的次数。</p>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>首先我们先 import 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/3_Sarsa_maze/maze_env.py">这里下载</a>。<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。我也有<a href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的。而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> SarsaTable</span><br></pre></td></tr></table></figure>

<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Sarsa 最重要的迭代更新部分啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sarsa 根据 state 观测选择行为</span></span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 根据下一个 state (obervation_) 选取下一个 action_</span></span><br><span class="line">            action_ = RL.choose_action(<span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个当成下一步的 state (observation) and action</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 终止时跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 大循环完毕</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = SarsaTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>

<p>下一节我们会来讲解 <code>SarsaTable</code> 这种算法具体要怎么编。</p>
<h3 id="Sarsa思维决策"><a href="#Sarsa思维决策" class="headerlink" title="Sarsa思维决策"></a>Sarsa思维决策</h3><h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>和之前定义 Qlearning 中的 <code>QLearningTable</code> 一样，因为使用 tabular 方式的 <code>Sarsa</code> 和 <code>Qlearning</code> 的相似度极高，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>

<p>我们甚至可以定义一个主class <code>RL</code>，然后将 <code>QLearningTable</code> 和 <code>SarsaTable</code> 作为 主class <code>RL</code> 的衍生，这个主 <code>RL</code> 可以这样定义。所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、<code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RL</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, action_space, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># 每种的都有点不同, 所以用 pass</span></span><br></pre></td></tr></table></figure>

<p>如果是这样定义父类的 <code>RL</code> class，通过继承关系，那之子类 <code>QLearningTable</code> class 就能简化成这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>(<span class="params">RL</span>):</span>   <span class="comment"># 继承了父类 RL</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    <span class="comment"># 表示继承关系</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span>   <span class="comment"># learn 的方法在每种类型中有不一样, 需重新定义</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r</span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>(<span class="params">RL</span>):</span>   <span class="comment"># 继承 RL class</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    <span class="comment"># 表示继承关系</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="comment"># q_target 基于选好的 a_ 而不是 Q(s_) 的最大值</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># 如果 s_ 是终止符</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># 更新 q_table</span></span><br></pre></td></tr></table></figure>



<h3 id="什么是-Sarsa-lambda"><a href="#什么是-Sarsa-lambda" class="headerlink" title="什么是 Sarsa(lambda)"></a>什么是 Sarsa(lambda)</h3><p>今天我们会来说说强化学习中基于 Sarsa 的一种提速方法，叫做 sarsa-lambda。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<h4 id="Sarsa-n"><a href="#Sarsa-n" class="headerlink" title="Sarsa(n)"></a>Sarsa(n)</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl1.png"></p>
<p>通过上个视频的介绍，我们知道这个 [Sarsa]/tutorials/machine-learning/reinforcement-learning/intro-sarsa)) 的算法是一种在线学习法，on-policy。但是这个 lambda 到底是什么。其实吧，Sarsa 是一种单步更新法，在环境中每走一步，更新一次自己的行为准则，我们可以在这样的 Sarsa 后面打一个括号，说他是 Sarsa(0)，因为他等走完这一步以后直接更新行为准则。如果延续这种想法，走完这步，再走一步，然后再更新，我们可以叫他 Sarsa(1)。同理，如果等待回合完毕我们一次性再更新呢，比如这回合我们走了 n 步，那我们就叫 Sarsa(n)。为了统一这样的流程，我们就有了一个 lambda 值来代替我们想要选择的步数，这也就是 Sarsa(lambda) 的由来。我们看看最极端的两个例子，对比单步更新和回合更新，看看回合更新的优势在哪里。</p>
<h4 id="单步更新-and-回合更新"><a href="#单步更新-and-回合更新" class="headerlink" title="单步更新 and 回合更新"></a>单步更新 and 回合更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl2.png"></p>
<p>虽然我们每一步都在更新，但是在没有获取宝藏的时候，我们现在站着的这一步也没有得到任何更新，也就是直到获取宝藏时，我们才为获取到宝藏的上一步更新为：这一步很好，和获取宝藏是有关联的，而之前为了获取宝藏所走的所有步都被认为和获取宝藏没关系。回合更新虽然我要等到这回合结束，才开始对本回合所经历的所有步都添加更新，但是这所有的步都是和宝藏有关系的，都是为了得到宝藏需要学习的步，所以每一个脚印在下回合被选则的几率又高了一些。在这种角度来看，回合更新似乎会有效率一些。</p>
<h4 id="有时迷茫"><a href="#有时迷茫" class="headerlink" title="有时迷茫"></a>有时迷茫</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl3.png"></p>
<p>我们看看这种情况，还是使用单步更新的方法在每一步都进行更新，但是同时记下之前的寻宝之路。你可以想像，每走一步，插上一个小旗子，这样我们就能清楚的知道除了最近的一步，找到宝物时还需要更新哪些步了。不过，有时候情况可能没有这么乐观。开始的几次，因为完全没有头绪，我可能在原地打转了很久，然后才找到宝藏，那些重复的脚步真的对我拿到宝藏很有必要吗？答案我们都知道。所以Sarsa(lambda)就来拯救你啦。</p>
<h4 id="Lambda-含义"><a href="#Lambda-含义" class="headerlink" title="Lambda 含义"></a>Lambda 含义</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl4.png"></p>
<p>其实 lambda 就是一个衰变值，他可以让你知道离奖励越远的步可能并不是让你最快拿到奖励的步，所以我们想象我们站在宝藏的位置，回头看看我们走过的寻宝之路，离宝藏越近的脚印越看得清，远处的脚印太渺小，我们都很难看清，那我们就索性记下离宝藏越近的脚印越重要，越需要被好好的更新。和之前我们提到过的奖励衰减值 gamma 一样，lambda 是脚步衰减值，都是一个在 0 和 1 之间的数。</p>
<h4 id="Lambda-取值"><a href="#Lambda-取值" class="headerlink" title="Lambda 取值"></a>Lambda 取值</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl5.png"></p>
<p>当 lambda 取0，就变成了 Sarsa 的单步更新，当 lambda 取 1，就变成了回合更新，对所有步更新的力度都是一样。当 lambda 在 0 和 1 之间，取值越大，离宝藏越近的步更新力度越大。这样我们就不用受限于单步更新的每次只能更新最近的一步，我们可以更有效率的更新所有相关步了。</p>
<h3 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa-lambda"></a>Sarsa-lambda</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>Sarsa-lambda 是基于 Sarsa 方法的升级版，他能更有效率地学习到怎么样获得好的 reward。如果说 Sarsa 和 Qlearning 都是每次获取到 reward，只更新获取到 reward 的前一步。那 Sarsa-lambda 就是更新获取到 reward 的前 lambda 步。lambda 是在 [0, 1] 之间取值，</p>
<p>如果 lambda = 0，Sarsa-lambda 就是 Sarsa，只更新获取到 reward 前经历的最后一步。</p>
<p>如果 lambda = 1，Sarsa-lambda 更新的是获取到 reward 前所有经历的步。</p>
<p>这样解释起来有点抽象，还是建议大家观看我制作的 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda">什么是 Sarsa-lambda 短视频</a>, 用动画展示具体的区别。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa_lambda.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="代码主结构-1"><a href="#代码主结构-1" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>使用 <code>SarsaLambdaTable</code> 在算法更新迭代的部分，是和之前的 <code>SarsaTable</code> 一样的，所以这一节，我们没有算法更新部分, 直接变成思维决策部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>

<p>同样, 我们选择继承的方式，将 <code>SarsaLambdaTable</code> 继承到 <code>RL</code>，所以我们将之前的 <code>__init__</code>，<code>check_state_exist</code>，<code>choose_action</code>，<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<p>算法的相应更改请参考这个：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-3-1.png"></p>
<h4 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h4><p>在预设值当中，我们添加了 <code>trace_decay=0.9</code> 这个就是 <code>lambda</code> 的值了。这个值将会使得拿到 reward 前的每一步都有价值。如果还不太明白其他预设值的意思，请查看我的 <a href="https://mofanpy.com/tutorials/machine-learning/ML-intro">关于强化学习的短视频列表</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 后向观测算法, eligibility trace.</span></span><br><span class="line">        self.lambda_ = trace_decay</span><br><span class="line">        self.eligibility_trace = self.q_table.copy()    <span class="comment"># 空的 eligibility trace 表</span></span><br></pre></td></tr></table></figure>



<h4 id="检测-state-是否存在"><a href="#检测-state-是否存在" class="headerlink" title="检测 state 是否存在"></a>检测 state 是否存在</h4><p><code>check_state_exist</code> 和之前的是高度相似的。唯一不同的地方是我们考虑了 <code>eligibility_trace</code>，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            to_be_append = pd.Series(</span><br><span class="line">                    [<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            self.q_table = self.q_table.append(to_be_append)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># also update eligibility trace</span></span><br><span class="line">            self.eligibility_trace = self.eligibility_trace.append(to_be_append)</span><br></pre></td></tr></table></figure>



<h4 id="学习-1"><a href="#学习-1" class="headerlink" title="学习"></a>学习</h4><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaLambdaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaLambdaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        <span class="comment"># 这部分和 Sarsa 一样</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.ix[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.ix[s_, a_]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r</span><br><span class="line">        error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里开始不同:</span></span><br><span class="line">        <span class="comment"># 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环</span></span><br><span class="line">        self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q table 更新</span></span><br><span class="line">        self.q_table += self.lr * error * self.eligibility_trace</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随着时间衰减 eligibility trace 的值, 离获取 reward 越远的步, 他的&quot;不可或缺性&quot;越小</span></span><br><span class="line">        self.eligibility_trace *= self.gamma*self.lambda_</span><br></pre></td></tr></table></figure>

<p>除了图中和上面代码这种更新方式，还有一种会更加有效率。我们可以将上面的这一步替换成下面这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上面代码中的方式:</span></span><br><span class="line">self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更有效的方式:</span></span><br><span class="line">self.eligibility_trace.ix[s, :] *= <span class="number">0</span></span><br><span class="line">self.eligibility_trace.ix[s, a] = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>他们两的不同之处可以用这张图来概括:</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-3-2.png"></p>
<p>这是针对于一个 state-action 值按经历次数的变化。最上面是经历 state-action 的时间点，第二张图是使用这种方式所带来的 不可或缺性值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.eligibility_trace.ix[s, a] +&#x3D; 1</span><br></pre></td></tr></table></figure>

<p>下面图是使用这种方法带来的不可或缺性值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.eligibility_trace.ix[s, :] *&#x3D; 0; self.eligibility_trace.ix[s, a] &#x3D; 1</span><br></pre></td></tr></table></figure>

<p>实验证明选择下面这种方法会有更好的效果。大家也可以自己玩一玩，试试两种方法的不同表现。</p>
<p>最后不要忘了，eligibility trace 只是记录每个回合的每一步，新回合开始的时候需要将 Trace 清零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 新回合, 清零</span></span><br><span class="line">    RL.eligibility_trace *= <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>: <span class="comment"># 开始回合</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/3_Sarsa_maze">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p>学习资料:</p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/3_Sarsa_maze">全部代码</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q-learning 简介视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">什么是 Sarsa 短视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda">什么是 Sarsa-lambda 短视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/tabular-sarsa1">Sarsa Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（DQN下）</title>
    <url>/YingYingMonstre.github.io/2021/11/30/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DQN%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<h3 id="OpenAI-gym-环境库"><a href="#OpenAI-gym-环境库" class="headerlink" title="OpenAI gym 环境库"></a>OpenAI gym 环境库</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>手动编环境是一件很耗时间的事情，所以如果有能力使用别人已经编好的环境，可以节约我们很多时间。OpenAI gym 就是这样一个模块，他提供了我们很多优秀的模拟环境。我们的各种 RL 算法都能使用这些环境。不过 OpenAI gym 暂时只支持 MacOS 和 Linux 系统。Windows 已经支持，但是听说还没有全面支持，大家时不时查看下官网，可能就有惊喜。实在等不及更新了，也行用 tkinter 来手动编写一下环境。这里有我制作的很好的 <a href="https://mofanpy.com/tutorials/python-basic/tkinter">tkinter 入门教程</a>，之前的 maze 环境也是用 tkinter 编出来的。大家可以仿照<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/maze_env.py">这个文件</a>编写就 ok 啦。或者还可以玩玩更厉害的，像 OpenAI 一样，使用 pyglet 模块来编写，我做了一个从环境开始编写的<a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a>。</p>
<h4 id="安装-gym"><a href="#安装-gym" class="headerlink" title="安装 gym"></a>安装 gym</h4><p>在 MacOS 和 Linux 系统下，安装 gym 很方便，首先确定你是 python 2.7 或者 python 3.5 版本。然后在你的 terminal 中复制下面这些。但是 gym 暂时还不完全支持 Windows，不过有些虚拟环境已经的到了支持，想立杆子那个已经支持了。所以接下来要说的安装方法只有 MacOS 和 Linux 的。Windows 用户的安装方式应该也差不多，如果 Windows 用户遇到了问题，欢迎在留言区分享解决的方法。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> python 2.7, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install gym</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python 3.5, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip3 install gym</span></span><br></pre></td></tr></table></figure>

<p>如果没有报错，恭喜你，这样你就装好了 gym 的最基本款，可以开始玩以下游戏啦：</p>
<ul>
<li><a href="https://gym.openai.com/envs#algorithmic">algorithmic</a></li>
<li><a href="https://gym.openai.com/envs#toy_text">toy_text</a></li>
<li><a href="https://gym.openai.com/envs#classic_control">classic_control</a> (这个需要 pyglet 模块)</li>
</ul>
<p>如果在安装中遇到问题。可能是缺少了一些必要模块，可以使用下面语句来安装这些模块(安装时间可能有点久)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> MacOS:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew install cmake boost boost-python sdl2 swig wget</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Ubuntu 14.04:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig</span></span><br></pre></td></tr></table></figure>

<p>如果想要玩 gym 提供的全套游戏，下面这几句就能满足你：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> python 2.7, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install gym[all]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python 3.5, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip3 install gym[all]</span></span><br></pre></td></tr></table></figure>



<h4 id="CartPole-例子"><a href="#CartPole-例子" class="headerlink" title="CartPole 例子"></a>CartPole 例子</h4><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/cartpole%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>

<p>之前我编写的 <code>maze_env</code> 基本上是按照 <code>gym</code> 环境格式写的，所以你换成 <code>gym</code> 格式会很简单。</p>
<p>接下来我们对应上面的算法，来实现主循环。首先 import 所需模块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DeepQNetwork</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)   <span class="comment"># 定义使用 gym 库中的那一个环境</span></span><br><span class="line">env = env.unwrapped <span class="comment"># 不做这个会有很多限制</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space) <span class="comment"># 查看这个环境中可用的 action 有多少个</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space)    <span class="comment"># 查看这个环境中可用的 state 的 observation 有多少个</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space.high)   <span class="comment"># 查看 observation 最高取值</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space.low)    <span class="comment"># 查看 observation 最低取值</span></span><br></pre></td></tr></table></figure>

<p>与之前使用 tkinter 定义的环境有点不一样，我们可以不适用 <code>if __name__ == &quot;__main__&quot;</code> 的方式，下面是一种类似，却更简单的写法。之中我们会用到里面的 reward，但是 <code>env.step()</code> 说提供的 <code>reward</code> 不一定是最有效率的 <code>reward</code>，我们大可对这些进行修改，使 DQN 学得更有效率。你可以自己对比一下不修改 reward 和按我这样修改，他们学习过程的不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义使用 DQN 的算法</span></span><br><span class="line">RL = DeepQNetwork(n_actions=env.action_space.n,</span><br><span class="line">                  n_features=env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">                  learning_rate=<span class="number">0.01</span>, e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                  replace_target_iter=<span class="number">100</span>, memory_size=<span class="number">2000</span>,</span><br><span class="line">                  e_greedy_increment=<span class="number">0.0008</span>,)</span><br><span class="line"></span><br><span class="line">total_steps = <span class="number">0</span> <span class="comment"># 记录步数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取回合 i_episode 第一个 observation</span></span><br><span class="line">    observation = env.reset()</span><br><span class="line">    ep_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        env.render()    <span class="comment"># 刷新环境</span></span><br><span class="line"></span><br><span class="line">        action = RL.choose_action(observation)  <span class="comment"># 选行为</span></span><br><span class="line"></span><br><span class="line">        observation_, reward, done, info = env.step(action) <span class="comment"># 获取下一个 state</span></span><br><span class="line"></span><br><span class="line">        x, x_dot, theta, theta_dot = observation_   <span class="comment"># 细分开, 为了修改原配的 reward</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少</span></span><br><span class="line">        <span class="comment"># theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高</span></span><br><span class="line"></span><br><span class="line">        x, x_dot, theta, theta_dot = observation_</span><br><span class="line">        r1 = (env.x_threshold - <span class="built_in">abs</span>(x))/env.x_threshold - <span class="number">0.8</span></span><br><span class="line">        r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta))/env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">        reward = r1 + r2   <span class="comment"># 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存这一组记忆</span></span><br><span class="line">        RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; <span class="number">1000</span>:</span><br><span class="line">            RL.learn()  <span class="comment"># 学习</span></span><br><span class="line"></span><br><span class="line">        ep_r += reward</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;episode: &#x27;</span>, i_episode,</span><br><span class="line">                  <span class="string">&#x27;ep_r: &#x27;</span>, <span class="built_in">round</span>(ep_r, <span class="number">2</span>),</span><br><span class="line">                  <span class="string">&#x27; epsilon: &#x27;</span>, <span class="built_in">round</span>(RL.epsilon, <span class="number">2</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        observation = observation_</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line"><span class="comment"># 最后输出 cost 曲线</span></span><br><span class="line">RL.plot_cost()</span><br></pre></td></tr></table></figure>

<p>这是更为典型的 RL cost 曲线：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-4-1.png"></p>
<h4 id="MountainCar-例子"><a href="#MountainCar-例子" class="headerlink" title="MountainCar 例子"></a>MountainCar 例子</h4><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>

<p>代码基本和上述代码相同，就只是在 reward 上动了下手脚。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DeepQNetwork</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;MountainCar-v0&#x27;</span>)</span><br><span class="line">env = env.unwrapped</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space.high)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space.low)</span><br><span class="line"></span><br><span class="line">RL = DeepQNetwork(n_actions=<span class="number">3</span>, n_features=<span class="number">2</span>, learning_rate=<span class="number">0.001</span>, e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                  replace_target_iter=<span class="number">300</span>, memory_size=<span class="number">3000</span>,</span><br><span class="line">                  e_greedy_increment=<span class="number">0.0001</span>,)</span><br><span class="line"></span><br><span class="line">total_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    observation = env.reset()</span><br><span class="line">    ep_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        env.render()</span><br><span class="line"></span><br><span class="line">        action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">        observation_, reward, done, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        position, velocity = observation_</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 车开得越高 reward 越大</span></span><br><span class="line">        reward = <span class="built_in">abs</span>(position - (-<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">        RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; <span class="number">1000</span>:</span><br><span class="line">            RL.learn()</span><br><span class="line"></span><br><span class="line">        ep_r += reward</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            get = <span class="string">&#x27;| Get&#x27;</span> <span class="keyword">if</span> observation_[<span class="number">0</span>] &gt;= env.unwrapped.goal_position <span class="keyword">else</span> <span class="string">&#x27;| ----&#x27;</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epi: &#x27;</span>, i_episode,</span><br><span class="line">                  get,</span><br><span class="line">                  <span class="string">&#x27;| Ep_r: &#x27;</span>, <span class="built_in">round</span>(ep_r, <span class="number">4</span>),</span><br><span class="line">                  <span class="string">&#x27;| Epsilon: &#x27;</span>, <span class="built_in">round</span>(RL.epsilon, <span class="number">2</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        observation = observation_</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">RL.plot_cost()</span><br></pre></td></tr></table></figure>

<p>出来的 cost 曲线是这样：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-4-2.png"></p>
<p>这两个都只是例子而已，具体的实施你也可以大动手脚，比如你的 reward 定义得更好，你的神经网络结构更好，使得他们学的更快。这些都是自己定义的。</p>
<h3 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p><strong>本篇教程是基于 Deep Q network (DQN) 的选学教程。以下教程缩减了在 DQN 方面的介绍，着重强调 Double DQN 和 DQN 在代码上不同的地方。所以还没了解 DQN 的同学们，有关于 DQN 的知识，请从 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">这个视频</a> 和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">这个Python教程</a> 开始学习。</strong></p>
<p>接下来我们说说为什么会有 Double DQN 这种算法。所以我们从 Double DQN 相对于 Natural DQN (传统 DQN) 的优势说起。</p>
<p>一句话概括，DQN 基于 Q-learning, Q-Learning 中有 <code>Qmax</code>，<code>Qmax</code> 会导致 <code>Q现实</code> 当中的过估计 (overestimate)。而 Double DQN 就是用来解决过估计的。在实际问题中，如果你输出你的 DQN 的 Q 值，可能就会发现，Q 值都超级大。这就是出现了 overestimate。</p>
<p>这次的 Double DQN 的算法基于的是 OpenAI Gym 中的 <code>Pendulum</code> 环境。如果还不了解如何使用 OpenAI 的话，这里有<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/gym">我的一个教程</a>。以下就是这次的结果，先睹为快。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/Pendulum%20DQN.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="Double-DQN-算法"><a href="#Double-DQN-算法" class="headerlink" title="Double DQN 算法"></a>Double DQN 算法</h4><p>我们知道 DQN 的神经网络部分可以看成一个 <code>最新的神经网络</code> + <code>老神经网络</code>，他们有相同的结构，但内部的参数更新却有时差。而它的 <code>Q现实</code> 部分是这样的：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-5-1.png"></p>
<p>因为我们的神经网络预测 <code>Qmax</code> 本来就有误差，每次也向着最大误差的 <code>Q现实</code> 改进神经网络，就是因为这个 <code>Qmax</code> 导致了 overestimate。所以 Double DQN 的想法就是引入另一个神经网络来打消一些最大误差的影响。而 DQN 中本来就有两个神经网络，我们何不利用一下这个地理优势呢。所以，我们用 <code>Q估计</code> 的神经网络估计 <code>Q现实</code> 中 <code>Qmax(s&#39;, a&#39;)</code> 的最大动作值。然后用这个被 <code>Q估计</code> 估计出来的动作来选择 <code>Q现实</code> 中的 <code>Q(s&#39;)</code>。总结一下：</p>
<p>有两个神经网络：<code>Q_eval</code> (Q估计中的)，<code>Q_next</code> (Q现实中的)。</p>
<p>原本的 <code>Q_next = max(Q_next(s&#39;, a_all))</code>。</p>
<p>Double DQN 中的 <code>Q_next = Q_next(s&#39;, argmax(Q_eval(s&#39;, a_all)))</code>。也可以表达成下面那样。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-5-2.png"></p>
<h4 id="更新方法"><a href="#更新方法" class="headerlink" title="更新方法"></a>更新方法</h4><p>好了，有了理论，我们就来用 Python 实现它吧。</p>
<p>这里的代码都是基于之前 DQN 教程中的代码 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py">(github)</a>，在 <code>RL_brain</code> 中，我们将 class 的名字改成 <code>DoubleDQN</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>double_q</code> 参数来表示使用的是 Natural DQN 还是 Double DQN。为了对比的需要，我们的 <code>tf.Session()</code> 也单独传入。并移除原本在 DQN 代码中的这一句：</p>
<p><code>self.sess.run(tf.global_variables_initializer())</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">..., double_q=<span class="literal">True</span>, sess=<span class="literal">None</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.double_q = double_q</span><br><span class="line">        <span class="keyword">if</span> sess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sess = sess</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<p>接着我们来修改 <code>learn()</code> 中的代码。我们对比 Double DQN 和 Natural DQN 在 tensorboard 中的图，发现他们的结构并没有不同，但是在计算 <code>q_target</code> 的时候，方法是不同的。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-5-3.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 这一段和 DQN 一样:</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.sess.run(self.replace_target_op)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\ntarget_params_replaced\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        batch_memory = self.memory[sample_index, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这一段和 DQN 不一样</span></span><br><span class="line">        q_next, q_eval4next = self.sess.run(</span><br><span class="line">            [self.q_next, self.q_eval],</span><br><span class="line">            feed_dict=&#123;self.s_: batch_memory[:, -self.n_features:],    <span class="comment"># next observation</span></span><br><span class="line">                       self.s: batch_memory[:, -self.n_features:]&#125;)    <span class="comment"># next observation</span></span><br><span class="line">        q_eval = self.sess.run(self.q_eval, &#123;self.s: batch_memory[:, :self.n_features]&#125;)</span><br><span class="line">        q_target = q_eval.copy()</span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>)</span><br><span class="line">        reward = batch_memory[:, self.n_features + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.double_q:   <span class="comment"># 如果是 Double DQN</span></span><br><span class="line">            max_act4next = np.argmax(q_eval4next, axis=<span class="number">1</span>)        <span class="comment"># q_eval 得出的最高奖励动作</span></span><br><span class="line">            selected_q_next = q_next[batch_index, max_act4next]  <span class="comment"># Double DQN 选择 q_next 依据 q_eval 选出的动作</span></span><br><span class="line">        <span class="keyword">else</span>:       <span class="comment"># 如果是 Natural DQN</span></span><br><span class="line">            selected_q_next = np.<span class="built_in">max</span>(q_next, axis=<span class="number">1</span>)    <span class="comment"># natural DQN</span></span><br><span class="line"></span><br><span class="line">        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这下面和 DQN 一样:</span></span><br><span class="line">        _, self.cost = self.sess.run([self._train_op, self.loss],</span><br><span class="line">                                     feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                self.q_target: q_target&#125;)</span><br><span class="line">        self.cost_his.append(self.cost)</span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h4 id="记录-Q-值"><a href="#记录-Q-值" class="headerlink" title="记录 Q 值"></a>记录 Q 值</h4><p>为了记录下我们选择动作时的 Q 值，接下来我们就修改 <code>choose_action()</code> 功能，让它记录下每次选择的 Q 值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        observation = observation[np.newaxis, :]</span><br><span class="line">        actions_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: observation&#125;)</span><br><span class="line">        action = np.argmax(actions_value)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;q&#x27;</span>):  <span class="comment"># 记录选的 Qmax 值</span></span><br><span class="line">            self.q = []</span><br><span class="line">            self.running_q = <span class="number">0</span></span><br><span class="line">        self.running_q = self.running_q*<span class="number">0.99</span> + <span class="number">0.01</span> * np.<span class="built_in">max</span>(actions_value)</span><br><span class="line">        self.q.append(self.running_q)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &gt; self.epsilon:  <span class="comment"># 随机选动作</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h4><p>接着我们就来对比 Natural DQN 和 Double DQN 带来的不同结果啦。<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/run_Pendulum.py">代码在这</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DoubleDQN</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">1</span>) <span class="comment"># 可重复实验</span></span><br><span class="line">MEMORY_SIZE = <span class="number">3000</span></span><br><span class="line">ACTION_SPACE = <span class="number">11</span>    <span class="comment"># 将原本的连续动作分离成 11 个动作</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Natural_DQN&#x27;</span>):</span><br><span class="line">    natural_DQN = DoubleDQN(</span><br><span class="line">        n_actions=ACTION_SPACE, n_features=<span class="number">3</span>, memory_size=MEMORY_SIZE,</span><br><span class="line">        e_greedy_increment=<span class="number">0.001</span>, double_q=<span class="literal">False</span>, sess=sess</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Double_DQN&#x27;</span>):</span><br><span class="line">    double_DQN = DoubleDQN(</span><br><span class="line">        n_actions=ACTION_SPACE, n_features=<span class="number">3</span>, memory_size=MEMORY_SIZE,</span><br><span class="line">        e_greedy_increment=<span class="number">0.001</span>, double_q=<span class="literal">True</span>, sess=sess, output_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">RL</span>):</span></span><br><span class="line">    total_steps = <span class="number">0</span></span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># if total_steps - MEMORY_SIZE &gt; 8000: env.render()</span></span><br><span class="line"></span><br><span class="line">        action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">        f_action = (action-(ACTION_SPACE-<span class="number">1</span>)/<span class="number">2</span>)/((ACTION_SPACE-<span class="number">1</span>)/<span class="number">4</span>)   <span class="comment"># 在 [-2 ~ 2] 内离散化动作</span></span><br><span class="line"></span><br><span class="line">        observation_, reward, done, info = env.step(np.array([f_action]))</span><br><span class="line"></span><br><span class="line">        reward /= <span class="number">10</span>     <span class="comment"># normalize 到这个区间 (-1, 0). 立起来的时候 reward = 0.</span></span><br><span class="line">        <span class="comment"># 立起来以后的 Q target 会变成 0, 因为 Q_target = r + gamma * Qmax(s&#x27;, a&#x27;) = 0 + gamma * 0</span></span><br><span class="line">        <span class="comment"># 所以这个状态时的 Q 值大于 0 时, 就出现了 overestimate.</span></span><br><span class="line"></span><br><span class="line">        RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; MEMORY_SIZE:   <span class="comment"># learning</span></span><br><span class="line">            RL.learn()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps - MEMORY_SIZE &gt; <span class="number">20000</span>:   <span class="comment"># stop game</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        observation = observation_</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> RL.q <span class="comment"># 返回所有动作 Q 值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train 两个不同的 DQN</span></span><br><span class="line">q_natural = train(natural_DQN)</span><br><span class="line">q_double = train(double_DQN)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出对比图</span></span><br><span class="line">plt.plot(np.array(q_natural), c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;natural&#x27;</span>)</span><br><span class="line">plt.plot(np.array(q_double), c=<span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;double&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Q eval&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>所以这个出来的图是这样：</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-5-4.png"></p>
<p>可以看出，Natural DQN 学得差不多后，在立起来时，大部分时间都是估计的 Q值要大于0，这时就出现了 overestimate，而 Double DQN 的 Q值就消除了一些 overestimate，将估计值保持在 0 左右。</p>
<h3 id="Prioritized-Experience-Replay-DQN"><a href="#Prioritized-Experience-Replay-DQN" class="headerlink" title="Prioritized Experience Replay (DQN)"></a>Prioritized Experience Replay (DQN)</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>这一次还是使用 MountainCar 来进行实验，因为这次我们不需要重度改变他的 reward 了。所以只要是没有拿到小旗子，reward=-1，拿到小旗子时，我们定义它获得了 +10 的 reward。比起之前 DQN 中，这个 reward 定义更加准确。如果使用这种 reward 定义方式，可以想象 Natural DQN 会花很久的时间学习，因为记忆库中只有很少很少的 +10 reward 可以学习。正负样本不一样。而使用 Prioritized replay，就会重视这种少量的，但值得学习的样本。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="Prioritized-replay-算法"><a href="#Prioritized-replay-算法" class="headerlink" title="Prioritized replay 算法"></a>Prioritized replay 算法</h4><p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-6-1.png"></p>
<p>这一套算法重点就在我们 batch 抽样的时候并不是随机抽样，而是按照 Memory 中的样本优先级来抽。所以这能更有效地找到我们需要学习的样本。</p>
<p>那么样本的优先级是怎么定的呢？原来我们可以用到 <code>TD-error</code>，也就是 <code>Q现实 - Q估计</code> 来规定优先学习的程度。如果 <code>TD-error</code> 越大，就代表我们的预测精度还有很多上升空间，那么这个样本就越需要被学习，也就是优先级 <code>p</code> 越高。</p>
<p>有了 <code>TD-error</code> 就有了优先级 <code>p</code>，那我们如何有效地根据 <code>p</code> 来抽样呢？如果每次抽样都需要针对 <code>p</code> 对所有样本排序，这将会是一件非常消耗计算能力的事。好在我们还有其他方法，这种方法不会对得到的样本进行排序。这就是这篇 <a href="https://arxiv.org/abs/1511.05952">paper</a> 中提到的 <code>SumTree</code>。</p>
<p>SumTree 是一种树形结构，每片树叶存储每个样本的优先级 <code>p</code>，每个树枝节点只有两个分叉，节点的值是两个分叉的合，所以 SumTree 的顶端就是所有 <code>p</code> 的合。正如下面<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/">图片(来自Jaromír Janisch)</a>，最下面一层树叶存储样本的 <code>p</code>，叶子上一层最左边的 13 = 3 + 10，按这个规律相加，顶层的 root 就是全部 <code>p</code> 的合了。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-6-2.png"></p>
<p>抽样时，我们会将 <code>p</code> 的总合除以 batch size，分成 batch size 那么多区间，(n=sum(p)/batch_size)。如果将所有 node 的 priority 加起来是42的话，我们如果抽6个样本，这时的区间拥有的 priority 可能是这样。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0-7), [7-14), [14-21), [21-28), [28-35), [35-42]</span><br></pre></td></tr></table></figure>

<p>然后在每个区间里随机选取一个数。比如在第区间 <code>&lt;a href=&quot;https://github.com/jaara/AI-blog/blob/master/SumTree.py&quot; target=&#39;_blank&#39; &gt;21-28)</code> 里选到了24，就按照这个 24 从最顶上的42开始向下搜索。首先看到最顶上 <code>42</code> 下面有两个 child nodes，拿着手中的24对比左边的 child <code>29</code>，如果 左边的 child 比自己手中的值大，那我们就走左边这条路，接着再对比 <code>29</code> 下面的左边那个点 <code>13</code>，这时，手中的 24 比 <code>13</code> 大，那我们就走右边的路，并且将手中的值根据 <code>13</code> 修改一下，变成 24-13 = 11。接着拿着 11 和 <code>13</code> 左下角的 <code>12</code> 比，结果 <code>12</code> 比 11 大，那我们就选 12 当做这次选到的 priority，并且也选择 12 对应的数据。</p>
<h4 id="SumTree-有效抽样"><a href="#SumTree-有效抽样" class="headerlink" title="SumTree 有效抽样"></a>SumTree 有效抽样</h4><p><strong>注意: 下面的代码和视频中有一点点不同, 下面的代码是根据评论中讨论的进行了修改, 多谢大家的评论.</strong></p>
<p>首先要提的是，这个 SumTree 的算法是对于 Jaromír Janisch 写的 Sumtree 的修改版。Jaromír Janisch 的代码在更新 sumtree 的时候和抽样的时候多次用到了 recursive 递归结构，我使用的是 while 循环，测试要比递归结构运行快。在 class 中的功能也比它的代码少几个，我优化了一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SumTree</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 建立 tree 和 data,</span></span><br><span class="line">    <span class="comment"># 因为 SumTree 有特殊的数据结构,</span></span><br><span class="line">    <span class="comment"># 所以两者都能用一个一维 np.array 来存储</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, capacity</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当有新 sample 时, 添加进 tree 和 data</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, p, data</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当 sample 被 train, 有了新的 TD-error, 就在 tree 中更新</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, tree_idx, p</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选取的 v 点抽取样本</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_leaf</span>(<span class="params">self, v</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取 sum(priorities)</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">totoal_p</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>

<p>具体的抽要和更新值的规则和上面说的类似。具体的代码在这里呈现的话比较累赘，详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L18-L86">Github对应的位置</a></p>
<h4 id="Memory-类"><a href="#Memory-类" class="headerlink" title="Memory 类"></a>Memory 类</h4><p>这个 Memory 类也是基于 <a href="https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py">Jaromír Janisch 所写的 Memory</a> 进行了修改和优化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 建立 SumTree 和各种参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, capacity</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储数据, 更新 SumTree</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 抽取 sample</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># train 完被抽取的 samples 后更新在 tree 中的 sample 的 priority</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch_update</span>(<span class="params">self, tree_idx, abs_errors</span>):</span></span><br></pre></td></tr></table></figure>

<p>具体的代码在这里呈现的话比较累赘，详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L89-L129">Github对应的位置</a> 下面有很多朋友经常问的一个问题，这个 ISweight 到底怎么算。需要提到的一点是，代码中的计算方法是经过了简化的，将 paper 中的步骤合并了一些。比如 <code>prob = p / self.tree.total_p; ISWeights = np.power(prob/min_prob, -self.beta)</code></p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-6-5.png"></p>
<p>下面是我的推导，如果有不正确还请指出。在paper 中，<code>ISWeight = (N*Pj)^(-beta) / maxi_wi</code> 里面的 <code>maxi_wi</code> 是为了 normalize ISWeight，所以我们先把他放在一边。所以单纯的 importance sampling 就是 <code>(N*Pj)^(-beta)</code>，那 <code>maxi_wi = maxi[(N*Pi)^(-beta)]</code>。</p>
<p>如果将这两个式子合并，</p>
<p><code>ISWeight = (N*Pj)^(-beta) / maxi[ (N*Pi)^(-beta) ]</code></p>
<p>而且如果将 <code>maxi[ (N*Pi)^(-beta) ]</code> 中的 (-beta) 提出来，这就变成了 <code>mini[ (N*Pi) ] ^ (-beta)</code></p>
<p>看出来了吧，有的东西可以抵消掉的。最后</p>
<p><code>ISWeight = (Pj / mini[Pi])^(-beta)</code></p>
<p>这样我们就有了代码中的样子。</p>
<p>还有代码中的 <code>alpha</code> 是一个决定我们要使用多少 ISweight 的影响，如果 <code>alpha = 0</code>，我们就没使用到任何 Importance Sampling。</p>
<h4 id="更新方法-1"><a href="#更新方法-1" class="headerlink" title="更新方法"></a>更新方法</h4><p>基于之前的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py">DQN 代码</a>，我们做出以下修改。我们将 class 的名字改成 <code>DQNPrioritiedReplay</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>prioritized</code> 参数来表示 DQN 是否具备 prioritized 能力。为了对比的需要，我们的 <code>tf.Session()</code> 也单独传入。并移除原本在 DQN 代码中的这一句：</p>
<p><code>self.sess.run(tf.global_variables_initializer())</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritiedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">..., prioritized=<span class="literal">True</span>, sess=<span class="literal">None</span></span>)</span></span><br><span class="line">        self.prioritized = prioritized</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            self.memory = Memory(capacity=memory_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.memory = np.zeros((self.memory_size, n_features*<span class="number">2</span>+<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sess = sess</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-6-3.png"></p>
<p>搭建神经网络时，我们发现 DQN with Prioritized replay 只多了一个 <code>ISWeights</code>，这个正是<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/4-6-prioritized-replay/#algorithm">刚刚算法中</a>提到的 <code>Importance-Sampling Weights</code>，用来恢复被 Prioritized replay 打乱的抽样概率分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritizedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">        ...</span></span><br><span class="line"><span class="function">        # <span class="title">self</span>.<span class="title">prioritized</span> 时 <span class="title">eval</span> <span class="title">net</span> 的 <span class="title">input</span> 多加了一个 <span class="title">ISWeights</span></span></span><br><span class="line">        self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#x27;s&#x27;)  # input</span><br><span class="line">        self.q_target = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_actions], name=<span class="string">&#x27;Q_target&#x27;</span>)  <span class="comment"># for calculating loss</span></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            self.ISWeights = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">&#x27;IS_weights&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 为了得到 abs 的 TD error 并用于修改这些 sample 的 priority, 我们修改如下</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> self.prioritized:</span><br><span class="line">                self.abs_errors = tf.reduce_sum(tf.<span class="built_in">abs</span>(self.q_target - self.q_eval), axis=<span class="number">1</span>)    <span class="comment"># for updating Sumtree</span></span><br><span class="line">                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br></pre></td></tr></table></figure>

<p>因为和 Natural DQN 使用的 Memory 不一样，所以在存储 transition 的时候方式也略不相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritizedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:    <span class="comment"># prioritized replay</span></span><br><span class="line">            transition = np.hstack((s, [a, r], s_))</span><br><span class="line">            self.memory.store(transition)</span><br><span class="line">        <span class="keyword">else</span>:       <span class="comment"># random replay</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;memory_counter&#x27;</span>):</span><br><span class="line">                self.memory_counter = <span class="number">0</span></span><br><span class="line">            transition = np.hstack((s, [a, r], s_))</span><br><span class="line">            index = self.memory_counter % self.memory_size</span><br><span class="line">            self.memory[index, :] = transition</span><br><span class="line">            self.memory_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>接下来是相对于 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py">Natural DQN 代码</a>，我们在 <code>learn()</code> 改变的部分也在如下展示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritizedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 相对于 DQN 代码, 改变的部分</span></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">            batch_memory = self.memory[sample_index, :]</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            _, abs_errors, self.cost = self.sess.run([self._train_op, self.abs_errors, self.loss],</span><br><span class="line">                                         feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                    self.q_target: q_target,</span><br><span class="line">                                                    self.ISWeights: ISWeights&#125;)</span><br><span class="line">            self.memory.batch_update(tree_idx, abs_errors)   <span class="comment"># update priority</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            _, self.cost = self.sess.run([self._train_op, self.loss],</span><br><span class="line">                                         feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                    self.q_target: q_target&#125;)</span><br><span class="line"></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<h4 id="对比结果-1"><a href="#对比结果-1" class="headerlink" title="对比结果"></a>对比结果</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-6-4.png"></p>
<p>运行我 Github 中的这个 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/run_MountainCar.py">MountainCar 脚本</a>，我们就不难发现，我们都从两种方法最初拿到第一个 <code>R=+10</code> 奖励的时候算起，看看经历过一次 <code>R=+10</code> 后，他们有没有好好利用这次的奖励，可以看出，有 Prioritized replay 的可以高效的利用这些不常拿到的奖励，并好好学习他们。所以 Prioritized replay 会更快结束每个 episode，很快就到达了小旗子。</p>
<h3 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h3><h4 id="要点-3"><a href="#要点-3" class="headerlink" title="要点"></a>要点</h4><p>只要稍稍修改 DQN 中神经网络的结构，就能大幅提升学习效果，加速收敛。这种新方法叫做 Dueling DQN。用一句话来概括 Dueling DQN 就是。它将每个动作的 Q 拆分成了 state 的 Value 加上每个动作的 Advantage。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/Pendulum%20DQN.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="Dueling-算法"><a href="#Dueling-算法" class="headerlink" title="Dueling 算法"></a>Dueling 算法</h4><p>上一个 Paper 中的经典解释图片，上者是一般的 DQN 的 Q值神经网络。下者就是 Dueling DQN 中的 Q值神经网络了。那具体是哪里不同了呢？</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-7-1.png"></p>
<p>下面这个公式解释了不同之处。原来 DQN 神经网络直接输出的是每种动作的 Q值，而 Dueling DQN 每个动作的 Q值是由下面的公式确定的。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-7-2.png"></p>
<p>它分成了这个 state 的值，加上每个动作在这个 state 上的 advantage。因为有时候在某种 state，无论做什么动作，对下一个 state 都没有多大影响。比如 paper 中的这张图。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-7-3.png"></p>
<p>这是开车的游戏，左边是 state value，发红的部分证明了 state value 和前面的路线有关，右边是 advantage，发红的部分说明了 advantage 很在乎旁边要靠近的车子，这时的动作会受更多 advantage 的影响。发红的地方左右了自己车子的移动原则。</p>
<h4 id="更新方法-2"><a href="#更新方法-2" class="headerlink" title="更新方法"></a>更新方法</h4><p>下面的修改都是基于我之前写的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py">DQN 代码</a>。这次修改的部分比较少。我们把它们写在一块。如果想直接看全部代码，<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5.3_Dueling_DQN">请戳这里</a>。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-7-4.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuelingDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">..., dueling=<span class="literal">True</span>, sess=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">        ...</span></span><br><span class="line">        self.dueling = dueling  # 会建立两个 DQN, 其中一个是 Dueling DQN</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> sess <span class="keyword">is</span> <span class="literal">None</span>:    <span class="comment"># 针对建立两个 DQN 的模式修改了 tf.Session() 的建立方式</span></span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sess = sess</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">build_layers</span>(<span class="params">s, c_names, n_l1, w_initializer, b_initializer</span>):</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):   <span class="comment"># 第一层, 两种 DQN 都一样</span></span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.dueling:</span><br><span class="line">                <span class="comment"># Dueling DQN</span></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Value&#x27;</span>):    <span class="comment"># 专门分析 state 的 Value</span></span><br><span class="line">                    w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, <span class="number">1</span>], initializer=w_initializer, collections=c_names)</span><br><span class="line">                    b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, <span class="number">1</span>], initializer=b_initializer, collections=c_names)</span><br><span class="line">                    self.V = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Advantage&#x27;</span>):    <span class="comment"># 专门分析每种动作的 Advantage</span></span><br><span class="line">                    w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                    b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                    self.A = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):    <span class="comment"># 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值</span></span><br><span class="line">                    out = self.V + (self.A - tf.reduce_mean(self.A, axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>))     <span class="comment"># Q = V(s) + A(s,a)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):    <span class="comment"># 普通的 DQN 第二层</span></span><br><span class="line">                    w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                    b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                    out = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<h4 id="对比结果-2"><a href="#对比结果-2" class="headerlink" title="对比结果"></a>对比结果</h4><p>对比的代码不在这里呈现，如果想观看对比的详细代码，请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.3_Dueling_DQN/run_Pendulum.py">Github</a>。</p>
<p>这次我们看看累积奖励 reward，杆子立起来的时候奖励 = 0，其他时候都是负值，所以当累积奖励没有在降低时，说明杆子已经被成功立了很久了。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-7-5.png"></p>
<p>我们发现当可用动作越高，学习难度就越大，不过 Dueling DQN 还是会比 Natural DQN 学习得更快、收敛效果更好。</p>
<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/6_OpenAI_gym">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">什么是 DQN 短视频</a></li>
<li><a href="https://gym.openai.com/envs/">OpenAI gym 官网</a></li>
<li>本节内容的模拟视频效果:<ul>
<li>CartPole: <a href="https://www.youtube.com/watch?v=qlqqezju0xo">Youtube</a>, <a href="https://www.youtube.com/watch?v=qlqqezju0xo">Youtube</a></li>
<li>Mountain Car: <a href="https://www.youtube.com/watch?v=r1mNIDN3zNM">Youtube</a>, <a href="https://www.youtube.com/watch?v=r1mNIDN3zNM">Youtube</a></li>
</ul>
</li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>论文 <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a></li>
<li>论文 <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></li>
<li>论文 <a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
</search>
