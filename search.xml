<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MacBook Air M1安装gym问题汇总</title>
    <url>/YingYingMonstre.github.io/2021/11/03/MacBook%20Air%20M1%E5%AE%89%E8%A3%85gym%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<p>安装gym花了一天时间，中途出了很多问题，特此记录。</p>
<ol start="0">
<li>关于本机</li>
</ol>
<ul>
<li>macOS Monterey版本12.0.1</li>
<li>MacBook Air（M1，2020）</li>
<li>芯片 Apple M1</li>
</ul>
<ol>
<li>环境相关</li>
</ol>
<p>创建pycharm环境参考文章：<a href="https://zhuanlan.zhihu.com/p/410961551">https://zhuanlan.zhihu.com/p/410961551</a></p>
<ol start="2">
<li>下载gym</li>
</ol>
<p>可以直接在终端上pip install gym，但是在pycharm中使用时会报错缺一些组件，比如No module named ‘pyglet’。本人使用的是下边的方法：</p>
<p>git clone <a href="https://github.com/openai/gym.git">https://github.com/openai/gym.git</a></p>
<p>cd gym</p>
<p>pip install -e ‘.[all]’</p>
<ol start="3">
<li>No matching distribution found for ale-py~=0.7.1 (from gym==0.21.0)</li>
</ol>
<p>问题状况：在pip install -e ‘.[all]’时报错。</p>
<p>解决方法：直接pip install ale-py，如果不能下载（具体报错的原因忘了），尝试conda update pip，我是在更新完pip后可以下载的。</p>
<ol start="4">
<li>unable to execute ‘swig’: No such file or directory</li>
</ol>
<p>问题状况：解决了第3步的问题后，在pip install -e ‘.[all]’过程中出现。</p>
<p>解决方法：brew install swig</p>
<ol start="5">
<li>zsh: command not found: brew</li>
</ol>
<p>问题状况：在第4步输入brew install swig后报错。</p>
<p>解决方法：mac安装homebrew，</p>
<p>用以下命令安装，序列号选择中科大（1）的</p>
<p>/bin/zsh -c “$(curl -fsSL <a href="https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;">https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;</a></p>
<p>原文地址：<a href="https://links.jianshu.com/go?to=https://blog.csdn.net/wangyun71/article/details/108560873">https://blog.csdn.net/wangyun71/article/details/108560873</a></p>
<ol start="6">
<li>from . import multiarray等</li>
</ol>
<p>问题状况：在解决了第4、5步的问题后继续运行pip install -e ‘.[all]’，成功安装。但是在测试用例代码时报的错误。提示检查python和numpy的版本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试用例</span></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></span><br></pre></td></tr></table></figure>

<p>解决方法：重新安装了numpy之后就可以用了。</p>
<p>pip uninstall numpy</p>
<p>pip install numpy</p>
<ol start="7">
<li>zsh:killed</li>
</ol>
<p>问题状况：偶然遇到的，在conda activate环境后不能用clear、pip等命令，不知道是什么原因。</p>
<p>解决方法：删除环境重新搭建环境就可以了</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ModbusTCP协议学习</title>
    <url>/YingYingMonstre.github.io/2021/09/17/ModbusTCP%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Modbus由MODICON公司于1979年开发，是一种<strong>工业现场总线</strong>协议标准。1996年<strong>施耐德</strong>公司推出基于<strong>以太网TCP/IP</strong>的Modbus协议：<strong>Modbus TCP</strong>。</p>
<p>Modbus协议是一项应用层报文传输协议，包括ASCII、RTU、<strong>TCP</strong>三种报文类型。</p>
<p>标准的Modbus协议物理层接口有RS232、RS422、RS485和<strong>以太网</strong>接口，采用<strong>master/slave</strong>方式通信。</p>
<h2 id="Modbus-TCP数据帧"><a href="#Modbus-TCP数据帧" class="headerlink" title="Modbus TCP数据帧"></a>Modbus TCP数据帧</h2><p>Modbus TCP的数据帧可分为两部分：<strong>MBAP</strong>+<strong>PDU</strong>。其协议特征如图所示。</p>
<p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fwww.51wendang.com%2Fpic%2Fe5675f007044fd1446490edf%2F2-537-png_6_0_0_464_608_341_194_892.8_1262.699-947-0-526-947.jpg&refer=http%3A%2F%2Fwww.51wendang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1634438115&t=92f4357504d2885188a29589d7de4e7f"></p>
<center>Modbus TCP协议特征</center>

<h3 id="报文头MBAP"><a href="#报文头MBAP" class="headerlink" title="报文头MBAP"></a>报文头MBAP</h3><p>MBAP为报文头，长度为7字节，组成如下：</p>
<table>
<thead>
<tr>
<th align="left">事务处理标识</th>
<th align="left">协议标识</th>
<th align="left">长度</th>
<th align="left">单元标识符</th>
</tr>
</thead>
<tbody><tr>
<td align="left">2字节</td>
<td align="left">2字节</td>
<td align="left">2字节</td>
<td align="left">1字节</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>内容</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td><strong>事务处理标识</strong></td>
<td>可以理解为报文的序列号，一般每次通信之后就要加1以区别不同的通信数据报文。</td>
</tr>
<tr>
<td><strong>协议标识符</strong></td>
<td>00 00表示Modbus TCP协议。</td>
</tr>
<tr>
<td><strong>长度</strong></td>
<td>表示接下来的数据长度，单位为字节。</td>
</tr>
<tr>
<td><strong>单元标识符</strong></td>
<td>可以理解为设备地址。</td>
</tr>
</tbody></table>
<h3 id="帧结构PDU"><a href="#帧结构PDU" class="headerlink" title="帧结构PDU"></a>帧结构PDU</h3><p>PDU由<strong>功能码+数据</strong>组成。功能码为1字节，数据长度不定，由具体功能决定。</p>
<p><strong>功能码</strong></p>
<p>Modbus的操作对象有四种：线圈、离散输入、保持寄存器、输入寄存器。</p>
<table>
<thead>
<tr>
<th align="center">对象</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线圈</td>
<td align="center">PLC的输出位，开关量，在Modbus中可读可写</td>
</tr>
<tr>
<td align="center">离散量</td>
<td align="center">PLC的输入位，开关量，在Modbus中只读</td>
</tr>
<tr>
<td align="center">输入寄存器</td>
<td align="center">PLC中只能从模拟量输入端改变的寄存器，在Modbus中只读</td>
</tr>
<tr>
<td align="center">保持寄存器</td>
<td align="center">PLC中用于输出模拟量信号的寄存器，在Modbus中可读可写</td>
</tr>
</tbody></table>
<p>根据对象的不同，Modbus的功能码有：</p>
<table>
<thead>
<tr>
<th align="center">功能码</th>
<th align="center">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0x01</td>
<td align="center">读线圈</td>
</tr>
<tr>
<td align="center">0x05</td>
<td align="center">写单个线圈</td>
</tr>
<tr>
<td align="center">0x0F</td>
<td align="center">写多个线圈</td>
</tr>
<tr>
<td align="center">0x02</td>
<td align="center">读离散量输入</td>
</tr>
<tr>
<td align="center">0x04</td>
<td align="center">读输入寄存器</td>
</tr>
<tr>
<td align="center">0x03</td>
<td align="center">读保持寄存器</td>
</tr>
<tr>
<td align="center">0x06</td>
<td align="center">写单个保持寄存器</td>
</tr>
<tr>
<td align="center">0x10</td>
<td align="center">写多个保持寄存器</td>
</tr>
</tbody></table>
<p>说明更详细的表</p>
<table>
<thead>
<tr>
<th align="center">代码</th>
<th align="center">中文名称</th>
<th align="center">英文名</th>
<th align="center">位操作/字操作</th>
<th align="center">操作数量</th>
</tr>
</thead>
<tbody><tr>
<td align="center">01</td>
<td align="center">读线圈状态</td>
<td align="center">READ COIL STATUS</td>
<td align="center">位操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">02</td>
<td align="center">读离散输入状态</td>
<td align="center">READ INPUT STATUS</td>
<td align="center">位操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">03</td>
<td align="center">读保持寄存器</td>
<td align="center">READ HOLDING REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">04</td>
<td align="center">读输入寄存器</td>
<td align="center">READ INPUT REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个或多个</td>
</tr>
<tr>
<td align="center">05</td>
<td align="center">写线圈状态</td>
<td align="center">WRITE SINGLE COIL</td>
<td align="center">位操作</td>
<td align="center">单个</td>
</tr>
<tr>
<td align="center">06</td>
<td align="center">写单个保持寄存器</td>
<td align="center">WRITE SINGLE REGISTER</td>
<td align="center">字操作</td>
<td align="center">单个</td>
</tr>
<tr>
<td align="center">15</td>
<td align="center">写多个线圈</td>
<td align="center">WRITE MULTIPLE COIL</td>
<td align="center">位操作</td>
<td align="center">多个</td>
</tr>
<tr>
<td align="center">16</td>
<td align="center">写多个保持寄存器</td>
<td align="center">WRITE MULTIPLE REGISTER</td>
<td align="center">字操作</td>
<td align="center">多个</td>
</tr>
</tbody></table>
<h2 id="PDU详细结构"><a href="#PDU详细结构" class="headerlink" title="PDU详细结构"></a>PDU详细结构</h2><p>测试软件：mod_RSsim5.3</p>
<table>
<thead>
<tr>
<th align="center">模式</th>
<th align="center">对应</th>
</tr>
</thead>
<tbody><tr>
<td align="center">线圈</td>
<td align="center">Coil Outputs</td>
</tr>
<tr>
<td align="center">离散量</td>
<td align="center">Digital Inputs</td>
</tr>
<tr>
<td align="center">输入寄存器</td>
<td align="center">Analogue Inputs</td>
</tr>
<tr>
<td align="center">保持寄存器</td>
<td align="center">Holding Registers</td>
</tr>
</tbody></table>
<p><strong>0x01：读线圈</strong></p>
<p>在从站中读1~2000个连续线圈状态，ON=1,OFF=0</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 数量H 数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 数据（一个地址的数据为1位）</li>
<li>如：在从站0x01中，读取开始地址为0x0002的线圈数据，读0x0008位<br>00 01 00 00 00 06 01 01 00 02 00 08</li>
<li>回：数据长度为0x01个字节，数据为0x01，第一个线圈为ON，其余为OFF<br>00 01 00 00 00 04 01 01 01 01</li>
</ul>
<p><strong>0x05：写单个线圈</strong></p>
<p>将从站中的一个输出写成ON或OFF，0xFF00请求输出为ON,0x000请求输出为OFF</p>
<ul>
<li>请求：MBAP 功能码 输出地址H 输出地址L 输出值H 输出值L（共12字节）</li>
<li>响应：MBAP 功能码 输出地址H 输出地址L 输出值H 输出值L（共12字节）</li>
<li>如：将地址为0x0003的线圈设为ON<br>00 01 00 00 00 06 01 05 00 03 FF 00</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 05 00 03 FF 00</li>
</ul>
<p><strong>0x0F：写多个线圈</strong></p>
<p>将一个从站中的一个线圈序列的每个线圈都强制为ON或OFF，数据域中置1的位请求相应输出位ON，置0的位请求响应输出为OFF</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 输出数量H 输出数量L 字节长度 输出值H 输出值L</li>
<li>响应：MBAP 功能码 起始地址H 起始地址L 输出数量H 输出数量L</li>
</ul>
<p><strong>0x02：读离散量输入</strong></p>
<p>从一个从站中读1~2000个连续的离散量输入状态</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 数量H 数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 数据（长度：9+ceil（数量/8））</li>
<li>如：从地址0x0000开始读0x0012个离散量输入<br>00 01 00 00 00 06 01 02 00 00 00 12</li>
<li>回：数据长度为0x03个字节，数据为0x01 04 00，表示第一个离散量输入和第11个离散量输入为ON，其余为OFF<br>00 01 00 00 00 06 01 02 03 01 04 00</li>
</ul>
<p><strong>0x04：读输入寄存器</strong></p>
<p>从一个远程设备中读1~2000个连续输入寄存器</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 寄存器数据(长度：9+寄存器数量×2)</li>
<li>如：读起始地址为0x0002，数量为0x0005的寄存器数据<br>00 01 00 00 00 06 01 04 00 02 00 05</li>
<li>回：数据长度为0x0A，第一个寄存器的数据为0x0c，其余为0x00<br>00 01 00 00 00 0D 01 04 0A 00 0C 00 00 00 00 00 00 00 00</li>
</ul>
<p><strong>0x03：读保持寄存器</strong></p>
<p>从远程设备中读保持寄存器连续块的内容</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>响应：MBAP 功能码 数据长度 寄存器数据(长度：9+寄存器数量×2)</li>
<li>如：起始地址是0x0000，寄存器数量是 0x0003<br>00 01 00 00 00 06 01 03 00 00 00 03</li>
<li>回：数据长度为0x06，第一个寄存器的数据为0x21，其余为0x00<br>00 01 00 00 00 09 01 03 06 00 21 00 00 00 00</li>
</ul>
<p><strong>0x06：写单个保持寄存器</strong></p>
<p>在一个远程设备中写一个保持寄存器</p>
<ul>
<li>请求：MBAP 功能码 寄存器地址H 寄存器地址L 寄存器值H 寄存器值L（共12字节）</li>
<li>响应：MBAP 功能码 寄存器地址H 寄存器地址L 寄存器值H 寄存器值L（共12字节）</li>
<li>如：向地址是0x0000的寄存器写入数据0x000A<br>00 01 00 00 00 06 01 06 00 00 00 0A</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 06 00 00 00 0A</li>
</ul>
<p><strong>0x10：写多个保持寄存器</strong></p>
<p>在一个远程设备中写连续寄存器块（1~123个寄存器）</p>
<ul>
<li>请求：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L 字节长度 寄存器值（13+寄存器数量×2）</li>
<li>响应：MBAP 功能码 起始地址H 起始地址L 寄存器数量H 寄存器数量L（共12字节）</li>
<li>如：向起始地址为0x0000，数量为0x0001的寄存器写入数据，数据长度为0x02，数据为0x000F<br>00 01 00 00 00 09 01 10 00 00 00 01 02 00 0F</li>
<li>回：写入成功<br>00 01 00 00 00 06 01 10 00 00 00 01</li>
</ul>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    HOST = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    TCP_IP = <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    TCP_PORT = <span class="number">502</span></span><br><span class="line">    MaxBytes = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建socket连接</span></span><br><span class="line">    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        s.connect((TCP_IP, TCP_PORT))</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;error&#x27;</span>, e)</span><br><span class="line">        s.close()</span><br><span class="line">        sys.exit()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 连接成功</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;have connected with server&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 十进制、十六进制都可以</span></span><br><span class="line">    <span class="comment"># 示例为写一个保持寄存器</span></span><br><span class="line">    arr = [<span class="number">00</span>, <span class="number">1</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">00</span>, <span class="number">0x0A</span>]</span><br><span class="line">    data = struct.pack(<span class="string">&quot;%dB&quot;</span> % (<span class="built_in">len</span>(arr)), *arr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        s.settimeout(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># s.sendall(data) 发送数据包</span></span><br><span class="line">        sendBytes = s.send(data)</span><br><span class="line">        <span class="keyword">if</span> sendBytes &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接受响应信息</span></span><br><span class="line">        recvData = s.recv(MaxBytes)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> recvData:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;接收数据为空，我要退出了&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        localTime = time.asctime(time.localtime(time.time()))</span><br><span class="line">        <span class="built_in">print</span>(localTime, <span class="string">&#x27; 接收到数据字节数:&#x27;</span>, <span class="built_in">len</span>(recvData))</span><br><span class="line">        <span class="built_in">print</span>(struct.unpack(<span class="string">&quot;%dB&quot;</span> % (<span class="built_in">len</span>(recvData)), recvData))</span><br><span class="line"></span><br><span class="line">        localTime = time.asctime(time.localtime(time.time()))</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    s.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;我已经退出了，后会无期&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="Modbus-TCP-示例报文"><a href="#Modbus-TCP-示例报文" class="headerlink" title="Modbus TCP 示例报文"></a>Modbus TCP 示例报文</h2><p>ModBusTcp与串行链路Modbus的数据域是一致的，具体数据域可以参考串行Modbus。这里给出几个ModbusTcp的链路解析说明，辅助新人分析报文。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b0fa067f61a600643c84f36ea69c49bb.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/2a92c75438fa7336652d27ecd6081a08.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/616f2f4ff46da56aacec9bf7266043db.png"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/c62b523c4e499d641ae06973b6b29c95.png"></p>
<blockquote>
<p>功能码 0x10：写多个保持寄存器。上面图片3和图片4都写错了。</p>
</blockquote>
<h2 id="ModbusTCP通信"><a href="#ModbusTCP通信" class="headerlink" title="ModbusTCP通信"></a>ModbusTCP通信</h2><p><strong>通信方式</strong></p>
<p>Modbus设备可分为主站(poll)和从站(slave)。主站只有一个，从站有多个，主站向各从站发送请求帧，从站给予响应。在使用TCP通信时，主站为client端，主动建立连接；从站为server端，等待连接。</p>
<ul>
<li>主站请求：功能码+数据</li>
<li>从站正常响应：请求功能码+响应数据</li>
<li>从站异常响应：异常功能码+异常码，其中异常功能码即将请求功能码的最高有效位置1，异常码指示差错类型</li>
<li><strong>注意：需要超时管理机制，避免无期限的等待可能不出现的应答</strong></li>
</ul>
<p>IANA（Internet Assigned Numbers Authority，互联网编号分配管理机构）给Modbus协议赋予TCP端口号为<strong>502</strong>，这是目前在仪表与自动化行业中唯一分配到的端口号。</p>
<p><strong>通信过程</strong></p>
<ol>
<li>connect 建立TCP连接</li>
<li>准备Modbus报文</li>
<li>使用send命令发送报文</li>
<li>在同一连接下等待应答</li>
<li>使用recv命令读取报文，完成一次数据交换</li>
<li>通信任务结束时，关闭TCP连接</li>
</ol>
<h2 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h2><p>在工业自动化控制中，经常会遇到开关量，数字量，模拟量，离散量，脉冲量等各种概念，而人们在实际应用中，对于这些概念又很容易混淆。现将各种概念罗列如下：</p>
<p><strong>1.开关量：</strong></p>
<p>一般指的是触点的“开”与“关”的状态，一般在计算机设备中也会用“0”或“1”来表示开关量的状态。开关量分为有源开关量信号和无源开关量信号，有源开关量信号指的是“开”与“关”的状态是带电源的信号，专业叫法为跃阶信号，可以理解为脉冲量，一般的都有220VAC, 110VAC,24VDC,12VDC等信号，无源开关量信号指的是“开”和“关”的状态时不带电源的信号，一般又称之为干接点。电阻测试法为电阻0或无穷大。</p>
<p><strong>2.数字量：</strong></p>
<p>很多人会将数字量与开关量混淆，也将其与模拟量混淆。数字量在时间和数量上都是离散的物理量，其表示的信号则为数字信号。数字量是由0和1组成的信号，经过编码形成有规律的信号，量化后的模拟量就是数字量。</p>
<p><strong>3.模拟量：</strong></p>
<p>模拟量的概念与数字量相对应，但是经过量化之后又可以转化为数字量。模拟量是在时间和数量上都是连续的物理量，其表示的信号则为模拟信号。模拟量在连续的变化过程中任何一个取值都是一个具体有意义的物理量，如温度，电压，电流等。</p>
<p><strong>4.离散量：</strong></p>
<p>离散量是将模拟量离散化之后得到的物理量。即任何仪器设备对于模拟量都不可能有个完全精确的表示，因为他们都有一个采样周期，在该采样周期内，其物理量的数值都是不变的，而实际上的模拟量则是变化的。这样就将模拟量离散化，成为了离散量。</p>
<p><strong>5.脉冲量：</strong></p>
<p>脉冲量就是瞬间电压或电流由某一值跃变到另一值的信号量。在量化后，其变化持续有规律就是数字量，如果其由0变成某一固定值并保持不变，其就是开关量。</p>
<blockquote>
<p>综上所述，模拟量就是在某个过程中时间和数量连续变化的物理量，由于在实际的应用中，所有的仪器设备对于外界数据的采集都有一个采样周期，其采集的数据只有在下一个采样周期开始时才有变动，采样周期内其数值并不随模拟量的变化而变动。</p>
<p>这样就将模拟量离散化了，例如：某设备的采样周期为1秒，其在第五秒的时间采集的温度为35度，而第六秒的温度为36度，该设备就只能标称第五秒时间温度35度，第六秒时间温度36度，而第五点五秒的时间其标称也只是35度，但是其实际的模拟量是35.5度。这样就将模拟信号离散化。其采集的数据就是离散化了，不再是连续的模拟量信号。</p>
<p>由于计算机只识别0和1两个信号，即开关量信号，用其来表示数值都是使用数字串来表示，由于计算能力的问题，其数字串不能无限长，即其表达的精度也是有限的，同样的以温度为例，由于数字串限制，其表达温度的精度只能达到0.1度，小于该单位的数值则不能被标称，这样就必须将离散量进行量化，将其变为数字量。即35.68度的温度则表示为35.6度。</p>
</blockquote>
<p>参考文章：<a href="https://www.cnblogs.com/ioufev/articles/10830028.html">ModbusTCP协议 - ioufev - 博客园 (cnblogs.com)</a></p>
]]></content>
      <categories>
        <category>protocol</category>
      </categories>
      <tags>
        <tag>Modbus</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch神经网络基础</title>
    <url>/YingYingMonstre.github.io/2021/12/07/PyTorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h3 id="Torch-或-Numpy"><a href="#Torch-或-Numpy" class="headerlink" title="Torch 或 Numpy"></a>Torch 或 Numpy</h3><h4 id="用-Numpy-还是-Torch"><a href="#用-Numpy-还是-Torch" class="headerlink" title="用 Numpy 还是 Torch"></a>用 Numpy 还是 Torch</h4><p>Torch 自称为神经网络界的 Numpy，因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU)，就像 Numpy 会把 array 放在 CPU 中加速运算。所以神经网络的话，当然是用 Torch 的 tensor 形式数据最好咯。就像 Tensorflow 当中的 tensor 一样。</p>
<p>当然，我们对 Numpy 还是爱不释手的，因为我们太习惯 numpy 的形式了。不过 torch 看出来我们的喜爱，他把 torch 做的和 numpy 能很好的兼容。比如这样就能自由地转换 numpy array 和 torch tensor 了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nnumpy array:&#x27;</span>, np_data,          <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch tensor:&#x27;</span>, torch_data,      <span class="comment">#  0  1  2 \n 3  4  5    [torch.LongTensor of size 2x3]</span></span><br><span class="line">    <span class="string">&#x27;\ntensor to array:&#x27;</span>, tensor2array, <span class="comment"># [[0 1 2], [3 4 5]]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h4 id="Torch-中的数学运算"><a href="#Torch-中的数学运算" class="headerlink" title="Torch 中的数学运算"></a>Torch 中的数学运算</h4><p>其实 torch 中 tensor 的运算和 numpy array 的如出一辙，我们就以对比的形式来看。如果想了解 torch 中其它更多有用的运算符，<a href="http://pytorch.org/docs/torch.html#math-operations">API就是你要去的地方</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># abs 绝对值计算</span></span><br><span class="line">data = [-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nabs&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.<span class="built_in">abs</span>(data),          <span class="comment"># [1 2 1 2]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.<span class="built_in">abs</span>(tensor)      <span class="comment"># [1 2 1 2]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sin   三角函数 sin</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nsin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.sin(data),      <span class="comment"># [-0.84147098 -0.90929743  0.84147098  0.90929743]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.sin(tensor)  <span class="comment"># [-0.8415 -0.9093  0.8415  0.9093]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean  均值</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmean&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.mean(data),         <span class="comment"># 0.0</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.mean(tensor)     <span class="comment"># 0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>除了简单的计算，矩阵运算才是神经网络中最重要的部分。所以我们展示下矩阵的乘法。注意一下包含了一个 numpy 中可行，但是 torch 中不可行的方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication 矩阵点乘</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)  <span class="comment"># 转换成32位浮点 tensor</span></span><br><span class="line"><span class="comment"># correct method</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmatrix multiplication (matmul)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, np.matmul(data, data),     <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, torch.mm(tensor, tensor)   <span class="comment"># [[7, 10], [15, 22]]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!  下面是错误的方法 !!!!</span></span><br><span class="line">data = np.array(data)</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">&#x27;\nmatrix multiplication (dot)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;\nnumpy: &#x27;</span>, data.dot(data),        <span class="comment"># [[7, 10], [15, 22]] 在numpy 中可行</span></span><br><span class="line">    <span class="string">&#x27;\ntorch: &#x27;</span>, tensor.dot(tensor)     <span class="comment"># torch 会转换成 [1,2,3,4].dot(&lt;a href=&quot;http://pytorch.org/docs/master/torch.html&quot; target=&#x27;_blank&#x27; &gt;1,2,3,4) = 30.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>新版本中(&gt;=0.3.0)，关于 <code>tensor.dot()</code> 有了新的改变，它只能针对于一维的数组。所以上面的有所改变。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor.dot(tensor)     <span class="comment"># torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 变为</span></span><br><span class="line">torch.dot(tensor.dot(tensor))</span><br></pre></td></tr></table></figure>



<h3 id="变量-Variable"><a href="#变量-Variable" class="headerlink" title="变量 (Variable)"></a>变量 (Variable)</h3><h4 id="什么是-Variable"><a href="#什么是-Variable" class="headerlink" title="什么是 Variable"></a>什么是 Variable</h4><p>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置。里面的值会不停的变化。就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那谁是里面的鸡蛋呢，自然就是 Torch 的 Tensor 咯。如果用一个 Variable 进行计算，那返回的也是一个同类型的 Variable。</p>
<p>我们定义一个 Variable：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="comment"># torch 中 Variable 模块</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先生鸡蛋</span></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="comment"># 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="Variable-计算-梯度"><a href="#Variable-计算-梯度" class="headerlink" title="Variable 计算, 梯度"></a>Variable 计算, 梯度</h4><p>我们再对比一下 tensor 的计算和 variable 的计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line"><span class="built_in">print</span>(t_out)</span><br><span class="line"><span class="built_in">print</span>(v_out)    <span class="comment"># 7.5</span></span><br></pre></td></tr></table></figure>

<p>到目前为止，我们看不出什么不同，<strong>但是时刻记住, Variable 计算时，它在背景幕布后面一步步默默地搭建着一个庞大的系统，叫做计算图，computational graph。这个图是用来干嘛的？原来是将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候，一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来，而 tensor 就没有这个能力啦。</strong></p>
<p><code>v_out = torch.mean(variable*variable)</code> 就是在计算图中添加的一个计算步骤，计算误差反向传递的时候有他一份功劳，我们就来举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v_out.backward()    <span class="comment"># 模拟 v_out 的误差反向传递</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面两步看不懂没关系, 只要知道 Variable 是计算图的一部分, 可以用来传递误差就好.</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable) 这是计算图中的 v_out 计算步骤</span></span><br><span class="line"><span class="comment"># 针对于 v_out 的梯度就是, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.grad)    <span class="comment"># 初始 Variable 的梯度</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>



<h4 id="获取-Variable-里面的数据"><a href="#获取-Variable-里面的数据" class="headerlink" title="获取 Variable 里面的数据"></a>获取 Variable 里面的数据</h4><p>直接<code>print(variable)</code>只会输出 Variable 形式的数据，在很多时候是用不了的(比如想要用 plt 画图)，所以我们要转换一下，将它变成 tensor 形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(variable)     <span class="comment">#  Variable 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.data)    <span class="comment"># tensor 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(variable.data.numpy())    <span class="comment"># numpy 形式</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="什么是激励函数-Activation-Function"><a href="#什么是激励函数-Activation-Function" class="headerlink" title="什么是激励函数 (Activation Function)"></a>什么是激励函数 (Activation Function)</h3><p>今天我们会来聊聊现代神经网络中必不可少的一个组成部分，激励函数，activation function。</p>
<h4 id="非线性方程"><a href="#非线性方程" class="headerlink" title="非线性方程"></a>非线性方程</h4><p>我们为什么要使用激励函数？用简单的语句来概括。就是因为，现实并没有我们想象的那么美好，它是残酷多变的。哈哈，开个玩笑，不过激励函数也就是为了解决我们日常生活中 不能用线性方程所概括的问题。好了，我知道你的问题来了。什么是线性方程 (linear function)？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/active1.png"></p>
<p>说到线性方程，我们不得不提到另外一种方程，非线性方程 (nonlinear function)。我们假设，女生长得越漂亮，越多男生爱。这就可以被当做一个线性问题。但是如果我们假设这个场景是发生在校园里。校园里的男生数是有限的，女生再漂亮，也不可能会有无穷多的男生喜欢她。所以这就变成了一个非线性问题。再说…女生也不可能是无穷漂亮的。这个问题我们以后有时间私下讨论。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/active2.png"></p>
<p>然后我们就可以来讨论如何在神经网络中达成我们描述非线性的任务了。我们可以把整个网络简化成这样一个式子。Y = Wx，W 就是我们要求的参数，y 是预测值，x 是输入值。用这个式子，我们很容易就能描述刚刚的那个线性问题，因为 W 求出来可以是一个固定的数。不过这似乎并不能让这条直线变得扭起来，激励函数见状，拔刀相助，站出来说道: “让我来掰弯它!”。</p>
<h4 id="激励函数"><a href="#激励函数" class="headerlink" title="激励函数"></a>激励函数</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/active3.png"></p>
<p>这里的 AF 就是指的激励函数。激励函数拿出自己最擅长的”掰弯利器”，套在了原函数上用力一扭，原来的 Wx 结果就被扭弯了。</p>
<p>其实这个 AF, 掰弯利器，也不是什么触不可及的东西。它其实就是另外一个非线性函数。比如说relu、sigmoid、tanh。将这些掰弯利器嵌套在原有的结果之上，强行把原有的线性结果给扭曲了。使得输出结果 y 也有了非线性的特征。举个例子，比如我使用了 relu 这个掰弯利器，如果此时 Wx 的结果是1，y 还将是1，不过 Wx 为-1的时候，y 不再是-1，而会是0。</p>
<p>你甚至可以创造自己的激励函数来处理自己的问题，不过要确保的是这些激励函数必须是可以微分的，因为在 backpropagation 误差反向传递的时候，只有这些可微分的激励函数才能把误差传递回去。</p>
<h4 id="常用选择"><a href="#常用选择" class="headerlink" title="常用选择"></a>常用选择</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/active4.png"></p>
<p>想要恰当使用这些激励函数，还是有窍门的。比如当你的神经网络层只有两三层，不是很多的时候，对于隐藏层，使用任意的激励函数，随便掰弯是可以的，不会有特别大的影响。不过，当你使用特别多层的神经网络，在掰弯的时候，玩玩不得随意选择利器。因为这会涉及到梯度爆炸，梯度消失的问题。因为时间的关系，我们可能会在以后来具体谈谈这个问题。</p>
<p>最后我们说说，在具体的例子中，我们默认首选的激励函数是哪些。在少量层结构中，我们可以尝试很多种不同的激励函数。在卷积神经网络 Convolutional neural networks 的卷积层中，推荐的激励函数是 relu。在循环神经网络中 recurrent neural networks，推荐的是 tanh 或者是 relu (这个具体怎么选, 我会在以后循环神经网络的介绍中在详细讲解)。</p>
<h3 id="激励函数-Activation"><a href="#激励函数-Activation" class="headerlink" title="激励函数 (Activation)"></a>激励函数 (Activation)</h3><h4 id="什么是-Activation"><a href="#什么是-Activation" class="headerlink" title="什么是 Activation"></a>什么是 Activation</h4><p>一句话概括 Activation：就是让神经网络可以描述非线性问题的步骤，是神经网络变得更强大。如果还不是特别了解，我有制作一个<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">动画短片</a>，浅显易懂的阐述了激励函数的作用。包懂。</p>
<h4 id="Torch-中的激励函数"><a href="#Torch-中的激励函数" class="headerlink" title="Torch 中的激励函数"></a>Torch 中的激励函数</h4><p>Torch 中的激励函数有很多，不过我们平时要用到的就这几个。<code>relu</code>，<code>sigmoid</code>，<code>tanh</code>，<code>softplus</code>。那我们就看看他们各自长什么样啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做一些假数据来观看图像</span></span><br><span class="line">x = torch.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">200</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">x = Variable(x)</span><br></pre></td></tr></table></figure>

<p>接着就是做生成不同的激励函数数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_np = x.data.numpy()   <span class="comment"># 换成 numpy array, 出图时用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 几种常用的 激励函数</span></span><br><span class="line">y_relu = F.relu(x).data.numpy()</span><br><span class="line">y_sigmoid = F.sigmoid(x).data.numpy()</span><br><span class="line">y_tanh = F.tanh(x).data.numpy()</span><br><span class="line">y_softplus = F.softplus(x).data.numpy()</span><br><span class="line"><span class="comment"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</span></span><br></pre></td></tr></table></figure>

<p>接着我们开始画图, 画图的代码也在下面：</p>
<p><img src="https://static.mofanpy.com/results/torch/2-3-1.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># python 的可视化模块, 我有教程 (https://mofanpy.com/tutorials/data-manipulation/plt/)</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>)</span><br><span class="line">plt.plot(x_np, y_relu, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">222</span>)</span><br><span class="line">plt.plot(x_np, y_sigmoid, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;sigmoid&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">0.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">223</span>)</span><br><span class="line">plt.plot(x_np, y_tanh, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">1.2</span>, <span class="number">1.2</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">224</span>)</span><br><span class="line">plt.plot(x_np, y_softplus, c=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;softplus&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">0.2</span>, <span class="number">6</span>))</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/tree/master/tutorial-contents">本章的全部代码</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
<li><a href="http://pytorch.org/docs/torch.html#math-operations">PyTorch 中的常用数学计算</a></li>
<li>Theano 激励函数 <a href="https://mofanpy.com/tutorials/machine-learning/theano/activation">教程</a></li>
<li>Tensorflow 激励函数 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/activation">教程</a></li>
<li>PyTorch 激励函数 <a href="https://mofanpy.com/tutorials/machine-learning/torch/activation">教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">我制作的 激励函数 动画简介</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>XCTF-Web基础篇</title>
    <url>/YingYingMonstre.github.io/2021/09/26/XCTF-Web%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
    <content><![CDATA[<h2 id="view-source"><a href="#view-source" class="headerlink" title="view_source"></a>view_source</h2><p>方法一：不能用右键审查元素，则按F12打开开发者工具查看，找到flag</p>
<p>方法二：在url中通过view-source:的方法来访问源码，在url中提交view-source:+url</p>
<p>方法三：通过Burpsuite抓包查看源代码</p>
<h2 id="robots"><a href="#robots" class="headerlink" title="robots"></a>robots</h2><p>[原理]</p>
<p>robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。</p>
<p>[步骤]</p>
<p>1.根据提示robots,可以直接想到robots.txt</p>
<p>2.或通过扫目录也可以扫到: <code>python dirsearch.py -u http://10.10.10.175:32793/ -e *</code>（这个脚本在cmd中可以用，git bash不行）</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/robots/1.png" alt="img"></p>
<p>3.访问<code>http://111.198.29.45:33982/robots.txt</code>发现<code>f1ag_1s_h3re.php</code></p>
<p>4.访问<code>http://111.198.29.45:33982/f1ag_1s_h3re.php</code>得到flag</p>
<h2 id="backup"><a href="#backup" class="headerlink" title="backup"></a>backup</h2><p><strong>[目标]</strong></p>
<p>掌握有关备份文件的知识</p>
<p>常见的备份文件后缀名有: <code>.git .svn .swp .svn .~ .bak .bash_history</code></p>
<p><strong>[环境]</strong></p>
<p>无</p>
<p><strong>[工具]</strong></p>
<p>扫目录脚本dirsearch(项目地址：<code>https://github.com/maurosoria/dirsearch</code>(<code>https://github.com/maurosoria/dirsearch</code>))</p>
<p><strong>[步骤]</strong></p>
<p>1.可以手动猜测,也可以使用扫目录脚本/软件,扫一下,这里使用的是github上的脚本dirsearch,命令行下: <code>py python3 dirsearch.py -u http://10.10.10.175:32770 -e *</code></p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/backup/1.png" alt="img"></p>
<p>2.看到存在备份文件index.php.bak访问 <code>http://10.10.10.175:32770/index.php.bak</code></p>
<p>3.保存到本地打开，即可看到flag</p>
<h2 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h2><p>[原理]</p>
<p> Cookie是当主机访问Web服务器时，由 Web 服务器创建的，将信息存储在用户计算机上的文件。一般网络用户习惯用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 Session 跟踪而存储在用户本地终端上的数据，而这些数据通常会经过加密处理。</p>
<p><strong>[目的]</strong></p>
<p>掌握有关cookie的知识，了解cookie所在位置</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.浏览器按下F12键打开开发者工具，刷新后，在存储一栏，可看到名为look-here的cookie的值为cookie.php</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/1.png" alt="img"></p>
<p>2.访问<code>http://111.198.29.45:47911/cookie.php</code>，提示查看http响应包，在网络一栏，可看到访问cookie.php的数据包</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/2.png" alt="img"></p>
<p>3.点击查看数据包，在消息头内可发现flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/cookie/3.png" alt="img"></p>
<p>用dirsearch可以找到cookie.php文件，再用burpsuite抓包也可行。</p>
<h2 id="disabled-button"><a href="#disabled-button" class="headerlink" title="disabled_button"></a>disabled_button</h2><p><strong>[目标]</strong></p>
<p>初步了解前端知识</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开开发者工具（得用火狐浏览器），在查看器窗口审查元素，发现存在disabled=””字段，</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/1.png" alt="img"></p>
<p>2.将<code>disabled=&quot;&quot;</code>删除后，按钮可按，按下后得到flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/2.png" alt="img"></p>
<p>3.或审计from表单代码，使用hackbar（不能用，现在收费了），用post方式传递auth=flag，同样可以获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/disabled_button/3.png" alt="img"></p>
<h2 id="weak-auth"><a href="#weak-auth" class="headerlink" title="weak_auth"></a>weak_auth</h2><p><strong>[目标]</strong></p>
<p>了解弱口令，掌握爆破方法</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<ul>
<li>burpsuite</li>
<li>字典<code>&lt;https://github.com/rootphantomer/Blasting_dictionary/blob/master/%E5%B8%B8%E7%94%A8%E5%AF%86%E7%A0%81.txt&gt;</code></li>
</ul>
<p><strong>[步骤]</strong></p>
<p>1.随便输入下用户名和密码,提示要用admin用户登入,然后跳转到了check.php,查看下源代码提示要用字典。</p>
<p>2.用burpsuite截下登录的数据包,把数据包发送到intruder爆破</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/1.png" alt="img"></p>
<p>2.设置爆破点为password</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/2.png" alt="img"></p>
<p>3.加载字典</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/3.png" alt="img"></p>
<p>4.开始攻击，查看响应包列表，发现密码为123456时，响应包的长度和别的不一样.</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/4.png" alt="img"></p>
<p>5.点进去查看响应包，获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/weak_auth/5.png" alt="img"></p>
<h2 id="simple-php"><a href="#simple-php" class="headerlink" title="simple_php"></a>simple_php</h2><p><strong>[原理]</strong></p>
<p>php中有两种比较符号</p>
<p>=== 会同时比较字符串的值和类型</p>
<p>== 会先将字符串换成相同类型（先类型转换），再作比较，属于弱类型比较</p>
<p><strong>[目地]</strong></p>
<p>掌握php的弱类型比较</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开页面，进行代码审计，发现同时满足 $a==0 和 $a 时，显示flag1。</p>
<p>2.php中的弱类型比较会使’abc’ == 0为真，所以输入a=abc时，可得到flag1，如图所示。（abc可换成任意字符）。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_php/1.png" alt="img"></p>
<p>3.is_numeric() 函数会判断如果是数字和数字字符串则返回 TRUE，否则返回 FALSE,且php中弱类型比较时，会使(‘1234a’ == 1234)为真，所以当输入a=abc&amp;b=1235a，可得到flag2，如图所示。==（数字和字符混合的字符串转换为整数后只保留数字）==</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_php/2.png" alt="img"></p>
<p> php类型比较表<a href="https://www.php.net/manual/zh/types.comparisons.php%E3%80%82">https://www.php.net/manual/zh/types.comparisons.php。</a></p>
<h2 id="get-post"><a href="#get-post" class="headerlink" title="get_post"></a>get_post</h2><p><strong>[原理]</strong></p>
<p>HTTP工作原理</p>
<p><strong>[目的]</strong></p>
<p>掌握常用http请求方式</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox和cmd</p>
<p><strong>[步骤]</strong></p>
<p>GET请求在url后加?a=1即可</p>
<p>POST请求需要用curl POST -d “b=2” <a href="http://111.200.241.244:65197/?a=1">http://111.200.241.244:65197/?a=1</a></p>
<p>返回如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl: (6) Could not resolve host: POST</span><br><span class="line">﻿&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang&#x3D;&quot;en&quot;&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;title&gt;POST&amp;GET&lt;&#x2F;title&gt;</span><br><span class="line">    &lt;link href&#x3D;&quot;http:&#x2F;&#x2F;libs.baidu.com&#x2F;bootstrap&#x2F;3.0.3&#x2F;css&#x2F;bootstrap.min.css&quot; rel&#x3D;&quot;stylesheet&quot; &#x2F;&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;请用GET方式提交一个名为a,值为1的变量&lt;&#x2F;h1&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;请再以POST方式随便提交一个名为b,值为2的变量&lt;&#x2F;h1&gt;&lt;h1&gt;cyberpeace&#123;51117d20e10dd8646f8d2eed10942e46&#125;&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>

<p>或者用burpsuite实现GET、POST的抓包：</p>
<p>GET的实现：在GET / HTTP/1.1中加入?a=1得到GET /?a=1 HTTP/1.1</p>
<p>POST的实现：GET / HTTP/1.1改为POST /?a=1 HTTP/1.1，在最下面（正文）加b=2，报文头加上Content-Type: application/x-www-form-urlencoded。</p>
<h2 id="xff-referer"><a href="#xff-referer" class="headerlink" title="xff_referer"></a>xff_referer</h2><p>[原理]</p>
<p>X-Forwarded-For:简称XFF头，它代表客户端，也就是HTTP的请求端真实的IP，只有在通过了HTTP 代理或者负载均衡服务器时才会添加该项</p>
<p>HTTP Referer是header的一部分，当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的</p>
<p><strong>[目的]</strong></p>
<p>掌握有关X-Forwarded-For和Referer的知识</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox、burpsuite</p>
<p><strong>[步骤]</strong></p>
<p>1.打开firefox和burp，使用burp对firefox进行代理拦截，在请求头添加<code>X-Forwarded-For: 123.123.123.123</code>，然后放包</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/xff_referer/1.png" alt="img"></p>
<p>2.接着继续在请求头内添加<code>Referer: https://www.google.com</code>，可获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/xff_referer/2.png" alt="img"></p>
<h2 id="webshell"><a href="#webshell" class="headerlink" title="webshell"></a>webshell</h2><p><strong>[目标]</strong></p>
<p>了解php一句话木马、如何使用webshell</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox、hackbar</p>
<p>蚁剑下载地址<code>https://github.com/AntSwordProject/antSword/releases</code>(<code>https://github.com/AntSwordProject/antSword/releases</code>)</p>
<p><strong>[步骤]</strong></p>
<p>1.直接提示给了php一句话，可以用菜刀或蚁剑连接,此处用蚁剑链接:</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/1.png" alt="img"></p>
<p>2.连接后在网站目录下发现了flag.txt文件，查看文件可获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/2.png" alt="img"></p>
<p>3.也可以使用hackbar，使用post方式传递shell=system(‘cat flag.txt’); 获得flag</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/webshell/3.png" alt="img"></p>
<h2 id="command-execution"><a href="#command-execution" class="headerlink" title="command_execution"></a>command_execution</h2><p><strong>[原理]</strong></p>
<p>| 的作用为将前一个命令的结果传递给后一个命令作为输入</p>
<p>&amp;&amp;的作用是前一条命令执行成功时，才执行后一条命令</p>
<p><strong>[目地]</strong></p>
<p>掌握命令拼接的方法</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开浏览器，在文本框内输入127.0.0.1 |  find / -name “flag.txt” （将 | 替换成 &amp; 或 &amp;&amp; 都可以）,查找flag所在位置，如图所示。</p>
<p>127.0.0.1 | ls ../../../可以找到一个home文件夹，里面有flag.txt文件。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/command_execution/1.png" alt="img"></p>
<p>2.在文本框内输入 127.0.0.1 |  cat /home/flag.txt 可得到flag，如图所示。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/command_execution/2.png" alt="img"></p>
<h2 id="simple-js"><a href="#simple-js" class="headerlink" title="simple_js"></a>simple_js</h2><p><strong>[原理]</strong></p>
<p>javascript的代码审计</p>
<p><strong>[目地]</strong></p>
<p>掌握简单的javascript函数</p>
<p><strong>[环境]</strong></p>
<p>windows</p>
<p><strong>[工具]</strong></p>
<p>firefox</p>
<p><strong>[步骤]</strong></p>
<p>1.打开页面，查看源代码，可以发现js代码，如图所示。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple_js/1.png" alt="img"></p>
<p>2.进行代码审计，发现不论输入什么都会跳到假密码，真密码位于 fromCharCode 。</p>
<p>3.先将字符串用python处理一下，得到数组[55,56,54,79,115,69,114,116,107,49,50]，exp如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s=<span class="string">&quot;\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30&quot;</span></span><br><span class="line"><span class="built_in">print</span> (s)</span><br></pre></td></tr></table></figure>

<p>4.将得到的数字分别进行ascii处理，可得到字符串786OsErtk12，exp如下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">a = [55,56,54,79,115,69,114,116,107,49,50]</span></span><br><span class="line"><span class="string">c = &quot;&quot;</span></span><br><span class="line"><span class="string">for i in a:</span></span><br><span class="line"><span class="string">    b = chr(i)</span></span><br><span class="line"><span class="string">    c = c + b</span></span><br><span class="line"><span class="string">print(c)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">string = <span class="string">&quot;\x35\x35\x2c\x35\x36\x2c\x35\x34\x2c\x37&quot;</span> \</span><br><span class="line">         <span class="string">&quot;\x39\x2c\x31\x31\x35\x2c\x36\x39\x2c\x31&quot;</span> \</span><br><span class="line">         <span class="string">&quot;\x31\x34\x2c\x31\x31\x36\x2c\x31\x30\x37\x2c\x34\x39\x2c\x35\x30&quot;</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">label = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> string:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="string">&quot;,&quot;</span>:</span><br><span class="line">        label += <span class="built_in">chr</span>(count)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        count = count * <span class="number">10</span> + <span class="built_in">int</span>(i)</span><br><span class="line"></span><br><span class="line">label += <span class="built_in">chr</span>(count)</span><br><span class="line"><span class="built_in">print</span>(label)</span><br></pre></td></tr></table></figure>

<p>5.规范flag格式，可得到Cyberpeace{786OsErtk12}</p>
<h2 id="baby-web"><a href="#baby-web" class="headerlink" title="baby_web"></a>baby_web</h2><p><strong>【实验原理】</strong></p>
<p>web请求头中的location作用</p>
<p><strong>【实验目的】</strong></p>
<p>掌握web响应包头部常见参数</p>
<p><strong>【实验环境】</strong></p>
<p>Windows</p>
<p><strong>【实验工具】</strong></p>
<p>firefox</p>
<p><strong>【实验步骤】</strong></p>
<p>1.根据提示，在url中输入index.php,发现打开的仍然还是1.php</p>
<p>2.打开火狐浏览器的开发者模式，选择网络模块，再次请求index.php,查看返回包，可以看到location参数被设置了1.php，并且得到flag。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/baby_web/1.png" alt="img"></p>
<p>我的：burpsuite抓包，看到有flag。</p>
]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>XCTF</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（简介）</title>
    <url>/YingYingMonstre.github.io/2021/10/31/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是强化学习"><a href="#什么是强化学习" class="headerlink" title="什么是强化学习"></a>什么是强化学习</h3><h4 id="从无到有"><a href="#从无到有" class="headerlink" title="从无到有"></a>从无到有</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL1.png"></p>
<p>强化学习是一类算法，是让计算机实现从一开始什么都不懂，脑袋里没有一点想法，通过不断地尝试，从错误中学习，最后找到规律，学会了达到目的的方法。这就是一个完整的强化学习过程。实际中的强化学习例子有很多。比如近期最有名的 Alpha go，机器头一次在围棋场上战胜人类高手；让计算机自己学着玩经典游戏 Atari，这些都是让计算机在不断的尝试中更新自己的行为准则，从而一步步学会如何下好围棋、如何操控游戏得到高分。既然要让计算机自己学，那计算机通过什么来学习呢？</p>
<h4 id="虚拟老师"><a href="#虚拟老师" class="headerlink" title="虚拟老师"></a>虚拟老师</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL2.png"></p>
<p>原来计算机也需要一位虚拟的老师，这个老师比较吝啬，他不会告诉你如何移动、如何做决定，他为你做的事只有给你的行为打分，那我们应该以什么形式学习这些现有的资源，或者说怎么样只从分数中学习到我应该怎样做决定呢？很简单，我只需要记住那些高分，低分对应的行为，下次用同样的行为拿高分，并避免低分的行为。</p>
<p>比如老师会根据我的开心程度来打分——我开心时，可以得到高分，我不开心时得到低分。有了这些被打分的经验，我就能判断为了拿到高分，我应该选择一张开心的脸， 避免选到伤心的脸。这也是强化学习的核心思想。可以看出在强化学习中，一种行为的分数是十分重要的。所以强化学习具有分数导向性。我们换一个角度来思考。这种分数导向性好比我们在监督学习中的正确标签。</p>
<h4 id="对比监督学习"><a href="#对比监督学习" class="headerlink" title="对比监督学习"></a>对比监督学习</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL3.png"></p>
<p>我们知道监督学习，是已经有了数据和数据对应的正确标签，比如这样。监督学习就能学习出那些脸对应哪种标签。不过强化学习还要更进一步，一开始它并没有数据和标签。</p>
<p>他要通过一次次在环境中的尝试，获取这些数据和标签，然后再学习通过哪些数据能够对应哪些标签，通过学习到的这些规律，尽可能地选择带来高分的行为 (比如这里的开心脸)。这也就证明了在强化学习中，分数标签就是他的老师，他和监督学习中的老师也差不多。</p>
<h4 id="RL-算法们"><a href="#RL-算法们" class="headerlink" title="RL 算法们"></a>RL 算法们</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RL4.png"></p>
<p>强化学习是一个大家族，他包含了很多种算法，我们也会一一提到之中一些比较有名的算法，比如有通过行为的价值来选取特定行为的方法，包括使用表格学习的Q learning、Sarsa，使用神经网络学习的 Deep Q Network，还有直接输出行为的 Policy Gradients，又或者了解所处的环境，想象出一个虚拟的环境并从虚拟的环境中学习 等等。</p>
<h3 id="强化学习方法汇总"><a href="#强化学习方法汇总" class="headerlink" title="强化学习方法汇总"></a>强化学习方法汇总</h3><p>了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法。比如说比较知名的控制方法 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>，<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a>，还有基于对环境的理解的 model-based RL 等等。接下来我们通过分类的方式来了解他们的区别。</p>
<h4 id="Model-free-和-Model-based"><a href="#Model-free-和-Model-based" class="headerlink" title="Model-free 和 Model-based"></a>Model-free 和 Model-based</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd1.png"></p>
<p>我们可以将所有强化学习的方法分为理不理解所处环境，如果我们不尝试去理解环境，环境给了我们什么就是什么。我们就把这种方法叫做 model-free，这里的 model 就是用模型来表示环境，那理解了环境也就是学会了用一个模型来代表环境，所以这种就是 model-based 方法。我们想象：现在环境就是我们的世界，我们的机器人正在这个世界里玩耍，他不理解这个世界是怎样构成的，也不理解世界对于他的行为会怎么样反馈。举个例子，他决定丢颗原子弹去真实的世界，结果把自己给炸死了，所有结果都是那么现实。不过如果采取的是 model-based RL，机器人会通过过往的经验，先理解真实世界是怎样的，并建立一个模型来模拟现实世界的反馈，最后他不仅可以在现实世界中玩耍，也能在模拟的世界中玩耍，这样就没必要去炸真实世界，连自己也炸死了，他可以像玩游戏一样炸炸游戏里的世界，也保住了自己的小命。那我们就来说说这两种方式的强化学习各用那些方法吧。</p>
<p>Model-free 的方法有很多，像 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">Sarsa</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a> 都是从环境中得到反馈然后从中学习。而 model-based RL 只是多了一道程序——为真实世界建模，也可以说他们都是 model-free 的强化学习，只是 model-based 多出了一个虚拟环境，我们不仅可以像 model-free 那样在现实中玩耍，还能在游戏中玩耍，而玩耍的方式也都是 model-free 中那些玩耍方式，最终 model-based 还有一个杀手锏是 model-free 超级羡慕的，那就是想象力。</p>
<p>Model-free 中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而 model-based，他能通过想象来预判断接下来将要发生的所有情况，然后选择这些想象情况中最好的那种。并依据这种情况来采取下一步的策略，这也就是围棋场上 AlphaGo 能够超越人类的原因。接下来，我们再来用另外一种分类方法将强化学习分为基于概率和基于价值。</p>
<h4 id="基于概率-和-基于价值"><a href="#基于概率-和-基于价值" class="headerlink" title="基于概率 和 基于价值"></a>基于概率 和 基于价值</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd2.png"></p>
<p>基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，我们会根据最高价值来选着动作，相比基于概率的方法，基于价值的决策部分更为铁定、毫不留情，就选价值最高的；而基于概率的，即使某个动作的概率最高，但是还是不一定会选到他。</p>
<p>我们现在说的动作都是一个一个不连续的动作，而对于选取连续的动作，基于价值的方法是无能为力的。我们却能用一个概率分布在连续动作中选取特定动作，这也是基于概率的方法的优点之一。那么这两类使用的方法又有哪些呢?</p>
<p>比如在基于概率这边，有 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-PG">Policy Gradients</a>，在基于价值这边有 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a>、<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">Sarsa</a> 等。而且我们还能结合这两类方法的优势之处，创造更牛逼的一种方法，叫做 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-AC">Actor-Critic</a>，actor 会基于概率做出动作，而 critic 会对做出的动作给出动作的价值，这样就在原有的 policy gradients 上加速了学习过程。</p>
<h4 id="回合更新-和-单步更新"><a href="#回合更新-和-单步更新" class="headerlink" title="回合更新 和 单步更新"></a>回合更新 和 单步更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd3.png"></p>
<p>强化学习还能用另外一种方式分类，回合更新和单步更新，想象强化学习就是在玩游戏，游戏回合有开始和结束。回合更新指的是游戏开始后，我们要等待游戏结束，然后再总结这一回合中的所有转折点，再更新我们的行为准则。而单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束，这样我们就能边玩边学习了。</p>
<p>再来说说方法，Monte-carlo learning 和基础版的 policy gradients 等 都是回合更新制，Q learning、Sarsa、升级版的 policy gradients 等都是单步更新制。因为单步更新更有效率，所以现在大多方法都是基于单步更新。比如有的强化学习问题并不属于回合问题。</p>
<h4 id="在线学习-和-离线学习"><a href="#在线学习-和-离线学习" class="headerlink" title="在线学习 和 离线学习"></a>在线学习 和 离线学习</h4><p><img src="https://static.mofanpy.com/results/ML-intro/RLmtd4.png"></p>
<p>这个视频的最后一种分类方式是 在线学习和离线学习。所谓在线学习，就是指我必须本人在场，并且一定是本人边玩边学习；而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则，离线学习同样是从过往的经验中学习，但是这些过往的经历没必要是自己的经历，任何人的经历都能被学习。或者我也不必要边玩边学习，我可以白天先存储下来玩耍时的记忆，然后晚上通过离线学习来学习白天的记忆。那么每种学习的方法又有哪些呢?</p>
<p>最典型的在线学习就是 Sarsa 了，还有一种优化 Sarsa 的算法，叫做 Sarsa lambda，最典型的离线学习就是 Q learning，后来人也根据离线学习的属性，开发了更强大的算法，比如让计算机学会玩电动的 Deep-Q-Network。</p>
<p>这就是我们从各种不同的角度来对比了强化学习中的多种算法。</p>
<h3 id="为什么用强化学习-Why"><a href="#为什么用强化学习-Why" class="headerlink" title="为什么用强化学习 Why?"></a>为什么用强化学习 Why?</h3><h4 id="强化学习介绍"><a href="#强化学习介绍" class="headerlink" title="强化学习介绍"></a>强化学习介绍</h4><p>强化学习 (Reinforcement Learning) 是一个机器学习大家族中的分支，由于近些年来的技术突破，和深度学习 (Deep Learning) 的整合，使得强化学习有了进一步的运用。比如让计算机学着玩游戏，AlphaGo 挑战世界围棋高手，都是强化学习在行的事。强化学习也是让你的程序从对当前环境完全陌生，成长为一个在环境中游刃有余的高手。</p>
<p>这些教程的教学，不依赖于任何强化学习的 python 模块。因为强化学习的复杂性、多样性，到现在还没有比较好的统一化模块。不过我们还是能用最基础的方法编出优秀的强化学习程序!</p>
<h4 id="模拟程序提前看"><a href="#模拟程序提前看" class="headerlink" title="模拟程序提前看"></a>模拟程序提前看</h4><p>以下是我们将要在后续的课程中实现的牛逼的自学程序。</p>
<p>Youtube 的模拟视频都在这里:</p>
<p><a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O">https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O</a>_.</p>
<p>优酷的模拟视频在这里:</p>
<p><a href="http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1">http://list.youku.com/albumlist/show?id=27485743&amp;ascending=1&amp;page=1</a></p>
<p>下面是其中一些模拟视频:</p>
<ul>
<li>Maze</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa_lambda.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>


<ul>
<li>Cartpole</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/cartpole%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>


<ul>
<li>Mountain car</li>
</ul>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h3 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h3><h4 id="教程必备模块"><a href="#教程必备模块" class="headerlink" title="教程必备模块"></a>教程必备模块</h4><p>强化学习有一些现成的模块可以使用，但是那些模块并不全面，而且强化学习很依赖与你给予的学习环境。对于不同学习环境的强化学习，可能 RL 的代码就不同。所以我们要抱着以不变应万变的心态，用基础的模块，从基础学起。懂了原理，再复杂的环境也不在话下。</p>
<p>所以用到的模块和对应的教程:</p>
<ul>
<li><a href="https://mofanpy.com/tutorials/data-manipulation/np-pd/">Numpy, Pandas</a> (必学), 用于学习的数据处理</li>
<li><a href="https://mofanpy.com/tutorials/data-manipulation/plt/">Matplotlib</a> (可学), 偶尔会用来呈现误差曲线什么的</li>
<li><a href="https://mofanpy.com/tutorials/python-basic/tkinter/">Tkinter</a> (可学), 你可以自己用它来编写模拟环境</li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow</a> (可学), 后面实现神经网络与强化学习结合的时候用到</li>
<li><a href="https://gym.openai.com/">OpenAI gym</a> (可学), 提供了很多现成的模拟环境</li>
</ul>
<h4 id="快速了解强化学习"><a href="#快速了解强化学习" class="headerlink" title="快速了解强化学习"></a>快速了解强化学习</h4><p>我也会制作每种强化学习对应的简介视频 (在这个学习列表里: <a href="https://mofanpy.com/tutorials/machine-learning/ML-intro/">有趣的机器学习</a>)，大家可以只花很少的时间来观看了解这些学习方法的不同之处。有了一定概念和基础，我们在这套教材里实现起来就容易多了。而且不懂的时候也能只花很少的时间回顾就行。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch简介</title>
    <url>/YingYingMonstre.github.io/2021/12/06/PyTorch%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h3 id="科普-人工神经网络-VS-生物神经网络"><a href="#科普-人工神经网络-VS-生物神经网络" class="headerlink" title="科普: 人工神经网络 VS 生物神经网络"></a>科普: 人工神经网络 VS 生物神经网络</h3><h4 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann1.png"></p>
<p>2-30年前，一想到神经网络，我们就会想到生物神经系统中数以万计的细胞联结，将感官和反射器联系在一起的系统。但是今天，你可能的第一反应却是…电脑和电脑程序当中的人工神经网络。昔日复杂的动神经网络系统居然神奇地放入了计算机？而且人类正在将这种人工神经网络系统推向更高的境界。今天的世界早已布满了人工神经网络的身影。</p>
<p>比如 Google 的搜索引擎。股票价格预测、机器人学习、围棋、家庭助手，等等等等。从金融到仿生样样都能运用。看起来人工神经网络的确很强大。但，是不是有这么一个问题一直在你脑海中环绕，没有答案。“计算机领域的神经网络和我们自己身体里的神经网络究竟是一样的吗?” 科学家们通过长久的探索，想让计算机像人一样思考，所以研发了人工神经网络，究竟和我们的神经网络有多像？那我们就先来看看人的神经网络到底是什么。</p>
<h4 id="你的神经网络系统"><a href="#你的神经网络系统" class="headerlink" title="你的神经网络系统"></a>你的神经网络系统</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann2.png"></p>
<p>9百亿神经细胞组成了我们复杂的神经网络系统，这个数量甚至可以和宇宙中的星球数相比较。如果仅仅靠单个的神经元，是永远没有办法让我们像今天一样，完成各种任务，处理各种复杂的问题。那我们是如何靠这些神经元来解决问题的呢？首先需要知道的是我们的记忆是如何产生的。想象我们还是婴儿，</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/ann3.png"></p>
<p>包着尿布的我们什么都不知道，神经元并没有形成系统和网络。可能只是一些分散的细胞而已，一端连着嘴巴的味觉感受器，一端连着手部的肌肉。小时候，世界上有一种神奇的东西叫做——糖果，当我们第一次品尝它的时候，美妙的感觉，让我们发现活着是多么有意义的事情。这时候神经元开始产生联结，记忆形成，但是形成的新联结怎么样变成记忆，仍然是科学界的一个迷。不过现在，我们的手和嘴产生了某种特定的搭配。每次发现有糖果的时候，某种生物信号就会从我们的嘴，通过之前形成的神经联结，传递到手上，让手的动作变得有意义，比如这样，然后爸妈就会再给我们一颗糖果啦~ 哈哈，吃糖的目的达成。现在我们来看看人工神经网络要怎样达到这个目的。</p>
<h4 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann4.png"></p>
<p>首先，替代掉生物神经网络的，就是已经成体系的人工神经网络。所有神经元之间的连接都是固定不可更换的，这也就是说，在人工神经网络里，没有凭空产生新联结这回事。人工神经网络典型的一种学习方式就是，我已经知道吃到糖果时，手会如何动，但是我想让神经网络学着帮我做这件动动手的事情。所以我预先准备好非常多吃糖的学习数据，然后将这些数据一次次放入这套人工神经网络系统中，糖的信号会通过这套系统传递到手。然后通过对比这次信号传递后，手的动作是不是”讨糖”动作，来修改人工神经网络当中的神经元强度。这种修改在专业术语中叫做”误差反向传递”，也可以看作是再一次将传过来的信号传回去，看看这个负责传递信号神经元对于”讨糖”的动作到底有没有贡献，让它好好反思与改正，争取下次做出更好的贡献。这样看来，人工神经网络和生物神经网络的确不是一回事。</p>
<h4 id="两者区别总结"><a href="#两者区别总结" class="headerlink" title="两者区别总结"></a>两者区别总结</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/ann5.png"></p>
<p>人工神经网络靠的是正向和反向传播来更新神经元，从而形成一个好的神经系统，本质上，这是一个能让计算机处理和优化的数学模型。而生物神经网络是通过刺激，产生新的联结，让信号能够通过新的联结传递而形成反馈。虽然现在的计算机技术越来越高超，不过我们身体里的神经系统经过了数千万年的进化，还是独一无二的，迄今为止，再复杂、再庞大的人工神经网络系统也不能替代我们的小脑袋。我们应该感到自豪，也应该珍惜上天的这份礼物。</p>
<h3 id="什么是神经网络-Neural-Network"><a href="#什么是神经网络-Neural-Network" class="headerlink" title="什么是神经网络 (Neural Network)"></a>什么是神经网络 (Neural Network)</h3><h4 id="内容简介"><a href="#内容简介" class="headerlink" title="内容简介"></a>内容简介</h4><p>这里提到的是人工神经网络，是存在于计算机里的神经系统。人工神经网络和自然神经网络的区别、神经网络是什么、它是怎么工作的、都会在影片里一一提到。</p>
<h3 id="神经网络-梯度下降"><a href="#神经网络-梯度下降" class="headerlink" title="神经网络 梯度下降"></a>神经网络 梯度下降</h3><p>欢迎观看有趣的机器学习系列视频，神经网络是当今为止最流行的一种深度学习框架，它的基本原理也很简单，就是一种梯度下降机制。我们今天就来看看这神奇的优化模式吧。</p>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p><img src="https://static.mofanpy.com/results/ML-intro/gd2.png"></p>
<p>学习机器学习的同学们常会遇到这样的图像，我了个天，看上去好复杂，哈哈，不过还挺好看的。这些和我们说的梯度下降又有什么关系呢？原来这些图片展示出来了一个家族的历史，这个家族的名字就是——”optimization” (优化问题)。优化能力是人类历史上的重大突破，他解决了很多实际生活中的问题。从而渐渐演化成了一个庞大的家族。</p>
<p>比如说牛顿法 (Newton’s method)、最小二乘法(Least Squares method)、梯度下降法 (Gradient Descent) 等等。而我们的神经网络就是属于梯度下降法这个分支中的一个。提到梯度下降，我们不得不说说大学里面学习过的求导求微分。因为这就是传说中”梯度下降”里面的”梯度” (gradient)啦。听到求导微分可别后怕，因为这个短视频只是让你有一个直观上的理解，并不会涉及太过复杂的东西。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gd3.png"></p>
<p>初学神经网络的时候，我们通常会遇到这样一个方程，叫做误差方程 (Cost Function)。用来计算预测出来的和我们实际中的值有多大差别。在预测数值的问题中，我们常用平方差 (Mean Squared Error) 来代替。我们简化一下这个方程，W是我们神经网络中的参数，x、y 都是我们的数据，因为 xy 都是实实在在的数据点，在这个假设情况中，是多少都无所谓，然后我们再厚颜无耻地像这样继续简化一下，(注意, 这个过程在在数学中并不正确, 不过我们只是为了看效果)，所以现在误差值曲线就成了这样。假设我们初始化的 W 在这个位置。而这个位置的斜率是这条线，这也就是梯度下降中的梯度啦。我们从图中可以看出，Cost 误差最小的时候正是这条 cost 曲线最低的地方，不过在蓝点的 W 却不知道这件事情，他目前所知道的就是梯度线为自己在这个位置指出的一个下降方向，我们就要朝着这个蓝色梯度的方向下降一点点。在做一条切线，发现我还能下降，那我就朝着梯度的方向继续下降，这时，再展示出现在的梯度，因为梯度线已经躺平了，我们已经指不出哪边是下降的方向了，所以这时我们就找到了 W 参数的最理想值。简而言之，就是找到梯度线躺平的点。可是神经网络的梯度下降可没这么简单。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/gd4.png"></p>
<p>神经网络中的 W 可不止一个，如果只有一个 W，我们就能画出之前那样的误差曲线，如果有两个 W 也简单，我们可以用一个3D 图来展示，可是超过3个 W，我们可就没办法很好的可视化出来啦。这可不是最要命的。在通常的神经网络中，误差曲线可没这么优雅。</p>
<h4 id="全局-and-局部最优"><a href="#全局-and-局部最优" class="headerlink" title="全局 and 局部最优"></a>全局 and 局部最优</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gd5.png"></p>
<p>在简化版的误差曲线中，我们只要找到梯度线躺平的地方，就能能迅速找到误差最小时的 W。可是很多情况是这样的，误差曲线并不只有一个沟，而且梯度躺平的点也不止一个。不同的 W 初始化的位置，将会带来不同的下降区域。不同的下降区域，又会带来不同的 W 解。在这个图像当中，W 的全局最优解(Global minima)在这个位置，而其它的解都是局部最优(Local minima)。全局最优固然是最好，但是很多时候，你手中的都是一个局部最优解，这也是无可避免的。不过你可以不必担心，因为虽然不是全局最优，但是神经网络也能让你的局部最优足够优秀，以至于即使拿着一个局部最优也能出色的完成手中的任务。</p>
<h3 id="科普-神经网络的黑盒不黑"><a href="#科普-神经网络的黑盒不黑" class="headerlink" title="科普: 神经网络的黑盒不黑"></a>科普: 神经网络的黑盒不黑</h3><p>今天我们来说说为了理解神经网络在做什么，对神经网络这个黑盒的正确打开方式。</p>
<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation1.png"></p>
<p>当然，这可不是人类的神经网络，因为至今我们都还没彻底弄懂人类复杂神经网络的运行方式。今天只来说说计算机中的人工神经网络。我们都听说过，神经网络是一个黑盒。</p>
<h4 id="黑盒"><a href="#黑盒" class="headerlink" title="黑盒"></a>黑盒</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation2.png"></p>
<p>呀，咋一看，的确挺黑的。我们还知道，如果你丢一个东西进这个黑盒，他会给你丢出来另一个东西。具体在黑盒里偷偷摸摸做了什么，我们不得而知。但丢出来的东西和丢进去的东西有着某些联系。这是为什么呢？这个黑盒里究竟又发生了什么呢？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation3.png"></p>
<p>正好我手边有一个手电筒，我们打开黑盒好好照亮看看。一般来说，神经网络是一连串神经层所组成的把输入进行加工再输出的系统。中间的加工过程就是我们所谓的黑盒。想把黑盒打开，就是把神经网络给拆开。按正常的逻辑，我们能将神经网络分成三部分。</p>
<h4 id="神经网络分区"><a href="#神经网络分区" class="headerlink" title="神经网络分区"></a>神经网络分区</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation4.png"></p>
<p>输入端，黑盒，输出端。输入端是我们能理解的物体，一个宝宝，输出端也是一个我们能理解的物体，一个奶瓶。对于神经网络，传统的理解就是，中间的这两层神经层在对输入信息进行加工，好让自己的输出信息和奶瓶吻合。但是我们如果换一个角度来想想。此时，我们将左边的红线移动一下</p>
<p>现在的输入端增加了一层，原本我们认定为黑盒的一部分被照亮，变成了一个已知部分。我们将最左边两层的神经层共同看成输入端。貌似怪怪的，你可能会问：可是这时的输入端不再是我们知道的”宝宝”了呀，为什么可以这样看？想得没错，它的确已经不是我们认识的宝宝啦，但是”宝宝”这个人类定义的形象通过了一层神经网络加工，变成了另外一种宝宝的形象，可能这种形象我们用肉眼看起来并不像宝宝，不过计算机却能理解，这是它所能识别的”宝宝”形象。在专业术语中，我们将宝宝当做特征(features)，将神经网络第一层加工后的宝宝叫做代表特征(feature representation)。如果再次移动红线，我们的黑盒就消失了，这时原本在黑盒里的所有神经层都被照亮。原本的代表特征再次被加工，变成了另一种代表特征，同样，再次加工形成的代表特征通常只有计算机自己看得懂，能够理解。所以，与其说黑盒是在加工处理，还不如说是在将一种代表特征转换成另一种代表特征，一次次特征之间的转换，也就是一次次的更有深度的理解。比如神经网络如果接收人类手写数字的图片。</p>
<h4 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation5.png"></p>
<p>然后我们将这个神经网络的输出层给拆掉，只留下前三层，那第3层输出的信息就是我们这些数字的3个最重要的代表特征，换句话说，就是用3个信息来代表整张手写数字图片的所有像素点。我们如果把这3个信息展示出来，我们就能很清楚的看到，计算机是如何用3个点来代表不同的数字内容，比如神经网络认为 1 和 0 是完全不同的，所以他们应该被放在空间里不同的地方。输出层就更好理解了，</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation6.png"></p>
<p>有了用3个点表示的数字代表特征，我们就能整理整理，将落在相同区域的数字分为一类，如果落在了那些1所在的区域，我们就认定张手写图片就是1，如果是2的区域，就认定为2。这就是神经网络的黑盒并不黑的原因啦，只是因为有时候代表特征太多了，我们人类没有办法看懂他们代表的是什么，然而计算机却能看清楚它所学到的规律，所以我们才觉得神经网络就是个黑盒。这种代表特征的理解方式其实非常有用，以至于人们拿着它来研究更高级的神经网络玩法。比如迁移学习(Transfer Learning)。我们举一个例子。</p>
<h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/feature_representation7.png"></p>
<p>对于一个有分类能力的神经网络，有时候我们只需要这套神经网络的理解能力，并拿这种能力去处理其他问题。所以我们保留它的代表特征转换能力。因为有了这种能力，就能将复杂的图片像素信息转换成更少量，但更精辟的信息，比如刚刚我们说将手写数字变成的3个点信息。然后我们需要干点坏事，将这个神经网络的输出层给拆掉。套上另外一个神经网络，用这种移植的方式再进行训练，让它处理不同的问题，比如，预测照片里事物的价值。现在看来，这黑盒里开个灯，其实还挺有用的嘛。当你看不懂神经网络的时候，这样想想，是不是更好理解啦。</p>
<h3 id="Why-Pytorch"><a href="#Why-Pytorch" class="headerlink" title="Why Pytorch?"></a>Why Pytorch?</h3><h4 id="为什么用-PyTorch"><a href="#为什么用-PyTorch" class="headerlink" title="为什么用 PyTorch"></a>为什么用 PyTorch</h4><p><a href="http://pytorch.org/">PyTorch</a> 是 <a href="http://pytorch.org/">PyTorch</a> 在 Python 上的衍生。因为 <a href="http://pytorch.org/">PyTorch</a> 是一个使用 <a href="http://pytorch.org/">PyTorch</a> 语言的神经网络库，Torch 很好用，但是 Lua 又不是特别流行，所有开发团队将 Lua 的 Torch 移植到了更流行的语言 Python 上。是的 PyTorch 一出生就引来了剧烈的反响。为什么呢？</p>
<p>很简单，我们就看看有谁在用 PyTorch 吧。</p>
<p><img src="https://static.mofanpy.com/results-small/torch/1-1-1.png"></p>
<p>可见，著名的 Facebook、twitter 等都在使用它，这就说明 PyTorch 的确是好用的，而且是值得推广。</p>
<p>而且如果你知道 <a href="http://www.numpy.org/">Numpy</a>，PyTorch 说他就是在神经网络领域可以用来替换 numpy 的模块。</p>
<h4 id="神经网络在做什么"><a href="#神经网络在做什么" class="headerlink" title="神经网络在做什么"></a>神经网络在做什么</h4><p>神经网络在学习拟合线条(回归)：</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-2.gif"></p>
<p>神经网络在学习区分数据(分类)：</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-3.gif"></p>
<h4 id="PyTorch-和-Tensorflow"><a href="#PyTorch-和-Tensorflow" class="headerlink" title="PyTorch 和 Tensorflow"></a>PyTorch 和 Tensorflow</h4><p>据 PyTorch 自己介绍，他们家的最大优点就是建立的神经网络是动态的，对比静态的 Tensorflow，他能更有效地处理一些问题，比如说 RNN 变化时间长度的输出。而我认为，各家有各家的优势和劣势，所以我们要以中立的态度。两者都是大公司，Tensorflow 自己说自己在分布式训练上下了很大的功夫，那我就默认 Tensorflow 在这一点上要超出 PyTorch，但是 Tensorflow 的静态计算图使得他在 RNN 上有一点点被动 (虽然它用其他途径解决了)，不过用 PyTorch 的时候，你会对这种动态的 RNN 有更好的理解。</p>
<p>而且 Tensorflow 的高度工业化，它的底层代码…你是看不懂的。PyTorch 好那么一点点，如果你深入 API，你至少能比看 Tensorflow 多看懂一点点 PyTorch 的底层在干嘛。</p>
<p>最后我的建议就是：</p>
<ul>
<li>如果你是学生，随便选一个学，或者稍稍偏向 PyTorch，因为写代码的时候应该更好理解。懂了一个模块，转换 Tensorflow 或者其他的模块都好说。</li>
<li>如果是上班了，跟着你公司来，公司用什么，你就用什么，不要脱群。</li>
</ul>
<h4 id="Pytorch-安装"><a href="#Pytorch-安装" class="headerlink" title="Pytorch 安装"></a>Pytorch 安装</h4><p>对于MacBook，安装推荐参考：<a href="https://zhuanlan.zhihu.com/p/410961551">Mac M1安装tensorflow和pytorch</a></p>
<h4 id="支持的系统"><a href="#支持的系统" class="headerlink" title="支持的系统"></a>支持的系统</h4><p>PyTorch 暂时只支持 MacOS、Linux。暂不支持 Windows！(可怜的 Windows 同学们.. 又被抛弃了)。不过说不定像 Tensorflow 一样，因为 Windows 用户的强烈要求，他们在某天就突然支持了。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>PyTorch 安装起来很简单，<a href="http://pytorch.org/">它自家网页</a>上就有很方便的选择方式 (网页升级改版后可能和下图有点不同)：</p>
<p><img src="https://static.mofanpy.com/results-small/torch/1-2-1.png"></p>
<p>所以根据你的情况选择适合你的安装方法，我已自己为例，我使用的是 MacOS，想用 pip 安装，我的 Python 是 3.5 版的，我没有 GPU 加速，那我就按上面的选：</p>
<p>然后根据上面的提示，我只需要在我的 Terminal 当中输入以下指令就好了：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install http://download.pytorch.org/whl/torch-0.1.11.post5-cp35-cp35m-macosx_10_7_x86_64.whl</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install torchvision</span></span><br></pre></td></tr></table></figure>

<p>注意，我安装的是0.1.11版本的 torch，你需要去他们网站上看是否有新版本的。安装 PyTorch 会安装两个模块，一个是 torch，一个 torchvision。torch 是主模块，用来搭建神经网络的，torchvision 是辅模块，有数据库，还有一些已经训练好的神经网络等着你直接用，比如 (<a href="http://pytorch.org/docs/torchvision/models.html">VGG, AlexNet, ResNet</a>)。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li>Python 做神经网络 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow 教程</a></li>
<li>Python 做神经网络 <a href="https://mofanpy.com/tutorials/machine-learning/torch/">Pytorch 教程</a></li>
<li>有网友根据我的 Tensorflow 系列做了一个很好的<a href="http://www.jianshu.com/p/e112012a4b2d">文字笔记</a>, 推荐阅读.</li>
<li>Tensorflow <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">学习目录</a></li>
<li>PyTorch <a href="https://mofanpy.com/tutorials/machine-learning/torch/">学习目录</a></li>
<li>Theano <a href="https://mofanpy.com/tutorials/machine-learning/theano/">学习目录</a></li>
<li>Keras <a href="https://mofanpy.com/tutorials/machine-learning/keras/">学习目录</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-NN">什么是神经网络 短视频</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch高阶内容</title>
    <url>/YingYingMonstre.github.io/2021/12/12/PyTorch%E9%AB%98%E9%98%B6%E5%86%85%E5%AE%B9/</url>
    <content><![CDATA[<h3 id="为什么-Torch-是动态的"><a href="#为什么-Torch-是动态的" class="headerlink" title="为什么 Torch 是动态的"></a>为什么 Torch 是动态的</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>听说过 Torch 的人都听说了 torch 是动态的，那他的动态到底是什么呢？我们用一个 RNN 的例子来展示一下动态计算到底长什么样。</p>
<h4 id="动态-静态"><a href="#动态-静态" class="headerlink" title="动态?静态?"></a>动态?静态?</h4><p>对比静态动态，我们就得知道谁是静态的。在流行的神经网络模块中，Tensorflow 就是最典型的静态计算模块。下图是一种我在<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习教程</a>中的 Tensorflow 计算图。也就是说，大部分时候，用 Tensorflow 是先搭建好这样一个计算系统，一旦搭建好了，就不能改动了 (也有例外, 比如<code>dynamic_rnn()</code>，但是总体来说他还是运用了一个静态思维)，所有的计算都会在这种图中流动，当然很多情况，这样就够了，我们不需要改动什么结构。不动结构当然可以提高效率。但是一旦计算流程不是静态的，计算图要变动。最典型的例子就是 RNN，有时候 RNN 的 time step 不会一样，或者在 training 和 testing 的时候，<code>batch_size</code> 和 <code>time_step</code> 也不一样，这时，Tensorflow 就头疼了，Tensorflow 的人也头疼了。哈哈，如果用一个动态计算图的 Torch，我们就好理解多了，写起来也简单多了。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/6-2-2.png"></p>
<h4 id="动态RNN"><a href="#动态RNN" class="headerlink" title="动态RNN"></a>动态RNN</h4><p>我们拿 <a href="https://mofanpy.com/tutorials/machine-learning/torch/RNN-regression">这一节内容的 RNN</a> 来解释动态计算图。那节内容的<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/11_RNN_regressor.py">代码在这</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">######################## 前面代码都一样, 下面开始不同 #########################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################ 那节内容的代码结构 (静态 time step) ##########</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    start, end = step * np.pi, (step+<span class="number">1</span>)*np.pi   <span class="comment"># time steps 都是一样长的</span></span><br><span class="line">    <span class="comment"># use sin predicts cos</span></span><br><span class="line">    steps = np.linspace(start, end, <span class="number">10</span>, dtype=np.float32)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################ 这节内容修改代码 (动态 time step) #########</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">    dynamic_steps = np.random.randint(<span class="number">1</span>, <span class="number">4</span>)  <span class="comment"># 随机 time step 长度</span></span><br><span class="line">    start, end = step * np.pi, (step + dynamic_steps) * np.pi  <span class="comment"># different time steps length</span></span><br><span class="line">    step += dynamic_steps</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use sin predicts cos</span></span><br><span class="line">    steps = np.linspace(start, end, <span class="number">10</span> * dynamic_steps, dtype=np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#######################  这下面又一样了 ###########################</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(steps))   <span class="comment"># print how many time step feed to RNN</span></span><br><span class="line"></span><br><span class="line">    x_np = np.sin(steps)    <span class="comment"># float32 for converting torch FloatTensor</span></span><br><span class="line">    y_np = np.cos(steps)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出的动态time step 长</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">10</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">20</span></span><br><span class="line"><span class="string">30</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>有人会说了，Tensorflow 也有类似的功能呀，比如说 <code>dynamic_rnn()</code>。对的，没错，不过大家是否想过，如果我在 Tensorflow 当中定义一个 input 的 <code>placeholder</code>，这个 <code>placeholder</code> 将会有 (<code>batch</code>, <code>time step</code>, <code>input size</code>) 这几个维度，<code>batch</code> 好说，随便什么大小都可以，可是 <code>time step</code> 可是固定的呀，这可不好改，或者说改起来很麻烦。那 PyTorch 中又可以变 <code>batch</code> 又可以变 <code>time step</code>，这不是很方便吗。这就体现了动态神经网络的好处。</p>
<p>经过这样的折腾，torch 还能 handle 住，已经很不容易啦。所以当你想要处理这些动态计算图的时候，Torch 还是你首选的神经网络模块。</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/501_why_torch_dynamic_graph.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="GPU-加速运算"><a href="#GPU-加速运算" class="headerlink" title="GPU 加速运算"></a>GPU 加速运算</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>在 GPU 训练可以大幅提升运算速度。而且 Torch 也有一套很好的 GPU 运算体系。但是要强调的是：* 你的电脑里有合适的 GPU 显卡(NVIDIA)，且支持 CUDA 模块。<a href="https://developer.nvidia.com/cuda-gpus">请在NVIDIA官网查询</a> * 必须安装 GPU 版的 Torch，<a href="https://mofanpy.com/tutorials/machine-learning/torch/install">点击这里查看如何安装</a></p>
<h4 id="用-GPU-训练-CNN"><a href="#用-GPU-训练-CNN" class="headerlink" title="用 GPU 训练 CNN"></a>用 GPU 训练 CNN</h4><p>这份 GPU 的代码是依据<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py">之前这份CNN</a>的代码修改的。大概修改的地方包括将数据的形式变成 GPU 能读的形式，然后将 CNN 也变成 GPU 能读的形式。做法就是在后面加上 <code>.cuda()</code>，很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./mnist/&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!! 修改 test data 形式 !!!!!!!!! #</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>].cuda()/<span class="number">255.</span>   <span class="comment"># Tensor on GPU</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>].cuda()</span><br></pre></td></tr></table></figure>

<p>再来把我们的 CNN 参数也变成 GPU 兼容形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!! 转换 cnn 去 CUDA !!!!!!!!! #</span></span><br><span class="line">cnn.cuda()      <span class="comment"># Moves all model parameters and buffers to the GPU.</span></span><br></pre></td></tr></table></figure>

<p>然后就是在 train 的时候，将每次的training data 变成 GPU 形式 + <code>.cuda()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch ..:</span><br><span class="line">    <span class="keyword">for</span> step, ...:</span><br><span class="line">        <span class="comment"># !!!!!!!! 这里有修改 !!!!!!!!! #</span></span><br><span class="line">        b_x = x.cuda()    <span class="comment"># Tensor on GPU</span></span><br><span class="line">        b_y = y.cuda()    <span class="comment"># Tensor on GPU</span></span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            test_output = cnn(test_x)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># !!!!!!!! 这里有修改  !!!!!!!!! #</span></span><br><span class="line">            pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].cuda().data.squeeze()  <span class="comment"># 将操作放去 GPU</span></span><br><span class="line"></span><br><span class="line">            accuracy = torch.<span class="built_in">sum</span>(pred_y == test_y) / test_y.size(<span class="number">0</span>)</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">test_output = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!!!!!!! 这里有修改 !!!!!!!!! #</span></span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].cuda().data.squeeze()  <span class="comment"># 将操作放去 GPU</span></span><br><span class="line">...</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">10</span>], <span class="string">&#x27;real number&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>大功告成~</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py">github 代码</a> 中的每一步的意义啦。</p>
<h4 id="转移至-CPU"><a href="#转移至-CPU" class="headerlink" title="转移至 CPU"></a>转移至 CPU</h4><p>如果你有些计算还是需要在 CPU 上进行的话呢，比如 <code>plt</code> 的可视化，我们需要将这些计算或者数据转移至 CPU。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cpu_data = gpu_data.cpu()</span><br></pre></td></tr></table></figure>



<h3 id="什么是过拟合-Overfitting"><a href="#什么是过拟合-Overfitting" class="headerlink" title="什么是过拟合 (Overfitting)"></a>什么是过拟合 (Overfitting)</h3><p>今天我们会来聊聊机器学习中的过拟合 overfitting 现象，和解决过拟合的方法。</p>
<h4 id="过于自负"><a href="#过于自负" class="headerlink" title="过于自负"></a>过于自负</h4><p><img src="https://static.mofanpy.com/results/ML-intro/overfitting1.png"></p>
<p>在细说之前，我们先用实际生活中的一个例子来比喻一下过拟合现象。说白了，就是机器学习模型于自信。已经到了自负的阶段了。那自负的坏处，大家也知道，就是在自己的小圈子里表现非凡，不过在现实的大圈子里却往往处处碰壁。所以在这个简介里，我们把自负和过拟合画上等号。</p>
<h4 id="回归分类的过拟合"><a href="#回归分类的过拟合" class="headerlink" title="回归分类的过拟合"></a>回归分类的过拟合</h4><p><img src="https://static.mofanpy.com/results/ML-intro/overfitting2.png"></p>
<p>机器学习模型的自负又表现在哪些方面呢。这里是一些数据。如果要你画一条线来描述这些数据，大多数人都会这么画。对，这条线也是我们希望机器也能学出来的一条用来总结这些数据的线。这时蓝线与数据的总误差可能是10。可是有时候，机器过于纠结这误差值，他想把误差减到更小，来完成他对这一批数据的学习使命。所以，他学到的可能会变成这样。它几乎经过了每一个数据点，这样，误差值会更小。可是误差越小就真的好吗？看来我们的模型还是太天真了。当我拿这个模型运用在现实中的时候，他的自负就体现出来。小二，来一打现实数据。这时，之前误差大的蓝线误差基本保持不变。误差小的红线误差值突然飙高，自负的红线再也骄傲不起来，因为他不能成功的表达除了训练数据以外的其他数据。这就叫做过拟合。Overfitting。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/overfitting3.png"></p>
<p>那么在分类问题当中。过拟合的分割线可能是这样，小二，再上一打数据。我们明显看出，有两个黄色的数据并没有被很好的分隔开来。这也是过拟合在作怪。好了，既然我们时不时会遇到过拟合问题，那解决的方法有那些呢。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/overfitting4.png"></p>
<p>方法一：增加数据量，大部分过拟合产生的原因是因为数据量太少了。如果我们有成千上万的数据，红线也会慢慢被拉直，变得没那么扭曲。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/overfitting5.png"></p>
<p>方法二：运用正规化。L1, l2 regularization等等，这些方法适用于大多数的机器学习，包括神经网络。他们的做法大同小异，我们简化机器学习的关键公式为 y=Wx。W为机器需要学习到的各种参数。在过拟合中，W 的值往往变化得特别大或特别小。为了不让W变化太大，我们在计算误差上做些手脚。原始的 cost 误差是这样计算，cost = 预测值-真实值的平方。如果 W 变得太大，我们就让 cost 也跟着变大，变成一种惩罚机制。所以我们把 W 自己考虑进来。这里 abs 是绝对值。这一种形式的正规化，叫做 l1 正规化。L2 正规化和 l1 类似，只是绝对值换成了平方。其他的l3，l4 也都是换成了立方和4次方等等。形式类似。用这些方法，我们就能保证让学出来的线条不会过于扭曲。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/overfitting6.png"></p>
<p>还有一种专门用在神经网络的正规化的方法，叫作 dropout。在训练的时候，我们随机忽略掉一些神经元和神经联结，是这个神经网络变得”不完整”。用一个不完整的神经网络训练一次。</p>
<p>到第二次再随机忽略另一些，变成另一个不完整的神经网络。有了这些随机 drop 掉的规则，我们可以想象其实每次训练的时候，我们都让每一次预测结果都不会依赖于其中某部分特定的神经元。像l1，l2正规化一样，过度依赖的 W，也就是训练参数的数值会很大，l1，l2会惩罚这些大的参数。Dropout 的做法是从根本上让神经网络没机会过度依赖。</p>
<h3 id="Dropout-缓解过拟合"><a href="#Dropout-缓解过拟合" class="headerlink" title="Dropout 缓解过拟合"></a>Dropout 缓解过拟合</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>过拟合让人头疼，明明训练时误差已经降得足够低，可是测试的时候误差突然飙升。这很有可能就是出现了过拟合现象。强烈推荐通过<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-overfitting">这个动画</a>的形式短时间了解什么是过拟合，怎么解决过拟合。下面动图就显示了我们成功缓解了过拟合现象。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-3-1.gif"></p>
<h4 id="做点数据"><a href="#做点数据" class="headerlink" title="做点数据"></a>做点数据</h4><p>自己做一些伪数据，用来模拟真实情况。数据少，才能凸显过拟合问题，所以我们就做10个数据点。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-3-2.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">N_SAMPLES = <span class="number">20</span></span><br><span class="line">N_HIDDEN = <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training data</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, N_SAMPLES), <span class="number">1</span>)</span><br><span class="line">y = x + <span class="number">0.3</span>*torch.normal(torch.zeros(N_SAMPLES, <span class="number">1</span>), torch.ones(N_SAMPLES, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># test data</span></span><br><span class="line">test_x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, N_SAMPLES), <span class="number">1</span>)</span><br><span class="line">test_y = test_x + <span class="number">0.3</span>*torch.normal(torch.zeros(N_SAMPLES, <span class="number">1</span>), torch.ones(N_SAMPLES, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># show data</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy(), c=<span class="string">&#x27;magenta&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c=<span class="string">&#x27;cyan&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.ylim((-<span class="number">2.5</span>, <span class="number">2.5</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h4><p>我们在这里搭建两个神经网络，一个没有 <code>dropout</code>，一个有 <code>dropout</code>。没有 <code>dropout</code> 的容易出现过拟合，那我们就命名为 <code>net_overfitting</code>，另一个就是 <code>net_dropped</code>。<code>torch.nn.Dropout(0.5)</code> 这里的 0.5 指的是随机有 50% 的神经元会被关闭/丢弃。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net_overfitting = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, N_HIDDEN),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, <span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">net_dropped = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, N_HIDDEN),</span><br><span class="line">    torch.nn.Dropout(<span class="number">0.5</span>),  <span class="comment"># drop 50% of the neuron</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</span><br><span class="line">    torch.nn.Dropout(<span class="number">0.5</span>),  <span class="comment"># drop 50% of the neuron</span></span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(N_HIDDEN, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练的时候，这两个神经网络分开训练。训练的环境都一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">optimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    pred_ofit = net_overfitting(x)</span><br><span class="line">    pred_drop = net_dropped(x)</span><br><span class="line"></span><br><span class="line">    loss_ofit = loss_func(pred_ofit, y)</span><br><span class="line">    loss_drop = loss_func(pred_drop, y)</span><br><span class="line"></span><br><span class="line">    optimizer_ofit.zero_grad()</span><br><span class="line">    optimizer_drop.zero_grad()</span><br><span class="line">    loss_ofit.backward()</span><br><span class="line">    loss_drop.backward()</span><br><span class="line">    optimizer_ofit.step()</span><br><span class="line">    optimizer_drop.step()</span><br></pre></td></tr></table></figure>



<h4 id="对比测试结果"><a href="#对比测试结果" class="headerlink" title="对比测试结果"></a>对比测试结果</h4><p>在这个 <code>for</code> 循环里，我们加上画测试图的部分。注意在测试时，要将网络改成 <code>eval()</code> 形式，特别是 <code>net_dropped</code>，<code>net_overfitting</code> 改不改其实无所谓。画好图再改回 <code>train()</code> 模式。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-3-1.gif"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">    optimizer_ofit.step()</span><br><span class="line">    optimizer_drop.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">10</span> == <span class="number">0</span>:     <span class="comment"># 每 10 步画一次图</span></span><br><span class="line">        <span class="comment"># 将神经网络转换成测试形式, 画好图之后改回 训练形式</span></span><br><span class="line">        net_overfitting.<span class="built_in">eval</span>()</span><br><span class="line">        net_dropped.<span class="built_in">eval</span>()  <span class="comment"># 因为 drop 网络在 train 的时候和 test 的时候参数不一样.</span></span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        test_pred_ofit = net_overfitting(test_x)</span><br><span class="line">        test_pred_drop = net_dropped(test_x)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将两个网络改回 训练形式</span></span><br><span class="line">        net_overfitting.train()</span><br><span class="line">        net_dropped.train()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/5-3-3.png"></p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="什么是批标准化-Batch-Normalization"><a href="#什么是批标准化-Batch-Normalization" class="headerlink" title="什么是批标准化 (Batch Normalization)"></a>什么是批标准化 (Batch Normalization)</h3><p>今天我们会来聊聊批标准化 Batch Normalization。</p>
<h4 id="普通数据标准化"><a href="#普通数据标准化" class="headerlink" title="普通数据标准化"></a>普通数据标准化</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/NB1.png"></p>
<p>Batch Normalization，批标准化，和普通的数据标准化类似，是将分散的数据统一的一种做法，也是优化神经网络的一种方法。在之前 Normalization 的简介视频中我们一提到，具有统一规格的数据，能让机器学习更容易学习到数据之中的规律。</p>
<h4 id="每层都做标准化"><a href="#每层都做标准化" class="headerlink" title="每层都做标准化"></a>每层都做标准化</h4><p><img src="https://static.mofanpy.com/results/ML-intro/NB2.png"></p>
<p>在神经网络中，数据分布对训练会产生影响。比如某个神经元 x 的值为1，某个 Weights 的初始值为 0.1，这样后一层神经元计算结果就是 Wx = 0.1；又或者 x = 20，这样 Wx 的结果就为 2。现在还不能看出什么问题，但是，当我们加上一层激励函数，激活这个 Wx 值的时候，问题就来了。如果使用 像 tanh 的激励函数，Wx 的激活值就变成了 ~0.1 和 ~1，接近于 1 的部已经处在了 激励函数的饱和阶段，也就是如果 x 无论再怎么扩大，tanh 激励函数输出值也还是接近1。换句话说，神经网络在初始阶段已经不对那些比较大的 x 特征范围敏感了。这样很糟糕，想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别，这就证明我的感官系统失效了。当然我们是可以用之前提到的对数据做 normalization 预处理，使得输入的 x 变化范围不会太大，让输入值经过激励函数的敏感部分。但刚刚这个不敏感问题不仅仅发生在神经网络的输入层，而且在隐藏层中也经常会发生。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/NB3.png"></p>
<p>只是时候 x 换到了隐藏层当中，我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢？答案是可以的，因为大牛们发明了一种技术，叫做 batch normalization，正是处理这种情况。</p>
<h4 id="BN-添加位置"><a href="#BN-添加位置" class="headerlink" title="BN 添加位置"></a>BN 添加位置</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/NB4.png"></p>
<p>Batch normalization 的 batch 是批数据，把数据分成小批小批进行 stochastic gradient descent。而且在每批数据进行前向传递 forward propagation 的时候，对每一层都进行 normalization 的处理。</p>
<h4 id="BN-效果"><a href="#BN-效果" class="headerlink" title="BN 效果"></a>BN 效果</h4><p>Batch normalization 也可以被看做一个层面。在一层层的添加神经网络的时候，我们先有数据 X，再添加全连接层，全连接层的计算结果会经过激励函数成为下一层的输入，接着重复之前的操作。Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/NB5.png"></p>
<p>之前说过，计算结果在进入激励函数前的值很重要，如果我们不单单看一个值，我们可以说，计算结果值的分布对于激励函数很重要。对于数据值大多分布在这个区间的数据，才能进行更有效的传递。对比这两个在激活之前的值的分布。上者没有进行 normalization，下者进行了 normalization，这样当然是下者能够更有效地利用 tanh 进行非线性化的过程。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/NB6.png"></p>
<p>没有 normalize 的数据使用 tanh 激活以后，激活值大部分都分布到了饱和阶段，也就是大部分的激活值不是-1，就是1，而 normalize 以后，大部分的激活值在每个分布区间都还有存在。再将这个激活后的分布传递到下一层神经网络进行后续计算，每个区间都有分布的这一种对于神经网络就会更加有价值。Batch normalization 不仅仅 normalize 了一下数据，他还进行了反 normalize 的手续。为什么要这样呢？</p>
<h4 id="BN-算法"><a href="#BN-算法" class="headerlink" title="BN 算法"></a>BN 算法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/NB7.png"></p>
<p>我们引入一些 batch normalization 的公式。这三步就是我们在刚刚一直说的 normalization 工序，但是公式的后面还有一个反向操作，将 normalize 后的数据再扩展和平移。原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma，和平移参数 β，这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用，如果没有起到作用，我就使用 gamma 和 belt 来抵消一些 normalization 的操作。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/NB8.png"></p>
<p>最后我们来看看一张神经网络训练到最后，代表了每层输出值的结果的分布图。这样我们就能一眼看出 Batch normalization 的功效啦。让每一层的值在有效的范围内传递下去。</p>
<h3 id="Batch-Normalization-批标准化"><a href="#Batch-Normalization-批标准化" class="headerlink" title="Batch Normalization 批标准化"></a>Batch Normalization 批标准化</h3><h4 id="要点-3"><a href="#要点-3" class="headerlink" title="要点"></a>要点</h4><p>批标准化通俗来说就是对每一层神经网络进行标准化 (normalize) 处理，我们知道对输入数据进行标准化能让机器学习有效率地学习。如果把每一层后看成这种接受输入数据的模式，那我们何不 批标准化所有的层呢？具体而且清楚的解释请看到 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-batch-normalization">我制作的 什么批标准化 动画简介(推荐)</a>。</p>
<p>那我们就看看下面的两个动图，这就是在每层神经网络有无 batch normalization 的区别啦。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-1.gif"></p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-2.gif"></p>
<h4 id="做点数据-1"><a href="#做点数据-1" class="headerlink" title="做点数据"></a>做点数据</h4><p>自己做一些伪数据，用来模拟真实情况。而且 Batch Normalization (之后都简称BN) 还能有效的控制坏的参数初始化 (initialization)，比如说 <code>ReLU</code> 这种激励函数最怕所有的值都落在附属区间，那我们就将所有的参数都水平移动一个 -0.2 (<code>bias_initialization = -0.2</code>)，来看看 BN 的实力。</p>
<p><img src="https://static.mofanpy.com/results-small/torch/5-4-3.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">N_SAMPLES = <span class="number">2000</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line">LR = <span class="number">0.03</span></span><br><span class="line">N_HIDDEN = <span class="number">8</span></span><br><span class="line">ACTIVATION = F.tanh     <span class="comment"># 你可以换 relu 试试</span></span><br><span class="line">B_INIT = -<span class="number">0.2</span>   <span class="comment"># 模拟不好的 参数初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training data</span></span><br><span class="line">x = np.linspace(-<span class="number">7</span>, <span class="number">10</span>, N_SAMPLES)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">2</span>, x.shape)</span><br><span class="line">y = np.square(x) - <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line"><span class="comment"># test data</span></span><br><span class="line">test_x = np.linspace(-<span class="number">7</span>, <span class="number">10</span>, <span class="number">200</span>)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">2</span>, test_x.shape)</span><br><span class="line">test_y = np.square(test_x) - <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line">train_x, train_y = torch.from_numpy(x).<span class="built_in">float</span>(), torch.from_numpy(y).<span class="built_in">float</span>()</span><br><span class="line">test_x = torch.from_numpy(test_x).<span class="built_in">float</span>()</span><br><span class="line">test_y = torch.from_numpy(test_y).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">train_dataset = Data.TensorDataset(train_x, train_y)</span><br><span class="line">train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show data</span></span><br><span class="line">plt.scatter(train_x.numpy(), train_y.numpy(), c=<span class="string">&#x27;#FF9359&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.2</span>, label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="搭建神经网络-1"><a href="#搭建神经网络-1" class="headerlink" title="搭建神经网络"></a>搭建神经网络</h4><p>这里就教你如何构建带有 BN 的神经网络的。BN 其实可以看做是一个 layer (<code>BN layer</code>)。我们就像平时加层一样加 <code>BN layer</code> 就好了。注意，我还对输入数据进行了一个 BN 处理，因为如果你把输入数据看出是从前面一层来的输出数据，我们同样也能对它进行 BN。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, batch_normalization=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.do_bn = batch_normalization</span><br><span class="line">        self.fcs = []   <span class="comment"># 太多层了, 我们用 for loop 建立</span></span><br><span class="line">        self.bns = []</span><br><span class="line">        self.bn_input = nn.BatchNorm1d(<span class="number">1</span>, momentum=<span class="number">0.5</span>)   <span class="comment"># 给 input 的 BN</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_HIDDEN):               <span class="comment"># 建层</span></span><br><span class="line">            input_size = <span class="number">1</span> <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="number">10</span></span><br><span class="line">            fc = nn.Linear(input_size, <span class="number">10</span>)</span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">&#x27;fc%i&#x27;</span> % i, fc)       <span class="comment"># 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug</span></span><br><span class="line">            self._set_init(fc)                  <span class="comment"># 参数初始化</span></span><br><span class="line">            self.fcs.append(fc)</span><br><span class="line">            <span class="keyword">if</span> self.do_bn:</span><br><span class="line">                bn = nn.BatchNorm1d(<span class="number">10</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">                <span class="built_in">setattr</span>(self, <span class="string">&#x27;bn%i&#x27;</span> % i, bn)   <span class="comment"># 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug</span></span><br><span class="line">                self.bns.append(bn)</span><br><span class="line"></span><br><span class="line">        self.predict = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)         <span class="comment"># output layer</span></span><br><span class="line">        self._set_init(self.predict)            <span class="comment"># 参数初始化</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_set_init</span>(<span class="params">self, layer</span>):</span>     <span class="comment"># 参数初始化</span></span><br><span class="line">        init.normal_(layer.weight, mean=<span class="number">0.</span>, std=<span class="number">.1</span>)</span><br><span class="line">        init.constant_(layer.bias, B_INIT)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        pre_activation = [x]</span><br><span class="line">        <span class="keyword">if</span> self.do_bn: x = self.bn_input(x)    <span class="comment"># 判断是否要加 BN</span></span><br><span class="line">        layer_input = [x]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N_HIDDEN):</span><br><span class="line">            x = self.fcs[i](x)</span><br><span class="line">            pre_activation.append(x)    <span class="comment"># 为之后出图</span></span><br><span class="line">            <span class="keyword">if</span> self.do_bn: x = self.bns[i](x)  <span class="comment"># 判断是否要加 BN</span></span><br><span class="line">            x = ACTIVATION(x)</span><br><span class="line">            layer_input.append(x)       <span class="comment"># 为之后出图</span></span><br><span class="line">        out = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> out, layer_input, pre_activation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立两个 net, 一个有 BN, 一个没有</span></span><br><span class="line">nets = [Net(batch_normalization=<span class="literal">False</span>), Net(batch_normalization=<span class="literal">True</span>)]</span><br></pre></td></tr></table></figure>



<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>训练的时候，这两个神经网络分开训练。训练的环境都一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opts = [torch.optim.Adam(net.parameters(), lr=LR) <span class="keyword">for</span> net <span class="keyword">in</span> nets]</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">losses = [[], []]  <span class="comment"># 每个网络一个 list 来记录误差</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        <span class="keyword">for</span> net, opt <span class="keyword">in</span> <span class="built_in">zip</span>(nets, opts):     <span class="comment"># 训练两个网络</span></span><br><span class="line">            pred, _, _ = net(b_x)</span><br><span class="line">            loss = loss_func(pred, b_y)</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()    <span class="comment"># 这也会训练 BN 里面的参数</span></span><br></pre></td></tr></table></figure>



<h4 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h4><p>这个教程有几张图要画，首先我们画训练时的动态图。我单独定义了一个画动图的功能 <code>plot_histogram()</code>，因为不是重点，所以代码的具体细节请看我的 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">github</a>。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-2.gif"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f, axs = plt.subplots(<span class="number">4</span>, N_HIDDEN+<span class="number">1</span>, figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_histogram</span>(<span class="params">l_in, l_in_bn, pre_ac, pre_ac_bn</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    layer_inputs, pre_acts = [], []</span><br><span class="line">    <span class="keyword">for</span> net, l <span class="keyword">in</span> <span class="built_in">zip</span>(nets, losses):</span><br><span class="line">        <span class="comment"># 一定要把 net 的设置成 eval 模式, eval下的 BN 参数会被固定</span></span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        pred, layer_input, pre_act = net(test_x)</span><br><span class="line">        l.append(loss_func(pred, test_y).data[<span class="number">0</span>])</span><br><span class="line">        layer_inputs.append(layer_input)</span><br><span class="line">        pre_acts.append(pre_act)</span><br><span class="line">        <span class="comment"># 收集好信息后将 net 设置成 train 模式, 继续训练</span></span><br><span class="line">        net.train()</span><br><span class="line">    plot_histogram(*layer_inputs, *pre_acts)     <span class="comment"># plot histogram</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 后面接着之前 for loop 中的代码来</span></span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>后面还有两张图，一张是预测曲线，一张是误差变化曲线，具体代码不在这里呈现，想知道如何画图的朋友，请参考我的 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">github</a></p>
<h4 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h4><p>首先来看看这次对比的两个激励函数是长什么样：</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-8.png"></p>
<p>然后我们来对比使用不同激励函数的结果。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-1.png"></p>
<p><img src="https://static.mofanpy.com/results-small/torch/5-4-4.png"></p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-5.png"></p>
<p>上面是使用 <code>relu</code> 激励函数的结果，我们可以看到，没有使用 BN 的误差要高，线条不能拟合数据，原因是我们有一个 Bad initialization，初始 <code>bias = -0.2</code>，这一招，让 <code>relu</code> 无法捕捉到在负数区间的输入值。而有了 BN，这就不成问题了。</p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-2.png"></p>
<p><img src="https://static.mofanpy.com/results-small/torch/5-4-6.png"></p>
<p><img src="https://static.mofanpy.com/results/torch/5-4-7.png"></p>
<p>上面结果是使用 <code>tanh</code> 作为激励函数的结果，可以看出，不好的初始化，让输入数据在激活前分散得非常离散，而有了 BN，数据都被收拢了。收拢的数据再放入激励函数就能很好地利用激励函数的非线性。而且可以看出没有 BN 的数据让激活后的结果都分布在 <code>tanh</code> 的两端，而这两端的梯度又非常的小，是的后面的误差都不能往前传，导致神经网络死掉了。</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">github 代码</a> 中的每一步的意义啦。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/501_why_torch_dynamic_graph.py">第一节的全部代码</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/502_GPU.py">第二节的全部代码</a></li>
</ul>
<hr>
<ul>
<li>Tensorflow: dropout <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/dropout">教程</a></li>
<li>PyTorch: dropout <a href="https://mofanpy.com/tutorials/machine-learning/torch/dropout">教程</a></li>
<li>Theano: l1 l2 regularization <a href="https://mofanpy.com/tutorials/machine-learning/theano/regularization">教程</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/503_dropout.py">第四节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/501_dropout.py">Tensorflow 的 50行 Dropout 代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-overfitting">我制作的 什么是过拟合 动画简介</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-intro/l1l2regularization">我制作的 L1/L2 正规化 动画简介</a></li>
</ul>
<hr>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/BN">Tensorflow 使用 Batch normalization</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/batch-normalization">PyTorch 使用 Batch normalization</a></li>
<li>论文 <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py">第六节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/502_batch_normalization.py">Tensorflow 的70行 批标准化代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-batch-normalization">我制作的 什么批标准化 动画简介</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>XCTF-逆向基础篇</title>
    <url>/YingYingMonstre.github.io/2021/09/26/XCTF-%E9%80%86%E5%90%91%E5%9F%BA%E7%A1%80%E7%AF%87/</url>
    <content><![CDATA[<h2 id="insanity"><a href="#insanity" class="headerlink" title="insanity"></a>insanity</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/1.png" alt="img"></p>
<p>发现这不是PE文件,是ELF文件，将程序在Linux环境下运行</p>
<p>0x02,查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/2.png" alt="img"></p>
<p>发现是32位的程序</p>
<p>0x03.IDA </p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/3.png" alt="img"></p>
<p>F5查看伪函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/4.png" alt="img"></p>
<p>发现一个关键的字符串，&amp;strs,发现是取这个字符串输出，然后，跟进strs</p>
<p>(shift + f12 字符串窗口)</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/insanity/5.png" alt="img"></p>
<p>发现有一个明显的提示：This_is_a_flag</p>
<p>猜测9447{This_is_a_flag}是最后的falg</p>
<p>或者直接用记事本打开，仔细找也能找到。</p>
<h2 id="python-trade"><a href="#python-trade" class="headerlink" title="python-trade"></a>python-trade</h2><p><strong>[工具]</strong></p>
<p>在线python反编译</p>
<p><strong>[分析过程]</strong></p>
<p>0x01.下载附件</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/1.png" alt="img"></p>
<p>注：</p>
<p>pyc文件是py文件编译后生成的字节码文件</p>
<p>0x02.在线Python反编译</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/2.png" alt="img"></p>
<p>这是生成的py文件</p>
<p>然后，对这个文件的运算逻辑进行逆向</p>
<p>0x03.写EXP</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line">correct = <span class="string">&quot;XlNkVmtUI1MgXWBZXCFeKY+AaXNt&quot;</span></span><br><span class="line">s = base64.b64decode(correct)</span><br><span class="line">flag = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">    flag += <span class="built_in">chr</span>((i-<span class="number">16</span>) ^ <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(flag)</span><br></pre></td></tr></table></figure>

<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/python-trade/3.png" alt="img"></p>
<p>先对字符串进行b64decode,然后，再进行xor运算得到最后的flag:nctf{d3c0mpil1n9_PyC}</p>
<p>0x04.运行脚本</p>
<p>nctf{d3c0mpil1n9_PyC}</p>
<h2 id="re1"><a href="#re1" class="headerlink" title="re1"></a>re1</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>参考</p>
<p>0x01.运行程序</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/1.png" alt="img"></p>
<p>可以看到需要输入正确的flag</p>
<p>那么现在，我们需要判断程序是多少位的，有没有加壳</p>
<p>0x02.exeinfope查详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/2.png" alt="img"></p>
<p>可以看到程序是32位的，是Microsoft Visual c++编译的，并且没有加壳</p>
<p>注：查壳工具还有PEID，EID，但是推荐EID或者exeinfope，因为，PEID查壳的时候有时候不准确</p>
<p>那么，我们可以用静态分析神器 IDA 打开，进一步分析了</p>
<p>0x03.</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/3.png" alt="img"></p>
<p>然后，查找主函数main,可以看到右侧的是反汇编的汇编代码，这时候，我们可以直接分析汇编语言，但是，汇编语言看起来太多，费劲。这个时候就可以是有IDA是最强大的功能F5了，它能够直接将汇编代码生成C语言代码，虽然和这个程序的源码不完全一样，但是逻辑关系是一样的</p>
<p>F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/4.png" alt="img"></p>
<p>这是整个main函数的运算逻辑</p>
<p>可以看到一个关键的字符串，print(aFlag)，那么证明这就是输入正确flag，然后，会输出aFlag证明你的flag正确，然后，继续往上分析，可以看到v3的值，是由strcmp()决定的，比较v5和输入的字符串，如果一样就会进入后面的if判断，所以,我们继续往上分析，看看哪里又涉及v5，可以看到开头的_mm_storeu_si128(），对其进行分析发现它类似于memset(),将xmmword_413E34的值赋值给v5，所以，我们可以得到正确的flag应该在xmmword_413E34中，然后，我们双击413E34进行跟进</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/5.png" alt="img"></p>
<p>可以看到一堆十六进制的数</p>
<p>这时，我们使用IDA的另一个功能 R ，能够将十六进制的数转换为字符串。</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/re1/6.png" alt="img"></p>
<p>这就是我们最后的flag了</p>
<p>注：这里要跟大家普及一个知识了，及大端与小端</p>
<p>假设一个十六进制数0x12345678</p>
<p>大端的存储方式是：12,34,56,78，然后读取的时候也是从前往后读</p>
<p>小端的存储方式是：78,56,34,12，然后读取的时候是从后往前读取</p>
<p>所以，最后的flag应该是：DUTCTF{We1c0met0DUTCTF}</p>
<p>0x04.运行程序输入正确的flag</p>
<p>方法二：记事本打开，也能找到。</p>
<p>方法三：OD–》插件–》中文搜索引擎–》ASCII也能搜到。</p>
<h2 id="game"><a href="#game" class="headerlink" title="game"></a>game</h2><p>我的做法（暴力破解）：先用exeinfo pe查壳，发现是32位未加壳。用OD打开，用插件里的中文搜索ASCII，能找到done flag is的字符串。然后找到关键CALL（00B301BB和00B30359），修改跳转（从%d那里直接改为CALL xxxx跳到done那里）。运行到那里之后就可以得到flag了。</p>
<p>直接玩游戏：从1输到8。</p>
<p>爆破方法二：找到F5后伪代码最后那部分判断的代码（空格切换到图形视图，对着最后的那部分再空格切换回来），patch修改，正好有8个JNZ，改5个为jz，然后Edit–》Patch program–》Apply patches to input file，点OK，再回去运行就可以得到了。</p>
<p>IDA分析代码逻辑：先是判断是输入的是否是1-8，然后进入后面的if判断然后进行循环，这个时候应该就是程序的亮暗的显示，然后，如果byte_532E28每一位都是1，那么，就会进入sub_457AB4,然后我们猜测这里应该就是最后的flag的地方。然后跟进 sub_457AB4。（注：这里说明一下，如果IDA不能正确的获得自定义函数的名字，那么IDA会用sub__加上自定义函数的起始地址来定义函数的名字）</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/8.png" alt="img"></p>
<p>这里只截取了后面的部分，发现函数进行了两次xor运算，xor的逆运算也是xor，那么我们就可以根据这个运算来写脚本得到最后的flag。</p>
<p>这里看到v2和v59这就证明了这是两个数组的运算，所以我们应该将上面的字符串分成两个数组，分别从v2和v59开始</p>
<p>0x05.写EXP</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/9.png" alt="img"></p>
<p>这里先是通过循环，将a和b数组的值进行xor运算，然后再将数组a的值与0x13xor运算</p>
<p>chr()：是将十六进制转换为字符串</p>
<p>0x05.运行脚本</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/game/10.png" alt="img"></p>
<p>得到最后的flag: zsctf{T9is_tOpic_1s_v5ry_int7resting_b6t_others_are_n0t}。</p>
<p>广度优先搜索法：未证。</p>
<h2 id="Hello，CTF"><a href="#Hello，CTF" class="headerlink" title="Hello，CTF"></a>Hello，CTF</h2><p>查壳，32位、无壳。</p>
<p>IDA从main开始分析，F5查看伪代码。首先，可以看到先是将字符串复制到v13的位置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">strcpy(&amp;v13, &quot;437261636b4d654a757374466f7246756e&quot;);</span><br></pre></td></tr></table></figure>

<p>然后，后面对输入进行了判断，输入的字符串不能大于17接着，将字符串以十六进制输出，然后，再将得到的十六进制字符添加到v10最后，进行比较，看输入的字符串是否和v10的字符串相等，如果相等，则得到真确的flag。最后将字符串转换为十六进制。</p>
<h2 id="open-source"><a href="#open-source" class="headerlink" title="open-source"></a>open-source</h2><p>代码审计：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">first = <span class="number">0xcafe</span></span><br><span class="line">flag = first * <span class="number">31337</span> + (second % <span class="number">17</span>) * <span class="number">11</span> + <span class="built_in">len</span>(<span class="string">&quot;h4cky0u&quot;</span>) - <span class="number">1615810207</span></span><br></pre></td></tr></table></figure>

<p>关键在于second的取值。观察代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> <span class="keyword">int</span> second = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="keyword">if</span> (second % <span class="number">5</span> == <span class="number">3</span> || second % <span class="number">17</span> != <span class="number">8</span>) &#123;</span><br><span class="line">    	<span class="built_in">printf</span>(<span class="string">&quot;ha, you won&#x27;t get it!\n&quot;</span>);</span><br><span class="line">    	<span class="built_in">exit</span>(<span class="number">3</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>这里想到要get it就要将if里的逻辑取反，即second % 5 != 3 &amp;&amp; second % 17 == 8。</p>
<p>故(second % 17) * 11=8*11=88。得到flag=12648430–化为16进制–》c0ffee。</p>
<h2 id="simple-unpack"><a href="#simple-unpack" class="headerlink" title="simple_unpack"></a>simple_unpack</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, upx </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/1.png" alt="img"></p>
<p>发现有upx壳。</p>
<p>注：windows下的文件是PE文件，Linux/Unix下的文件是ELF文件</p>
<p>0x02.UPX 脱壳</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/2.png" alt="img"></p>
<p>upx -d 即可对upx壳进行脱壳</p>
<p>0x03.载入IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/3.png" alt="img"></p>
<p>还是从main函数开始分析，结果我们再右侧发现了意外惊喜</p>
<p>运行程序，输入我们看到的flag:flag{Upx_1s_n0t_a_d3liv3r_c0mp4ny}</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/simple-unpack/4.png" alt="img"></p>
<h2 id="logmein"><a href="#logmein" class="headerlink" title="logmein"></a>logmein</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/1.png" alt="img"></p>
<p>发现程序是一个ELF文件，将其放入Linux环境中进行分析</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/2.png" alt="img"></p>
<p>发现程序是64位的，使用静态分析工具IDA进行分析</p>
<p>0x02.IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/3.png" alt="img"></p>
<p>从main函数开始分析，使用F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/4.png" alt="img"></p>
<p>发现main函数的整个运算逻辑</p>
<p>先是，将指定字符串复制到v8</p>
<p>s是用户输入的字符串，先进行比较长度，如果长度比v8小，则进入sub_4007c0函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/5.png" alt="img"></p>
<p>可以看出输出字符串Incorrect password,然后，退出</p>
<p>如果长度大于或等与v8则进入下面的循环</p>
<p>看到判断如果输入的字符串和经过运算后的后字符串不等，则进入sub_4007c0,输出Incorrect password,</p>
<p>如果想得，则进入sub_4007f0函数</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/logmein/6.png" alt="img"></p>
<p>证明输入的字符串就是flag</p>
<p>接下来写脚本</p>
<p>0x03.Write EXP</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">v7 &#x3D; &quot;harambe&quot;</span><br><span class="line">v6 &#x3D; 7</span><br><span class="line">v8 &#x3D; &quot;:\&quot;AL_RT^L*.?+6&#x2F;46&quot;</span><br><span class="line">flag &#x3D; &quot;&quot;</span><br><span class="line">for i in range(0, len(v8)):</span><br><span class="line">    flag +&#x3D; chr(ord((v7[i % 7])) ^ ord(v8[i]))</span><br><span class="line">print(flag)</span><br></pre></td></tr></table></figure>

<p>由于程序是小段的存储方式，所以，ebmarah就得变成harambe（C语言数据在内存中是小端存储，一开始v7是一个数据，so）</p>
<p>ord():是将字符串转换为ascii格式，为了方便运算</p>
<p>chr():是将ascii转换为字符串</p>
<p>运行脚本得到最后的flag:RC3-2016-XORISGUD</p>
<p>（直接用C语言更方便些）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#define BYTE unsigned char</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void main() &#123;</span><br><span class="line">	int i;</span><br><span class="line">	int v6 &#x3D; 7;</span><br><span class="line">	__int64 v7 &#x3D; 28537194573619560LL;</span><br><span class="line">	char v8[18] &#x3D; &quot;:\&quot;AL_RT^L*.?+6&#x2F;46&quot;;</span><br><span class="line">	char s[18] &#x3D; &quot;&quot;;</span><br><span class="line">	for ( i &#x3D; 0; i &lt; strlen(v8); ++i ) &#123;</span><br><span class="line">		s[i] +&#x3D; (char)(*((BYTE*)&amp;v7 + i % v6) ^ v8[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	printf(&quot;%s\n&quot;,s);</span><br><span class="line"></span><br><span class="line">	system(&quot;pause&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="no-strings-attached"><a href="#no-strings-attached" class="headerlink" title="no-strings-attached"></a>no-strings-attached</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, GDB</p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和查看程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/1.png" alt="img"></p>
<p>说明程序是ELF文件，32位</p>
<p>0x02.使用静态分析工具IDA进行分析</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/2.png" alt="img"></p>
<p>然后对main函数使用F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/3.png" alt="img"></p>
<p>然后，对每个函数进行跟进，最后发现authenricate(),符合获得flag的函数，对其进行跟进</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/4.png" alt="img"></p>
<p>然后我们发现一个特殊的函数decrypt,根据字面的意思是加密，那么我们可以大概的猜测是一个对dword_8048A90所对应的字符串进行加密，</p>
<p>加密得到的就应该是我们需要的flag，后面的判断应该就是将字符串输出。</p>
<p>这里我们有两种思维方式:</p>
<p>第一种就是跟进decrypt然后分析它的运算逻辑，然后，自己写脚本，得到最后的flag</p>
<p>第二种就涉及逆向的另一种调试方式，及动态调试，这里我就用动态调试了，之前的一直是静态调试</p>
<p>0x03.GDB动态调试</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/5.png" alt="img"></p>
<p>gdb ./no_strings_attached 将文件加载到GDB中</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/6.png" alt="img"></p>
<p>既然是动态调试，那么如果让它一直不停，那我不就相当于运行了嘛，所以，我们就需要下断点，断点就是让程序运行到断点处就停止</p>
<p>之前通过IDA，我们知道关键函数是decrypt,所以我们把断点设置在decrypt处，b在GDB中就是下断点的意思，及在decrypt处下断点</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/7.png" alt="img"></p>
<p>r就是运行的意思，这里运行到了我们之前下的断点处，停止。</p>
<p>我们要的是经过decrypt函数，生成的字符串，所以我们这里就需要运行一步，GDB中用n来表示运行一步</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/8.png" alt="img"></p>
<p>然后我们就需要去查看内存了，去查找最后生成的字符串</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/9.png" alt="img"></p>
<p>通过IDA生成的汇编指令，我们可以看出进过decrypt函数后，生成的字符串保存在EAX寄存器中，所以，我们在GDB就去查看eax寄存器的值</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/10.png" alt="img"></p>
<p>x:就是用来查看内存中数值的，后面的200代表查看多少个，wx代表是以word字节查看看，$eax代表的eax寄存器中的值</p>
<p>在这里我们看到0x00000000，这就证明这个字符串结束了，因为，在C中，代表字符串结尾的就是”\0”,那么前面的就是经过decrypt函数生成的falg</p>
<p>那我们就需要将这些转换为字符串的形式</p>
<p>0x04.Write EXP</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/11.png" alt="img"></p>
<p>首先将寄存器中的值提取出来，然后利用Python的decode函数，通过”hex”的方式转化为字符串，然后输出</p>
<p>0x05.运行脚本</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/no-strings-attached/12.png" alt="img"></p>
<p>得到最后的flag: 9447{you_are_an_international_mystery}</p>
<p>IDA分析就分析它的decrypt函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; [0x3a, 0x36, 0x37, 0x3b, 0x80, 0x7a, 0x71, 0x78,</span><br><span class="line">     0x63, 0x66, 0x73, 0x67, 0x62, 0x65, 0x73, 0x60,</span><br><span class="line">     0x6b, 0x71, 0x78, 0x6a, 0x73, 0x70, 0x64, 0x78,</span><br><span class="line">     0x6e, 0x70, 0x70, 0x64, 0x70, 0x64, 0x6e, 0x7b,</span><br><span class="line">     0x76, 0x78, 0x6a, 0x73, 0x7b, 0x80]</span><br><span class="line">v6 &#x3D; len(s)</span><br><span class="line">a2 &#x3D; [1, 2, 3, 4, 5]</span><br><span class="line">v7 &#x3D; len(a2)</span><br><span class="line">v2 &#x3D; v6</span><br><span class="line">dest &#x3D; s</span><br><span class="line">v4 &#x3D; 0</span><br><span class="line">while v4 &lt; v6:</span><br><span class="line">        dest[v4] -&#x3D; a2[v4 % 5]</span><br><span class="line">        v4 +&#x3D; 1</span><br><span class="line"></span><br><span class="line">flag &#x3D; &#39;&#39;</span><br><span class="line">for j in dest:</span><br><span class="line">    flag +&#x3D; chr(j)</span><br><span class="line">print(flag)</span><br></pre></td></tr></table></figure>



<h2 id="getit"><a href="#getit" class="headerlink" title="getit"></a>getit</h2><p><strong>[工具]</strong></p>
<p>exeinfo pe, IDA, GDB  </p>
<p><strong>[分析过程]</strong></p>
<p>0x01.查壳和程序的详细信息</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/1.png" alt="img"></p>
<p>可以看出这是一个ELF文件，64位</p>
<p>0x02.IDA</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/2.png" alt="img"></p>
<p>对main函数进行F5查看伪代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/3.png" alt="img"></p>
<p>可以看到先判断v5是否大于s存储字符串的长度，然后通过运算，最后将得到的flag写入文件。</p>
<p>但是有意思的地方在flag.txt文件所在的位置是/tmp目录，这个目录是Linux下的临时文件夹，程序运行完，生成flag的txt文件被清理了，所以我们找不到文件</p>
<p>我们这时候通过IDA查看汇编代码，按空格键可以生成所有的汇编文件</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/4.png" alt="img"></p>
<p>然后我们向下追踪，追踪到for循环的位置，因为，flag是在这里存入文件的，所以，我们可以在内存中找到正要存储的字符串</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/5.png" alt="img"></p>
<p>我们将鼠标指向strlen(),在下面可以看到汇编所在的地址，然后我们根据大概的地址去看汇编代码</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/6.png" alt="img"></p>
<p>可以看到这是调用strlen()函数的汇编指令</p>
<p>我们通过上一个图片，可以知道经过for()的判断条件后，还要进行一步fseek函数，所以，根据汇编代码，可以确定jnb loc_4008B5就是fseek()函数，那么，mov eax,[rbp+var_3C]肯定就是最后要得到的flag了</p>
<p>0x04.GDB</p>
<p>这里我们用linux下的动态调试工具gdb进行动态调试，这里介绍一下，对gdb进行强化的两个工具peda和pwndbg，这两个工具可以强化视觉效果，可以更加清楚的显示堆栈，内存，寄存机的情况</p>
<p>先加载程序</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/7.png" alt="img"></p>
<p>然后，用b 下断点</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/8.png" alt="img"></p>
<p>然后，运行 R</p>
<p><img src="https://adworld.xctf.org.cn/media/task/writeup/cn/getit/9.png" alt="img"></p>
<p>这里我们可以看出，程序停止在0x400832的位置，然后，要被移动的字符串在RDX的位置</p>
<p>注：</p>
<p>这里介绍一下一下RDX，RDX存的是i/0指针，0x6010e0,这个位置存的字符串是最后的flag:SharifCTF{b70c59275fcfa8aebf2d5911223c6589}</p>
<p>以为这里涉及的是程序读写函数，所以涉及的就是i/o指针</p>
<p>所以我们能得到最后的flag: SharifCTF{b70c59275fcfa8aebf2d5911223c6589}</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; &quot;c61b68366edeb7bdce3c6820314b7498&quot;</span><br><span class="line">v3 &#x3D; 0</span><br><span class="line">v5 &#x3D; 0</span><br><span class="line">t &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">while v5 &lt; len(s):</span><br><span class="line">    if v5 &amp; 1:</span><br><span class="line">        v3 &#x3D; 1</span><br><span class="line">    else:</span><br><span class="line">        v3 &#x3D; -1</span><br><span class="line">    t +&#x3D; chr(ord(s[v5]) + v3)</span><br><span class="line">    v5 +&#x3D; 1</span><br><span class="line"></span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>



<h2 id="csaw2013reversing2"><a href="#csaw2013reversing2" class="headerlink" title="csaw2013reversing2"></a>csaw2013reversing2</h2><p><a href="https://blog.csdn.net/weixin_43784056/article/details/103655968">XCTF-csaw2013reversing2_臭nana的博客-CSDN博客</a></p>
<p>默认if条件不成立，跳过了sub_401000的解码，输出一堆乱码。</p>
<h2 id="maze"><a href="#maze" class="headerlink" title="maze"></a>maze</h2><p>迷宫问题</p>
<p><img src="https://www.pianshen.com/images/271/7d91ebf895b8ed648ed3ab80de463137.JPEG" alt="在这里插入图片描述"></p>
]]></content>
      <categories>
        <category>CTF</category>
      </categories>
      <tags>
        <tag>XCTF</tag>
        <tag>逆向</tag>
      </tags>
  </entry>
  <entry>
    <title>建造第一个神经网络</title>
    <url>/YingYingMonstre.github.io/2021/12/11/%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h3 id="关系拟合-回归"><a href="#关系拟合-回归" class="headerlink" title="关系拟合 (回归)"></a>关系拟合 (回归)</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>我会这次会来见证神经网络是如何通过简单的形式将一群数据用一条线条来表示。或者说，是如何在数据当中找到他们的关系，然后用神经网络模型来建立一个可以代表他们关系的线条。</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-2.gif"></p>
<h4 id="建立数据集"><a href="#建立数据集" class="headerlink" title="建立数据集"></a>建立数据集</h4><p>我们创建一些假数据来模拟真实的情况。比如一个一元二次函数: <code>y = a * x^2 + b</code>，我们给 <code>y</code> 数据加上一点噪声来更加真实的展示它。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())                 <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="建立神经网络"><a href="#建立神经网络" class="headerlink" title="建立神经网络"></a>建立神经网络</h4><p>建立一个神经网络我们可以直接运用 torch 中的体系。先定义所有的层属性(<code>__init__()</code>)，然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接。建立关系的时候，我们会用到激励函数，如果还不清楚激励函数用途的同学，这里有非常好的<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">一篇动画教程</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span>  <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        <span class="comment"># 定义每层用什么样的形式</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span>   <span class="comment"># 这同时也是 Module 中的 forward 功能</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># 输出值</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">1</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练的步骤很简单，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line">loss_func = torch.nn.MSELoss()      <span class="comment"># 预测值和真实值的误差计算公式 (均方差)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    prediction = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出预测值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>



<h4 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h4><p>为了可视化整个训练的过程，更好的理解是如何训练，我们如下操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># plot and show learning process</span></span><br><span class="line">        plt.cla()</span><br><span class="line">        plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">        plt.plot(x.data.numpy(), prediction.data.numpy(), <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">5</span>)</span><br><span class="line">        plt.text(<span class="number">0.5</span>, <span class="number">0</span>, <span class="string">&#x27;Loss=%.4f&#x27;</span> % loss.data.numpy(), fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/3-1-1.png"></p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="区分类型-分类"><a href="#区分类型-分类" class="headerlink" title="区分类型 (分类)"></a>区分类型 (分类)</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>这次我们也是用最简单的途径来看看神经网络是怎么进行事物的分类。</p>
<p><img src="https://static.mofanpy.com/results/torch/1-1-3.gif"></p>
<h4 id="建立数据集-1"><a href="#建立数据集-1" class="headerlink" title="建立数据集"></a>建立数据集</h4><p>我们创建一些假数据来模拟真实的情况。比如两个二次分布的数据，不过他们的均值都不一样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">n_data = torch.ones(<span class="number">100</span>, <span class="number">2</span>)         <span class="comment"># 数据的基本形态</span></span><br><span class="line">x0 = torch.normal(<span class="number">2</span>*n_data, <span class="number">1</span>)      <span class="comment"># 类型0 x data (tensor), shape=(100, 2)</span></span><br><span class="line">y0 = torch.zeros(<span class="number">100</span>)               <span class="comment"># 类型0 y data (tensor), shape=(100, )</span></span><br><span class="line">x1 = torch.normal(-<span class="number">2</span>*n_data, <span class="number">1</span>)     <span class="comment"># 类型1 x data (tensor), shape=(100, 1)</span></span><br><span class="line">y1 = torch.ones(<span class="number">100</span>)                <span class="comment"># 类型1 y data (tensor), shape=(100, )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</span></span><br><span class="line">x = torch.cat((x0, x1), <span class="number">0</span>).<span class="built_in">type</span>(torch.FloatTensor)  <span class="comment"># FloatTensor = 32-bit floating</span></span><br><span class="line">y = torch.cat((y0, y1), ).<span class="built_in">type</span>(torch.LongTensor)    <span class="comment"># LongTensor = 64-bit integer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap=&#x27;RdYlGn&#x27;)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.scatter(x.data.numpy(), y.data.numpy())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<h4 id="建立神经网络-1"><a href="#建立神经网络-1" class="headerlink" title="建立神经网络"></a>建立神经网络</h4><p>建立一个神经网络我们可以直接运用 torch 中的体系。先定义所有的层属性(<code>__init__()</code>)，然后再一层层搭建(<code>forward(x)</code>)层于层的关系链接。这个和我们在前面 regression 的时候的神经网络基本没差。建立关系的时候，我们会用到激励函数，如果还不清楚激励函数用途的同学，这里有非常好的<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-activation-function">一篇动画教程</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F     <span class="comment"># 激励函数都在这</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span>     <span class="comment"># 继承 torch 的 Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()     <span class="comment"># 继承 __init__ 功能</span></span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   <span class="comment"># 隐藏层线性输出</span></span><br><span class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       <span class="comment"># 输出层线性输出</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 正向传播输入值, 神经网络分析出输出值</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># 激励函数(隐藏层的线性值)</span></span><br><span class="line">        x = self.out(x)                 <span class="comment"># 输出值, 但是这个不是预测值, 预测值还需要再另外计算</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net(n_feature=<span class="number">2</span>, n_hidden=<span class="number">10</span>, n_output=<span class="number">2</span>) <span class="comment"># 几个类别就几个 output</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)  <span class="comment"># net 的结构</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (2 -&gt; 10)</span></span><br><span class="line"><span class="string">  (out): Linear (10 -&gt; 2)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="训练网络-1"><a href="#训练网络-1" class="headerlink" title="训练网络"></a>训练网络</h4><p>训练的步骤很简单，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># optimizer 是训练的工具</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.02</span>)  <span class="comment"># 传入 net 的所有参数, 学习率</span></span><br><span class="line"><span class="comment"># 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)</span></span><br><span class="line"><span class="comment"># 但是预测值是2D tensor (batch, n_classes)</span></span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    out = net(x)     <span class="comment"># 喂给 net 训练数据 x, 输出分析值</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(out, y)     <span class="comment"># 计算两者的误差</span></span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 清空上一步的残余更新参数值</span></span><br><span class="line">    loss.backward()         <span class="comment"># 误差反向传播, 计算参数更新值</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 将参数更新值施加到 net 的 parameters 上</span></span><br></pre></td></tr></table></figure>



<h4 id="可视化训练过程-1"><a href="#可视化训练过程-1" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h4><p>为了可视化整个训练的过程，更好的理解是如何训练，我们如下操作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 画图</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着上面来</span></span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        plt.cla()</span><br><span class="line">        <span class="comment"># 过了一道 softmax 的激励函数后的最大概率才是预测值</span></span><br><span class="line">        prediction = torch.<span class="built_in">max</span>(F.softmax(out), <span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        pred_y = prediction.data.numpy().squeeze()</span><br><span class="line">        target_y = y.data.numpy()</span><br><span class="line">        plt.scatter(x.data.numpy()[:, <span class="number">0</span>], x.data.numpy()[:, <span class="number">1</span>], c=pred_y, s=<span class="number">100</span>, lw=<span class="number">0</span>, cmap=<span class="string">&#x27;RdYlGn&#x27;</span>)</span><br><span class="line">        accuracy = <span class="built_in">sum</span>(pred_y == target_y)/<span class="number">200.</span>  <span class="comment"># 预测中有多少和真实值一样</span></span><br><span class="line">        plt.text(<span class="number">1.5</span>, -<span class="number">4</span>, <span class="string">&#x27;Accuracy=%.2f&#x27;</span> % accuracy, fontdict=&#123;<span class="string">&#x27;size&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;color&#x27;</span>:  <span class="string">&#x27;red&#x27;</span>&#125;)</span><br><span class="line">        plt.pause(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">plt.ioff()  <span class="comment"># 停止画图</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/3-2-1.png"></p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="快速搭建法"><a href="#快速搭建法" class="headerlink" title="快速搭建法"></a>快速搭建法</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>Torch 中提供了很多方便的途径，同样是神经网络，能快则快，我们看看如何用更简单的方式搭建同样的回归神经网络。</p>
<h4 id="快速搭建"><a href="#快速搭建" class="headerlink" title="快速搭建"></a>快速搭建</h4><p>我们先看看之前写神经网络时用到的步骤。我们用 <code>net1</code> 代表这种方式搭建的神经网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net1 = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)   <span class="comment"># 这是我们用这种方式搭建的 net1</span></span><br></pre></td></tr></table></figure>

<p>我们用 class 继承了一个 torch 中的神经网络结构，然后对其进行了修改，不过还有更快的一招，用一句话就概括了上面所有的内容！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net2 = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们再对比一下两者的结构：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(net1)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (hidden): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (predict): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(net2)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Sequential (</span></span><br><span class="line"><span class="string">  (0): Linear (1 -&gt; 10)</span></span><br><span class="line"><span class="string">  (1): ReLU ()</span></span><br><span class="line"><span class="string">  (2): Linear (10 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们会发现 <code>net2</code> 多显示了一些内容，这是为什么呢？原来他把激励函数也一同纳入进去了，但是 <code>net1</code> 中，激励函数实际上是在 <code>forward()</code> 功能中才被调用的。这也就说明了，相比 <code>net2</code>，<code>net1</code> 的好处就是，你可以根据你的个人需要更加个性化你自己的前向传播过程，比如(RNN)。不过如果你不需要七七八八的过程，相信 <code>net2</code> 这种形式更适合你。</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="保存提取"><a href="#保存提取" class="headerlink" title="保存提取"></a>保存提取</h3><h4 id="要点-3"><a href="#要点-3" class="headerlink" title="要点"></a>要点</h4><p>训练好了一个模型，我们当然想要保存它，留到下次要用的时候直接提取直接用，这就是这节的内容啦。我们用回归的神经网络举例实现保存提取。</p>
<h4 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h4><p>我们快速地建造数据，搭建网络：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假数据</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>), dim=<span class="number">1</span>)  <span class="comment"># x data (tensor), shape=(100, 1)</span></span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.2</span>*torch.rand(x.size())  <span class="comment"># noisy y data (tensor), shape=(100, 1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save</span>():</span></span><br><span class="line">    <span class="comment"># 建网络</span></span><br><span class="line">    net1 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    optimizer = torch.optim.SGD(net1.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        prediction = net1(x)</span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>

<p>接下来我们有两种途径来保存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(net1, <span class="string">&#x27;net.pkl&#x27;</span>)  <span class="comment"># 保存整个网络</span></span><br><span class="line">torch.save(net1.state_dict(), <span class="string">&#x27;net_params.pkl&#x27;</span>)   <span class="comment"># 只保存网络中的参数 (速度快, 占内存少)</span></span><br></pre></td></tr></table></figure>



<h4 id="提取网络"><a href="#提取网络" class="headerlink" title="提取网络"></a>提取网络</h4><p>这种方式将会提取整个神经网络，网络大的时候可能会比较慢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_net</span>():</span></span><br><span class="line">    <span class="comment"># restore entire net1 to net2</span></span><br><span class="line">    net2 = torch.load(<span class="string">&#x27;net.pkl&#x27;</span>)</span><br><span class="line">    prediction = net2(x)</span><br></pre></td></tr></table></figure>



<h4 id="只提取网络参数"><a href="#只提取网络参数" class="headerlink" title="只提取网络参数"></a>只提取网络参数</h4><p>这种方式将会提取所有的参数，然后再放到你的新建网络中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_params</span>():</span></span><br><span class="line">    <span class="comment"># 新建 net3</span></span><br><span class="line">    net3 = torch.nn.Sequential(</span><br><span class="line">        torch.nn.Linear(<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">        torch.nn.ReLU(),</span><br><span class="line">        torch.nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将保存的参数复制到 net3</span></span><br><span class="line">    net3.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>))</span><br><span class="line">    prediction = net3(x)</span><br></pre></td></tr></table></figure>



<h4 id="显示结果"><a href="#显示结果" class="headerlink" title="显示结果"></a>显示结果</h4><p>调用上面建立的几个功能，然后出图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存 net1 (1. 整个网络, 2. 只有参数)</span></span><br><span class="line">save()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取整个网络</span></span><br><span class="line">restore_net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取网络参数, 复制到新网络</span></span><br><span class="line">restore_params()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/3-4-1.png"></p>
<p>这样我们就能看出三个网络完全一模一样啦。</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/304_save_reload.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="批训练"><a href="#批训练" class="headerlink" title="批训练"></a>批训练</h3><h4 id="要点-4"><a href="#要点-4" class="headerlink" title="要点"></a>要点</h4><p>Torch 中提供了一种帮你整理你的数据结构的好东西，叫做 <code>DataLoader</code>，我们能用它来包装自己的数据，进行批训练。而且批训练可以有很多种途径，详情请见 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-speed-up-learning">我制作的 训练优化器 动画简介</a>。</p>
<h4 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h4><p><code>DataLoader</code> 是 torch 给你用来包装你的数据的工具。所以你要将自己的 (numpy array 或其他) 数据形式转换成 Tensor，然后再放进这个包装器中。使用 <code>DataLoader</code> 有什么好处呢？就是他们帮你有效地迭代数据，举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">5</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>)       <span class="comment"># x data (torch tensor)</span></span><br><span class="line">y = torch.linspace(<span class="number">10</span>, <span class="number">1</span>, <span class="number">10</span>)       <span class="comment"># y data (torch tensor)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先转换成 torch 能识别的 Dataset</span></span><br><span class="line">torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 dataset 放入 DataLoader</span></span><br><span class="line">loader = Data.DataLoader(</span><br><span class="line">    dataset=torch_dataset,      <span class="comment"># torch TensorDataset format</span></span><br><span class="line">    batch_size=BATCH_SIZE,      <span class="comment"># mini batch size</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,               <span class="comment"># 要不要打乱数据 (打乱比较好)</span></span><br><span class="line">    num_workers=<span class="number">2</span>,              <span class="comment"># 多线程来读数据</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):   <span class="comment"># 训练所有!整套!数据 3 次</span></span><br><span class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):  <span class="comment"># 每一步 loader 释放一小批数据用来学习</span></span><br><span class="line">        <span class="comment"># 假设这里就是你训练的地方...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打出来一些数据</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| Step: &#x27;</span>, step, <span class="string">&#x27;| batch x: &#x27;</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">&#x27;| batch y: &#x27;</span>, batch_y.numpy())</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  0 | batch x:  [ 6.  7.  2.  3.  1.] | batch y:  [  5.   4.   9.   8.  10.]</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  1 | batch x:  [  9.  10.   4.   8.   5.] | batch y:  [ 2.  1.  7.  3.  6.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.] | batch y:  [ 8.  7.  9.  2.  1.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  1 | batch x:  [ 1.  7.  8.  5.  6.] | batch y:  [ 10.   4.   3.   6.   5.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  0 | batch x:  [ 3.  9.  2.  6.  7.] | batch y:  [ 8.  2.  9.  5.  4.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  1 | batch x:  [ 10.   4.   8.   1.   5.] | batch y:  [  1.   7.   3.  10.   6.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以看出，每步都导出了5个数据进行学习。然后每个 epoch 的导出数据都是先打乱了以后再导出。</p>
<p>真正方便的还不是这点。如果我们改变一下 <code>BATCH_SIZE = 8</code>，这样我们就知道，<code>step=0</code> 会导出8个数据，但是，<code>step=1</code> 时数据库中的数据不够 8个，这时怎么办呢：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">8</span>      <span class="comment"># 批训练的数据个数</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ...:</span><br><span class="line">    <span class="keyword">for</span> ...:</span><br><span class="line">        ...</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch, <span class="string">&#x27;| Step: &#x27;</span>, step, <span class="string">&#x27;| batch x: &#x27;</span>,</span><br><span class="line">              batch_x.numpy(), <span class="string">&#x27;| batch y: &#x27;</span>, batch_y.numpy())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  0 | batch x:  [  6.   7.   2.   3.   1.   9.  10.   4.] | batch y:  [  5.   4.   9.   8.  10.   2.   1.   7.]</span></span><br><span class="line"><span class="string">Epoch:  0 | Step:  1 | batch x:  [ 8.  5.] | batch y:  [ 3.  6.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.   1.   7.   8.] | batch y:  [  8.   7.   9.   2.   1.  10.   4.   3.]</span></span><br><span class="line"><span class="string">Epoch:  1 | Step:  1 | batch x:  [ 5.  6.] | batch y:  [ 6.  5.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  0 | batch x:  [  3.   9.   2.   6.   7.  10.   4.   8.] | batch y:  [ 8.  2.  9.  5.  4.  1.  7.  3.]</span></span><br><span class="line"><span class="string">Epoch:  2 | Step:  1 | batch x:  [ 1.  5.] | batch y:  [ 10.   6.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>这时，在 <code>step=1</code> 就只给你返回这个 epoch 中剩下的数据就好了。</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/305_batch_train.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="加速神经网络训练-Speed-Up-Training"><a href="#加速神经网络训练-Speed-Up-Training" class="headerlink" title="加速神经网络训练 (Speed Up Training)"></a>加速神经网络训练 (Speed Up Training)</h3><p>今天我们会来聊聊在怎么样加速你的神经网络训练过程。</p>
<p>包括以下几种模式:</p>
<ul>
<li>Stochastic Gradient Descent (SGD)</li>
<li>Momentum</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup1.png"></p>
<p>越复杂的神经网络、越多的数据，我们需要在训练神经网络的过程上花费的时间也就越多。原因很简单，就是因为计算量太大了。可是往往有时候为了解决复杂的问题，复杂的结构和大数据又是不能避免的，所以我们需要寻找一些方法，让神经网络聪明起来，快起来。</p>
<h4 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup2.png"></p>
<p>所以，最基础的方法就是 SGD 啦，想像红色方块是我们要训练的 data，如果用普通的训练方法，就需要重复不断的把整套数据放入神经网络 NN训练，这样消耗的计算资源会很大。</p>
<p>我们换一种思路，如果把这些数据拆分成小批小批的，然后再分批不断放入 NN 中计算，这就是我们常说的 SGD 的正确打开方式了。每次使用批数据，虽然不能反映整体数据的情况，不过却很大程度上加速了 NN 的训练过程，而且也不会丢失太多准确率。如果运用上了 SGD，你还是嫌训练速度慢，那怎么办？</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/speedup3.png"></p>
<p>没问题，事实证明，SGD 并不是最快速的训练方法，红色的线是 SGD，但它到达学习目标的时间是在这些方法中最长的一种。我们还有很多其他的途径来加速训练。</p>
<h4 id="Momentum-更新方法"><a href="#Momentum-更新方法" class="headerlink" title="Momentum 更新方法"></a>Momentum 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup4.png"></p>
<p>大多数其他途径是在更新神经网络参数那一步上动动手脚。传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx)。这种方法可能会让学习过程曲折无比，看起来像喝醉的人回家时, 摇摇晃晃走了很多弯路。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup5.png"></p>
<p>所以我们把这个人从平地上放到了一个斜坡上，只要他往下坡的方向走一点点，由于向下的惯性，他不自觉地就一直往下走，走的弯路也变少了。这就是 Momentum 参数更新。另外一种加速方法叫AdaGrad。</p>
<h4 id="AdaGrad-更新方法"><a href="#AdaGrad-更新方法" class="headerlink" title="AdaGrad 更新方法"></a>AdaGrad 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup6.png"></p>
<p>这种方法是在学习率上面动手脚，使得每一个参数更新都会有自己与众不同的学习率，他的作用和 momentum 类似，不过不是给喝醉酒的人安排另一个下坡，而是给他一双不好走路的鞋子，使得他一摇晃着走路就脚疼，鞋子成为了走弯路的阻力，逼着他往前直着走。他的数学形式是这样的。接下来又有什么方法呢？如果把下坡和不好走路的鞋子合并起来，是不是更好呢？没错，这样我们就有了 RMSProp 更新方法。</p>
<h4 id="RMSProp-更新方法"><a href="#RMSProp-更新方法" class="headerlink" title="RMSProp 更新方法"></a>RMSProp 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup7.png"></p>
<p>有了 momentum 的惯性原则，加上 adagrad 的对错误方向的阻力，我们就能合并成这样。让 RMSProp同时具备他们两种方法的优势。不过细心的同学们肯定看出来了，似乎在 RMSProp 中少了些什么。原来是我们还没把 Momentum合并完全，RMSProp 还缺少了 momentum 中的这一部分。所以，我们在 Adam 方法中补上了这种想法。</p>
<h4 id="Adam-更新方法"><a href="#Adam-更新方法" class="headerlink" title="Adam 更新方法"></a>Adam 更新方法</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/speedup8.png"></p>
<p>计算m 时有 momentum 下坡的属性，计算 v 时有 adagrad 阻力的属性，然后再更新参数时把 m 和 V 都考虑进去。实验证明，大多数时候，使用 adam 都能又快又好的达到目标，迅速收敛。所以说，在加速神经网络训练的时候，一个下坡，一双破鞋子，功不可没。</p>
<h3 id="Optimizer-优化器"><a href="#Optimizer-优化器" class="headerlink" title="Optimizer 优化器"></a>Optimizer 优化器</h3><h4 id="要点-5"><a href="#要点-5" class="headerlink" title="要点"></a>要点</h4><p>这节内容主要是用 Torch 实践 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-speed-up-learning">这个 优化器 动画简介</a> 中起到的几种优化器，这几种优化器具体的优势不会在这个节内容中说了，所以想快速了解的话，上面的那个动画链接是很好的去处。</p>
<p>下图就是这节内容对比各种优化器的效果：</p>
<p><img src="https://static.mofanpy.com/results-small/torch/3-6-2.png"></p>
<h4 id="伪数据"><a href="#伪数据" class="headerlink" title="伪数据"></a>伪数据</h4><p>为了对比各种优化器的效果，我们需要有一些数据，今天我们还是自己编一些伪数据，这批数据是这样的：</p>
<p><img src="https://static.mofanpy.com/results/torch/3-6-1.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fake dataset</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)</span><br><span class="line">y = x.<span class="built_in">pow</span>(<span class="number">2</span>) + <span class="number">0.1</span>*torch.normal(torch.zeros(*x.size()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot dataset</span></span><br><span class="line">plt.scatter(x.numpy(), y.numpy())</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用上节内容提到的 data loader</span></span><br><span class="line">torch_dataset = Data.TensorDataset(x, y)</span><br><span class="line">loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>,)</span><br></pre></td></tr></table></figure>



<h4 id="每个优化器优化一个神经网络"><a href="#每个优化器优化一个神经网络" class="headerlink" title="每个优化器优化一个神经网络"></a>每个优化器优化一个神经网络</h4><p>为了对比每一种优化器，我们给他们各自创建一个神经网络，但这个神经网络都来自同一个 <code>Net</code> 形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 默认的 network 形式</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(<span class="number">1</span>, <span class="number">20</span>)   <span class="comment"># hidden layer</span></span><br><span class="line">        self.predict = torch.nn.Linear(<span class="number">20</span>, <span class="number">1</span>)   <span class="comment"># output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))      <span class="comment"># activation function for hidden layer</span></span><br><span class="line">        x = self.predict(x)             <span class="comment"># linear output</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个优化器创建一个 net</span></span><br><span class="line">net_SGD         = Net()</span><br><span class="line">net_Momentum    = Net()</span><br><span class="line">net_RMSprop     = Net()</span><br><span class="line">net_Adam        = Net()</span><br><span class="line">nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]</span><br></pre></td></tr></table></figure>



<h4 id="优化器-Optimizer"><a href="#优化器-Optimizer" class="headerlink" title="优化器 Optimizer"></a>优化器 Optimizer</h4><p>接下来在创建不同的优化器，用来训练不同的网络。并创建一个 <code>loss_func</code> 用来计算误差。我们用几种常见的优化器，<code>SGD</code>，<code>Momentum</code>，<code>RMSprop</code>，<code>Adam</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># different optimizers</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</span><br><span class="line"></span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line">losses_his = [[], [], [], []]   <span class="comment"># 记录 training 时不同神经网络的 loss</span></span><br></pre></td></tr></table></figure>



<h4 id="训练-出图"><a href="#训练-出图" class="headerlink" title="训练/出图"></a>训练/出图</h4><p>接下来训练和 loss 画图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#x27;</span>, epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对每个优化器, 优化属于他的神经网络</span></span><br><span class="line">        <span class="keyword">for</span> net, opt, l_his <span class="keyword">in</span> <span class="built_in">zip</span>(nets, optimizers, losses_his):</span><br><span class="line">            output = net(b_x)              <span class="comment"># get output for every net</span></span><br><span class="line">            loss = loss_func(output, b_y)  <span class="comment"># compute loss for every net</span></span><br><span class="line">            opt.zero_grad()                <span class="comment"># clear gradients for next train</span></span><br><span class="line">            loss.backward()                <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">            opt.step()                     <span class="comment"># apply gradients</span></span><br><span class="line">            l_his.append(loss.data.numpy())     <span class="comment"># loss recoder</span></span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/3-6-2.png"></p>
<p><code>SGD</code> 是最普通的优化器，也可以说没有加速效果，而 <code>Momentum</code> 是 <code>SGD</code> 的改良版，它加入了动量原则。后面的 <code>RMSprop</code> 又是 <code>Momentum</code> 的升级版。而 <code>Adam</code> 又是 <code>RMSprop</code> 的升级版。不过从这个结果中我们看到，<code>Adam</code> 的效果似乎比 <code>RMSprop</code> 要差一点。所以说并不是越先进的优化器，结果越佳。我们在自己的试验中可以尝试不同的优化器，找到那个最适合你数据/网络的优化器。</p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py">github 代码</a> 中的每一步的意义啦。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/301_regression.py">第一节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/301_simple_regression.py">用 Tensorflow 达到第一节同样效果的代码</a></li>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/302_classification.py">第二节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/302_simple_classification.py">用 Tensorflow 达到第二节同样效果的代码</a></li>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/303_build_nn_quickly.py">第三节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/304_save_reload.py">第四节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/305_batch_train.py">第五节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/306_optimizer.py">第六节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/304_optimizer.py">Tensorflow 的 Optimizer 代码</a></li>
<li><a href="http://pytorch.org/docs/optim.html">PyTorch 优化器网页</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/303_save_reload.py">Tensorflow 的保存读取代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-NN">我制作的 什么是神经网络 动画简介</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-speed-up-learning">我制作的 训练优化器 动画简介</a></li>
<li>英文学习<a href="http://sebastianruder.com/optimizing-gradient-descent/">资料</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/optimizer">PyTorch 可视化优化器</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch 神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（DQN上）</title>
    <url>/YingYingMonstre.github.io/2021/11/30/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DQN%E4%B8%8A%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-DQN"><a href="#什么是-DQN" class="headerlink" title="什么是 DQN"></a>什么是 DQN</h3><p>今天我们会来说说强化学习中的一种强大武器，Deep Q Network 简称为 DQN。Google Deep mind 团队就是靠着这 DQN 使计算机玩电动玩得比我们还厉害。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<h4 id="强化学习与神经网络"><a href="#强化学习与神经网络" class="headerlink" title="强化学习与神经网络"></a>强化学习与神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN1.png"></p>
<p>之前我们所谈论到的强化学习方法都是比较传统的方式，而如今，随着机器学习在日常生活中的各种应用，各种机器学习方法也在融汇、合并、升级。而我们今天所要探讨的强化学习则是这么一种融合了神经网络和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 的方法，名字叫做 Deep Q Network。这种新型结构是为什么被提出来呢？原来，传统的表格形式的强化学习有这样一个瓶颈。</p>
<h4 id="神经网络的作用"><a href="#神经网络的作用" class="headerlink" title="神经网络的作用"></a>神经网络的作用</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN2.png"></p>
<p>我们使用表格来存储每一个状态 state，和在这个 state 每个行为 action 所拥有的 Q 值。而当今问题是在太复杂，状态可以多到比天上的星星还多(比如下围棋)。如果全用表格来存储它们，恐怕我们的计算机有再大的内存都不够，而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事。不过，在机器学习中，有一种方法对这种事情很在行，那就是神经网络。我们可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样我们就没必要在表格中记录 Q 值，而是直接使用神经网络生成 Q 值。还有一种形式的是这样，我们也能只输入状态值，输出所有的动作值，然后按照 Q learning 的原则，直接选择拥有最大值的动作当做下一步要做的动作。我们可以想象，神经网络接受外部的信息，相当于眼睛鼻子耳朵收集信息，然后通过大脑加工输出每种动作的值，最后通过强化学习的方式选择动作。</p>
<h4 id="更新神经网络"><a href="#更新神经网络" class="headerlink" title="更新神经网络"></a>更新神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN3.png"></p>
<p>接下来我们基于第二种神经网络来分析，我们知道，神经网络是要被训练才能预测出准确的值。那在强化学习中，神经网络是如何被训练的呢？首先，我们需要 a1、a2 正确的Q值，这个 Q 值我们就用之前在 Q learning 中的 Q 现实来代替。同样我们还需要一个 Q 估计来实现神经网络的更新。所以神经网络的的参数就是老的 NN 参数加学习率 alpha 乘以 Q 现实和 Q 估计的差距。我们整理一下。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/DQN4.png"></p>
<p>我们通过 NN 预测出Q(s2, a1) 和 Q(s2,a2) 的值，这就是 Q 估计。然后我们选取 Q 估计中最大值的动作来换取环境中的奖励 reward。而 Q 现实中也包含从神经网络分析出来的两个 Q 估计值，不过这个 Q 估计是针对于下一步在 s’ 的估计。最后再通过刚刚所说的算法更新神经网络中的参数。但是这并不是 DQN 会玩电动的根本原因。还有两大因素支撑着 DQN 使得它变得无比强大。这两大因素就是 Experience replay 和 Fixed Q-targets。</p>
<h4 id="DQN-两大利器"><a href="#DQN-两大利器" class="headerlink" title="DQN 两大利器"></a>DQN 两大利器</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN5.png"></p>
<p>简单来说，DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过，Q learning 是一种 off-policy 离线学习法，它能学习当前经历着的，也能学习过去经历过的，甚至是学习别人的经历。所以每次 DQN 更新的时候，我们都可以随机抽取一些之前的经历进行学习。随机抽取这种做法打乱了经历之间的相关性，也使得神经网络更新更有效率。Fixed Q-targets 也是一种打乱相关性的机理，如果使用 fixed Q-targets，我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络，预测 Q 估计的神经网络具备最新的参数，而预测 Q 现实的神经网络使用的参数则是很久以前的。有了这两种提升手段，DQN 才能在一些游戏中超越人类。</p>
<h3 id="DQN-算法更新"><a href="#DQN-算法更新" class="headerlink" title="DQN 算法更新"></a>DQN 算法更新</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>Deep Q Network 的简称叫 DQN，是将 Q learning 的优势 和 Neural networks 结合了。如果我们使用 tabular Q learning，对于每一个 state，action 我们都需要存放在一张 q_table 的表中。如果像显示生活中，情况可就比那个迷宫的状况复杂多了，我们有千千万万个 state，如果将这千万个 state 的值都放在表中，受限于我们计算机硬件，这样从表中获取数据，更新数据是没有效率的。这就是 DQN 产生的原因了。我们可以使用神经网络来 估算这个 state 的值，这样就不需要一张表了。</p>
<p>这次的教程我们还是基于熟悉的迷宫环境，重点在实现 DQN 算法，之后我们再拿着做好的 DQN 算法去跑其他更有意思的环境。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-1-1.jpg"></p>
<p>整个算法乍看起来很复杂，不过我们拆分一下，就变简单了。也就是个 Q learning 主框架上加了些装饰。</p>
<p>这些装饰包括：</p>
<ul>
<li>记忆库 (用于重复学习)</li>
<li>神经网络计算 Q 值</li>
<li>暂时冻结 <code>q_target</code> 参数 (切断相关性)</li>
</ul>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>接下来我们对应上面的算法，来实现主循环。首先 import 所需模块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DeepQNetwork</span><br></pre></td></tr></table></figure>

<p>下面的代码，就是 DQN 于环境交互最重要的部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_maze</span>():</span></span><br><span class="line">    step = <span class="number">0</span>    <span class="comment"># 用来控制什么时候学习</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据行为给出下一个 state, reward, 是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 存储记忆</span></span><br><span class="line">            RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 控制学习起始时间和频率 (先累积一些记忆再开始学习)</span></span><br><span class="line">            <span class="keyword">if</span> (step &gt; <span class="number">200</span>) <span class="keyword">and</span> (step % <span class="number">5</span> == <span class="number">0</span>):</span><br><span class="line">                RL.learn()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state_ 变为 下次循环的 state</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            step += <span class="number">1</span>   <span class="comment"># 总步数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># end of game</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = DeepQNetwork(env.n_actions, env.n_features,</span><br><span class="line">                      learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                      reward_decay=<span class="number">0.9</span>,</span><br><span class="line">                      e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                      replace_target_iter=<span class="number">200</span>,  <span class="comment"># 每 200 步替换一次 target_net 的参数</span></span><br><span class="line">                      memory_size=<span class="number">2000</span>, <span class="comment"># 记忆上限</span></span><br><span class="line">                      <span class="comment"># output_graph=True   # 是否输出 tensorboard 文件</span></span><br><span class="line">                      )</span><br><span class="line">    env.after(<span class="number">100</span>, run_maze)</span><br><span class="line">    env.mainloop()</span><br><span class="line">    RL.plot_cost()  <span class="comment"># 观看神经网络的误差曲线</span></span><br></pre></td></tr></table></figure>

<p>下一节我们会来讲解 <code>DeepQNetwork</code> 这种算法具体要怎么编。</p>
<h3 id="DQN-神经网络"><a href="#DQN-神经网络" class="headerlink" title="DQN 神经网络"></a>DQN 神经网络</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>接着上节内容，这节我们使用 Tensorflow (如果还不了解 Tensorflow，这里去往 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow">经典的 Tensorflow 视频教程</a>) 来搭建 DQN 当中的神经网络部分 (用来预测 Q 值)。</p>
<h4 id="两个神经网络"><a href="#两个神经网络" class="headerlink" title="两个神经网络"></a>两个神经网络</h4><p>为了使用 Tensorflow 来实现 DQN，比较推荐的方式是搭建两个神经网络，<code>target_net</code> 用于预测 <code>q_target</code> 值，它不会及时更新参数。<code>eval_net</code> 用于预测 <code>q_eval</code>，这个神经网络拥有最新的神经网络参数。不过这两个神经网络结构是完全一样的，只是里面的参数不一样。在<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">这个短视频里</a>，能找到我们为什么要建立两个不同参数的神经网络。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-2-1.png"></p>
<h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>因为 DQN 的结构相比之前所讲的内容都不一样，所以我们不使用继承来实现这次的功能。这次我们创建一个 <code>DeepQNetwork</code> 的 class，以及他神经网络部分的功能。下次再说强化学习的更新部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="comment"># 建立神经网络</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="创建两个网络"><a href="#创建两个网络" class="headerlink" title="创建两个网络"></a>创建两个网络</h4><p>两个神经网络是为了固定住一个神经网络 (<code>target_net</code>) 的参数，<code>target_net</code> 是 <code>eval_net</code> 的一个历史版本，拥有 <code>eval_net</code> 很久之前的一组参数，而且这组参数被固定一段时间，然后再被 <code>eval_net</code> 的新参数所替换。而 <code>eval_net</code> 是不断在被提升的，所以是一个可以被训练的网络 <code>trainable=True</code>。而 <code>target_net</code> 的 <code>trainable=False</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># -------------- 创建 eval 神经网络, 及时提升参数 --------------</span></span><br><span class="line">        self.s = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s&#x27;</span>)  <span class="comment"># 用来接收 observation</span></span><br><span class="line">        self.q_target = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_actions], name=<span class="string">&#x27;Q_target&#x27;</span>) <span class="comment"># 用来接收 q_target 的值, 这个之后会通过计算得到</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;eval_net&#x27;</span>):</span><br><span class="line">            <span class="comment"># c_names(collections_names) 是在更新 target_net 参数时会用到</span></span><br><span class="line">            c_names, n_l1, w_initializer, b_initializer = \</span><br><span class="line">                [<span class="string">&#x27;eval_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES], <span class="number">10</span>, \</span><br><span class="line">                tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>), tf.constant_initializer(<span class="number">0.1</span>)  <span class="comment"># config of layers</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># eval_net 的第一层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># eval_net 的第二层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l2&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                self.q_eval = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;loss&#x27;</span>): <span class="comment"># 求误差</span></span><br><span class="line">            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;train&#x27;</span>):    <span class="comment"># 梯度下降</span></span><br><span class="line">            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ---------------- 创建 target 神经网络, 提供 target Q ---------------------</span></span><br><span class="line">        self.s_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_features], name=<span class="string">&#x27;s_&#x27;</span>)    <span class="comment"># 接收下个 observation</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;target_net&#x27;</span>):</span><br><span class="line">            <span class="comment"># c_names(collections_names) 是在更新 target_net 参数时会用到</span></span><br><span class="line">            c_names = [<span class="string">&#x27;target_net_params&#x27;</span>, tf.GraphKeys.GLOBAL_VARIABLES]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># target_net 的第一层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):</span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># target_net 的第二层. collections 是在更新 target_net 参数时会用到</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l2&#x27;</span>):</span><br><span class="line">                w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                self.q_next = tf.matmul(l1, w2) + b2</span><br></pre></td></tr></table></figure>



<h3 id="DQN-思维决策"><a href="#DQN-思维决策" class="headerlink" title="DQN 思维决策"></a>DQN 思维决策</h3><p>接着上节内容，我们来定义 <code>DeepQNetwork</code> 的决策和思考部分。</p>
<h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>定义完上次的神经网络部分以后，这次我们来定义其他部分。包括：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="comment"># 上次的内容</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这次的内容:</span></span><br><span class="line">    <span class="comment"># 初始值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储记忆</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 看看学习效果 (可选)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="初始值"><a href="#初始值" class="headerlink" title="初始值"></a>初始值</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">            self,</span></span></span><br><span class="line"><span class="function"><span class="params">            n_actions,</span></span></span><br><span class="line"><span class="function"><span class="params">            n_features,</span></span></span><br><span class="line"><span class="function"><span class="params">            learning_rate=<span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            reward_decay=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            e_greedy=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            replace_target_iter=<span class="number">300</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            memory_size=<span class="number">500</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            batch_size=<span class="number">32</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            e_greedy_increment=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            output_graph=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">        self.n_actions = n_actions</span><br><span class="line">        self.n_features = n_features</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.gamma = reward_decay</span><br><span class="line">        self.epsilon_max = e_greedy     <span class="comment"># epsilon 的最大值</span></span><br><span class="line">        self.replace_target_iter = replace_target_iter  <span class="comment"># 更换 target_net 的步数</span></span><br><span class="line">        self.memory_size = memory_size  <span class="comment"># 记忆上限</span></span><br><span class="line">        self.batch_size = batch_size    <span class="comment"># 每次更新时从 memory 里面取多少记忆出来</span></span><br><span class="line">        self.epsilon_increment = e_greedy_increment <span class="comment"># epsilon 的增量</span></span><br><span class="line">        self.epsilon = <span class="number">0</span> <span class="keyword">if</span> e_greedy_increment <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.epsilon_max <span class="comment"># 是否开启探索模式, 并逐步减少探索次数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录学习次数 (用于判断是否更换 target_net 参数)</span></span><br><span class="line">        self.learn_step_counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化全 0 记忆 [s, a, r, s_]</span></span><br><span class="line">        self.memory = np.zeros((self.memory_size, n_features*<span class="number">2</span>+<span class="number">2</span>)) <span class="comment"># 和视频中不同, 因为 pandas 运算比较慢, 这里改为直接用 numpy</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建 [target_net, evaluate_net]</span></span><br><span class="line">        self._build_net()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 替换 target net 的参数</span></span><br><span class="line">        t_params = tf.get_collection(<span class="string">&#x27;target_net_params&#x27;</span>)  <span class="comment"># 提取 target_net 的参数</span></span><br><span class="line">        e_params = tf.get_collection(<span class="string">&#x27;eval_net_params&#x27;</span>)   <span class="comment"># 提取  eval_net 的参数</span></span><br><span class="line">        self.replace_target_op = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(t_params, e_params)] <span class="comment"># 更新 target_net 参数</span></span><br><span class="line"></span><br><span class="line">        self.sess = tf.Session()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出 tensorboard 文件</span></span><br><span class="line">        <span class="keyword">if</span> output_graph:</span><br><span class="line">            <span class="comment"># $ tensorboard --logdir=logs</span></span><br><span class="line">            tf.summary.FileWriter(<span class="string">&quot;logs/&quot;</span>, self.sess.graph)</span><br><span class="line"></span><br><span class="line">        self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        self.cost_his = []  <span class="comment"># 记录所有 cost 变化, 用于最后 plot 出来观看</span></span><br></pre></td></tr></table></figure>



<h4 id="存储记忆"><a href="#存储记忆" class="headerlink" title="存储记忆"></a>存储记忆</h4><p>DQN 的精髓部分之一：记录下所有经历过的步，这些步可以进行反复的学习，所以是一种 off-policy 方法，你甚至可以自己玩，然后记录下自己玩的经历，让这个 DQN 学习你是如何通关的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;memory_counter&#x27;</span>):</span><br><span class="line">            self.memory_counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 记录一条 [s, a, r, s_] 记录</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换</span></span><br><span class="line">        index = self.memory_counter % self.memory_size</span><br><span class="line">        self.memory[index, :] = transition <span class="comment"># 替换过程</span></span><br><span class="line"></span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h4 id="选行为"><a href="#选行为" class="headerlink" title="选行为"></a>选行为</h4><p>和之前的 <code>QLearningTable</code>，<code>SarsaTable</code> 等一样，都需要一个选行为的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        <span class="comment"># 统一 observation 的 shape (1, size_of_observation)</span></span><br><span class="line">        observation = observation[np.newaxis, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:</span><br><span class="line">            <span class="comment"># 让 eval_net 神经网络生成所有 action 的值, 并选择值最大的 action</span></span><br><span class="line">            actions_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: observation&#125;)</span><br><span class="line">            action = np.argmax(actions_value)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)   <span class="comment"># 随机选择</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>最重要的一步来了，就是在 <code>DeepQNetwork</code> 中，是如何学习，更新参数的。这里涉及了 <code>target_net</code> 和 <code>eval_net</code> 的交互使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_replace_target_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 检查是否替换 target_net 参数</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.sess.run(self.replace_target_op)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\ntarget_params_replaced\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 memory 中随机抽取 batch_size 这么多记忆</span></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        batch_memory = self.memory[sample_index, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取 q_next (target_net 产生了 q) 和 q_eval(eval_net 产生的 q)</span></span><br><span class="line">        q_next, q_eval = self.sess.run(</span><br><span class="line">            [self.q_next, self.q_eval],</span><br><span class="line">            feed_dict=&#123;</span><br><span class="line">                self.s_: batch_memory[:, -self.n_features:],</span><br><span class="line">                self.s: batch_memory[:, :self.n_features]</span><br><span class="line">            &#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面这几步十分重要. q_next, q_eval 包含所有 action 的值,</span></span><br><span class="line">        <span class="comment"># 而我们需要的只是已经选择好的 action 的值, 其他的并不需要.</span></span><br><span class="line">        <span class="comment"># 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新凭据.</span></span><br><span class="line">        <span class="comment"># 这是我们最终要达到的样子, 比如 q_target - q_eval = [1, 0, 0] - [-1, 0, 0] = [2, 0, 0]</span></span><br><span class="line">        <span class="comment"># q_eval = [-1, 0, 0] 表示这一个记忆中有我选用过 action 0, 而 action 0 带来的 Q(s, a0) = -1, 所以其他的 Q(s, a1) = Q(s, a2) = 0.</span></span><br><span class="line">        <span class="comment"># q_target = [1, 0, 0] 表示这个记忆中的 r+gamma*maxQ(s_) = 1, 而且不管在 s_ 上我们取了哪个 action,</span></span><br><span class="line">        <span class="comment"># 我们都需要对应上 q_eval 中的 action 位置, 所以就将 1 放在了 action 0 的位置.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面也是为了达到上面说的目的, 不过为了更方面让程序运算, 达到目的的过程有点不同.</span></span><br><span class="line">        <span class="comment"># 是将 q_eval 全部赋值给 q_target, 这时 q_target-q_eval 全为 0,</span></span><br><span class="line">        <span class="comment"># 不过 我们再根据 batch_memory 当中的 action 这个 column 来给 q_target 中的对应的 memory-action 位置来修改赋值.</span></span><br><span class="line">        <span class="comment"># 使新的赋值为 reward + gamma * maxQ(s_), 这样 q_target-q_eval 就可以变成我们所需的样子.</span></span><br><span class="line">        <span class="comment"># 具体在下面还有一个举例说明.</span></span><br><span class="line"></span><br><span class="line">        q_target = q_eval.copy()</span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>)</span><br><span class="line">        reward = batch_memory[:, self.n_features + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        q_target[batch_index, eval_act_index] = reward + self.gamma * np.<span class="built_in">max</span>(q_next, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        假如在这个 batch 中, 我们有2个提取的记忆, 根据每个记忆可以生产3个 action 的值:</span></span><br><span class="line"><span class="string">        q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        q_target = q_eval =</span></span><br><span class="line"><span class="string">        [[1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, 6]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        然后根据 memory 当中的具体 action 位置来修改 q_target 对应 action 上的值:</span></span><br><span class="line"><span class="string">        比如在:</span></span><br><span class="line"><span class="string">            记忆 0 的 q_target 计算值是 -1, 而且我用了 action 0;</span></span><br><span class="line"><span class="string">            记忆 1 的 q_target 计算值是 -2, 而且我用了 action 2:</span></span><br><span class="line"><span class="string">        q_target =</span></span><br><span class="line"><span class="string">        [[-1, 2, 3],</span></span><br><span class="line"><span class="string">         [4, 5, -2]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        所以 (q_target - q_eval) 就变成了:</span></span><br><span class="line"><span class="string">        [[(-1)-(1), 0, 0],</span></span><br><span class="line"><span class="string">         [0, 0, (-2)-(6)]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        最后我们将这个 (q_target - q_eval) 当成误差, 反向传递会神经网络.</span></span><br><span class="line"><span class="string">        所有为 0 的 action 值是当时没有选择的 action, 之前有选择的 action 才有不为0的值.</span></span><br><span class="line"><span class="string">        我们只反向传递之前选择的 action 的值,</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练 eval_net</span></span><br><span class="line">        _, self.cost = self.sess.run([self._train_op, self.loss],</span><br><span class="line">                                     feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                self.q_target: q_target&#125;)</span><br><span class="line">        self.cost_his.append(self.cost) <span class="comment"># 记录 cost 误差</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 逐渐增加 epsilon, 降低行为的随机性</span></span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h4 id="看学习效果"><a href="#看学习效果" class="headerlink" title="看学习效果"></a>看学习效果</h4><p>为了看看学习效果，我们在最后输出学习过程中的 <code>cost</code> 变化曲线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeepQNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_replace_target_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_cost</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">        plt.plot(np.arange(<span class="built_in">len</span>(self.cost_his)), self.cost_his)</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;Cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-3-1.png"></p>
<p>可以看出曲线并不是平滑下降的，这是因为 DQN 中的 input 数据是一步步改变的，而且会根据学习情况，获取到不同的数据。所以这并不像一般的监督学习，DQN 的 cost 曲线就有所不同了。</p>
<h4 id="修改版的-DQN"><a href="#修改版的-DQN" class="headerlink" title="修改版的 DQN"></a>修改版的 DQN</h4><p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-3-2.png"></p>
<p>最后提供一种修改版的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py">DQN 代码</a>，这是录制完视频以后做的，这是将 <code>q_target</code> 的计算也加在了 Tensorflow 的 graph 里面。这种结构还是有好处的，作为学习样本的话，计算结构全部在 tensorboard 上，就更好理解，代码结构也更好理解。</p>
<p>我只在原本的 DQN 代码上改了一点点东西，大家应该可以很容易辨别。修改版的<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py">代码在这</a>。</p>
<p>如果想一次性看到全部代码, 请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">DQN Tensorflow Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/DQN">DQN PyTorch Python 教程</a></li>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">什么是 DQN 短视频</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>论文 <a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（QLearning）</title>
    <url>/YingYingMonstre.github.io/2021/11/08/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88QLearning%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-Q-Learning"><a href="#什么是-Q-Learning" class="headerlink" title="什么是 Q Learning"></a>什么是 Q Learning</h3><p>今天我们会来说说强化学习中一个很有名的算法——Q-learning。</p>
<h4 id="行为准则"><a href="#行为准则" class="headerlink" title="行为准则"></a>行为准则</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q1.png"></p>
<p>我们做事情都会有一个自己的行为准则，比如小时候爸妈常说”不写完作业就不准看电视”。所以我们在写作业的这种状态下，好的行为就是继续写作业，直到写完它，我们还可以得到奖励。不好的行为就是没写完就跑去看电视了，被爸妈发现，后果很严重。小时候这种事情做多了，也就变成我们不可磨灭的记忆。这和我们要提到的Q learning 有什么关系呢?原来 Q learning 也是一个决策过程，和小时候的这种情况差不多。我们举例说明。</p>
<p>假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视，所以现在我们有两种选择：1.继续写作业，2. 跑去看电视。因为以前没有被罚过，所以我选看电视，然后现在的状态变成了看电视，我又选了继续看电视，接着我还是看电视。最后爸妈回家，发现我没写完作业就去看电视了，狠狠地惩罚了我一次，我也深刻地记下了这一次经历，并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为，我们在看看 Q learning 根据很多这样的经历是如何来决策的吧。</p>
<h4 id="QLearning-决策"><a href="#QLearning-决策" class="headerlink" title="QLearning 决策"></a>QLearning 决策</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q2.png"></p>
<p>假设我们的行为准则已经学习好了，现在我们处于状态s1，我在写作业，我有两个行为 a1、a2，分别是看电视和写作业，根据我的经验，在这种 s1 状态下, a2 写作业带来的潜在奖励要比 a1 看电视高，这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替，在我的记忆Q表格中, Q(s1, a1)=-2 要小于 Q(s1, a2)=1，所以我们判断要选择 a2 作为下一个行为。现在我们的状态更新成 s2，我们还是有两个同样的选择，重复上面的过程，在行为准则Q表中寻找 Q(s2, a1) Q(s2, a2) 的值, 并比较他们的大小，选取较大的一个。接着根据 a2 我们到达 s3 并在此重复上面的决策过程。Q learning 的方法也就是这样决策的。看完决策，我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改，提升的。</p>
<h4 id="QLearning-更新"><a href="#QLearning-更新" class="headerlink" title="QLearning 更新"></a>QLearning 更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q3.png"></p>
<p>所以我们回到之前的流程，根据 Q 表的估计，因为在 s1 中, a2 的值比较大，通过之前的决策方法，我们在 s1 采取了 a2，并到达 s2，这时我们开始更新用于决策的 Q 表，接着我们并没有在实际中采取任何行为，而是想象自己在 s2 上采取了每种行为，分别看看两种行为哪一个的 Q 值大。比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0)，因为会获取实实在在的奖励 R，我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值。所以有了现实和估计值，我们就能更新Q(s1, a2)，根据估计与现实的差距，将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值 变成新的值。但时刻记住，我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做。这就是 off-policy 的 Q learning 是如何决策和学习优化决策的过程。</p>
<h4 id="QLearning-整体算法"><a href="#QLearning-整体算法" class="headerlink" title="QLearning 整体算法"></a>QLearning 整体算法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q4.png"></p>
<p>这一张图概括了我们之前所有的内容。这也是 Q learning 的算法，每次更新我们都用到了 Q 现实和 Q 估计，而且 Q learning 的迷人之处就是在 Q(s1, a2) 现实中，也包含了一个 Q(s2) 的最大估计值，将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实，很奇妙吧。最后我们来说说这套算法中一些参数的意义。Epsilon greedy 是用在决策上的一种策略，比如 epsilon = 0.9 时，就说明有90% 的情况我会按照 Q 表的最优值选择行为，10% 的时间使用随机选行为。alpha是学习率，来决定这次的误差有多少是要被学习的，alpha是一个小于1的数。gamma 是对未来 reward 的衰减值。我们可以这样想象。</p>
<h4 id="QLearning-中的-Gamma"><a href="#QLearning-中的-Gamma" class="headerlink" title="QLearning 中的 Gamma"></a>QLearning 中的 Gamma</h4><p><img src="https://static.mofanpy.com/results/ML-intro/q5.png"></p>
<p>我们重写一下 Q(s1) 的公式，将 Q(s2) 拆开，因为Q(s2)可以像 Q(s1)一样，是关于Q(s3) 的，所以可以写成这样。然后以此类推，不停地这样写下去，最后就能写成这样，可以看出Q(s1) 是有关于之后所有的奖励，但这些奖励正在衰减，离 s1 越远的状态衰减越严重。不好理解? 行，我们想象 Qlearning 的机器人天生近视眼，gamma = 1 时，机器人有了一副合适的眼镜，在 s1 看到的 Q 是未来没有任何衰变的奖励，也就是机器人能清清楚楚地看到之后所有步的全部价值；但是当 gamma =0，近视机器人没了眼镜，只能摸到眼前的 reward，同样也就只在乎最近的大奖励；如果 gamma 从 0 变到 1，眼镜的度数由浅变深，对远处的价值看得越清楚，所以机器人渐渐变得有远见，不仅仅只看眼前的利益，也为自己的未来着想。</p>
<h3 id="小例子"><a href="#小例子" class="headerlink" title="小例子"></a>小例子</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>这一次我们会用 tabular Q-learning 的方法实现一个小例子，例子的环境是一个一维世界，在世界的右边有宝藏，探索者只要得到宝藏尝到了甜头，然后以后就记住了得到宝藏的方法，这就是他用强化学习所学习到的行为。</p>
<blockquote>
<p>-o—T</p>
<p>T 就是宝藏的位置, o 是探索者的位置</p>
</blockquote>
<p>Q-learning 是一种记录行为值 (Q value) 的方法，每种在一定状态的行为都会有一个值 <code>Q(s, a)</code>，就是说 行为 <code>a</code> 在 <code>s</code> 状态的值是 <code>Q(s, a)</code>。<code>s</code> 在上面的探索者游戏中，就是 <code>o</code> 所在的地点了。而每一个地点探索者都能做出两个行为 <code>left/right</code>，这就是探索者的所有可行的 <code>a</code> 啦。</p>
<p>如果在某个地点 <code>s1</code>，探索者计算了他能有的两个行为, <code>a1/a2=left/right</code>，计算结果是 <code>Q(s1, a1) &gt; Q(s1, a2)</code>，那么探索者就会选择 <code>left</code> 这个行为。这就是 Q learning 的行为选择简单规则。</p>
<p>==当然我们还会细说更具体的规则。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容，大家大概看看就行，有个大概的 RL 概念就行，知道 RL 的一些关键步骤就行，这节的算法不用仔细研究。==</p>
<h4 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h4><p>这一次需要的模块和参数设置：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">N_STATES = <span class="number">6</span>   <span class="comment"># 1维世界的宽度</span></span><br><span class="line">ACTIONS = [<span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;right&#x27;</span>]     <span class="comment"># 探索者的可用动作</span></span><br><span class="line">EPSILON = <span class="number">0.9</span>   <span class="comment"># 贪婪度 greedy</span></span><br><span class="line">ALPHA = <span class="number">0.1</span>     <span class="comment"># 学习率</span></span><br><span class="line">GAMMA = <span class="number">0.9</span>    <span class="comment"># 奖励递减值</span></span><br><span class="line">MAX_EPISODES = <span class="number">13</span>   <span class="comment"># 最大回合数</span></span><br><span class="line">FRESH_TIME = <span class="number">0.3</span>    <span class="comment"># 移动间隔时间</span></span><br></pre></td></tr></table></figure>



<h4 id="Q-表"><a href="#Q-表" class="headerlink" title="Q 表"></a>Q 表</h4><p>对于 tabular Q learning，我们必须将所有的 Q values (行为值) 放在 <code>q_table</code> 中，更新 <code>q_table</code> 也是在更新他的行为准则。<code>q_table</code> 的 index 是所有对应的 <code>state</code> (探索者位置)，columns 是对应的 <code>action</code> (探索者行为)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_q_table</span>(<span class="params">n_states, actions</span>):</span></span><br><span class="line">    table = pd.DataFrame(</span><br><span class="line">        np.zeros((n_states, <span class="built_in">len</span>(actions))),     <span class="comment"># q_table 全 0 初始</span></span><br><span class="line">        columns=actions,    <span class="comment"># columns 对应的是行为名称</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> table</span><br><span class="line"></span><br><span class="line"><span class="comment"># q_table:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   left  right</span></span><br><span class="line"><span class="string">0   0.0    0.0</span></span><br><span class="line"><span class="string">1   0.0    0.0</span></span><br><span class="line"><span class="string">2   0.0    0.0</span></span><br><span class="line"><span class="string">3   0.0    0.0</span></span><br><span class="line"><span class="string">4   0.0    0.0</span></span><br><span class="line"><span class="string">5   0.0    0.0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="定义动作"><a href="#定义动作" class="headerlink" title="定义动作"></a>定义动作</h4><p>接着定义探索者是如何挑选行为的。这是我们引入 <code>epsilon greedy</code> 的概念。因为在初始阶段，随机的探索环境，往往比固定的行为模式要好，所以这也是累积经验的阶段，我们希望探索者不会那么贪婪(greedy)。所以 <code>EPSILON</code> 就是用来控制贪婪程度的值。<code>EPSILON</code> 可以随着探索时间不断提升(越来越贪婪)，不过在这个例子中，我们就固定成 <code>EPSILON = 0.9</code>，90% 的时间是选择最优策略，10% 的时间来探索。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在某个 state 地点, 选择行为</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">state, q_table</span>):</span></span><br><span class="line">    state_actions = q_table.iloc[state, :]  <span class="comment"># 选出这个 state 的所有 action 值</span></span><br><span class="line">    <span class="keyword">if</span> (np.random.uniform() &gt; EPSILON) <span class="keyword">or</span> (state_actions.<span class="built_in">all</span>() == <span class="number">0</span>):  <span class="comment"># 非贪婪 or 或者这个 state 还没有探索过</span></span><br><span class="line">        action_name = np.random.choice(ACTIONS)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># action_name = state_actions.argmax()不能正常运行</span></span><br><span class="line">        <span class="comment"># argmax()返回的是序列中最大值的int位置</span></span><br><span class="line">        <span class="comment"># idxmax()返回的是最大值的行标签</span></span><br><span class="line">        action_name = state_actions.idxmax()    <span class="comment"># 贪婪模式</span></span><br><span class="line">    <span class="keyword">return</span> action_name</span><br></pre></td></tr></table></figure>



<h4 id="环境反馈-S-R"><a href="#环境反馈-S-R" class="headerlink" title="环境反馈 S_, R"></a>环境反馈 S_, R</h4><p>做出行为后，环境也要给我们的行为一个反馈，反馈出下个 state (S_) 和 在上个 state (S) 做出 action (A) 所得到的 reward (R)。这里定义的规则就是，只有当 <code>o</code> 移动到了 <code>T</code>，探索者才会得到唯一的一个奖励，奖励值 R=1，其他情况都没有奖励。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_env_feedback</span>(<span class="params">S, A</span>):</span></span><br><span class="line">    <span class="comment"># This is how agent will interact with the environment</span></span><br><span class="line">    <span class="keyword">if</span> A == <span class="string">&#x27;right&#x27;</span>:    <span class="comment"># move right</span></span><br><span class="line">        <span class="keyword">if</span> S == N_STATES - <span class="number">2</span>:   <span class="comment"># terminate</span></span><br><span class="line">            S_ = <span class="string">&#x27;terminal&#x27;</span></span><br><span class="line">            R = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            S_ = S + <span class="number">1</span></span><br><span class="line">            R = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:   <span class="comment"># move left</span></span><br><span class="line">        R = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> S == <span class="number">0</span>:</span><br><span class="line">            S_ = S  <span class="comment"># reach the wall</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            S_ = S - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> S_, R</span><br></pre></td></tr></table></figure>



<h4 id="环境更新"><a href="#环境更新" class="headerlink" title="环境更新"></a>环境更新</h4><p>接下来就是环境的更新了，不用细看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_env</span>(<span class="params">S, episode, step_counter</span>):</span></span><br><span class="line">    <span class="comment"># This is how environment be updated</span></span><br><span class="line">    env_list = [<span class="string">&#x27;-&#x27;</span>]*(N_STATES-<span class="number">1</span>) + [<span class="string">&#x27;T&#x27;</span>]   <span class="comment"># &#x27;---------T&#x27; our environment</span></span><br><span class="line">    <span class="keyword">if</span> S == <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">        interaction = <span class="string">&#x27;Episode %s: total_steps = %s&#x27;</span> % (episode+<span class="number">1</span>, step_counter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(interaction), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r                                &#x27;</span>, end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        env_list[S] = <span class="string">&#x27;o&#x27;</span></span><br><span class="line">        interaction = <span class="string">&#x27;&#x27;</span>.join(env_list)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\r&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(interaction), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        time.sleep(FRESH_TIME)</span><br></pre></td></tr></table></figure>



<h4 id="强化学习主循环"><a href="#强化学习主循环" class="headerlink" title="强化学习主循环"></a>强化学习主循环</h4><p>最重要的地方就在这里。你定义的 RL 方法都在这里体现。在之后的教程中，我们会更加详细得讲解 RL 中的各种方法，下面的内容，大家大概看看就行，这节内容不用仔细研究。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/2-1-1.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rl</span>():</span></span><br><span class="line">    q_table = build_q_table(N_STATES, ACTIONS)  <span class="comment"># 初始 q table</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(MAX_EPISODES):     <span class="comment"># 回合</span></span><br><span class="line">        step_counter = <span class="number">0</span></span><br><span class="line">        S = <span class="number">0</span>   <span class="comment"># 回合初始位置</span></span><br><span class="line">        is_terminated = <span class="literal">False</span>   <span class="comment"># 是否回合结束</span></span><br><span class="line">        update_env(S, episode, step_counter)    <span class="comment"># 环境更新</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_terminated:</span><br><span class="line"></span><br><span class="line">            A = choose_action(S, q_table)   <span class="comment"># 选行为</span></span><br><span class="line">            S_, R = get_env_feedback(S, A)  <span class="comment"># 实施行为并得到环境的反馈</span></span><br><span class="line">            q_predict = q_table.loc[S, A]    <span class="comment"># 估算的(状态-行为)值</span></span><br><span class="line">            <span class="keyword">if</span> S_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">                q_target = R + GAMMA * q_table.iloc[S_, :].<span class="built_in">max</span>()   <span class="comment">#  实际的(状态-行为)值 (回合没结束)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                q_target = R     <span class="comment">#  实际的(状态-行为)值 (回合结束)</span></span><br><span class="line">                is_terminated = <span class="literal">True</span>    <span class="comment"># terminate this episode</span></span><br><span class="line"></span><br><span class="line">            q_table.loc[S, A] += ALPHA * (q_target - q_predict)  <span class="comment">#  q_table 更新</span></span><br><span class="line">            S = S_  <span class="comment"># 探索者移动到下一个 state</span></span><br><span class="line"></span><br><span class="line">            update_env(S, episode, step_counter+<span class="number">1</span>)  <span class="comment"># 环境更新</span></span><br><span class="line"></span><br><span class="line">            step_counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> q_table</span><br></pre></td></tr></table></figure>

<p>写好所有的评估和更新准则后，我们就能开始训练了，把探索者丢到环境中，让它自己去玩吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    q_table = rl()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\r\nQ-table:\n&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(q_table)</span><br></pre></td></tr></table></figure>



<h3 id="Q-learning-算法更新"><a href="#Q-learning-算法更新" class="headerlink" title="Q-learning 算法更新"></a>Q-learning 算法更新</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>上次我们知道了 RL 之中的 Q-learning 方法是在做什么事，今天我们就来说说一个更具体的例子。让探索者学会走迷宫。黄色的是天堂 (reward 1)，黑色的地狱 (reward -1)。大多数 RL 是由 reward 导向的，所以定义 reward 是 RL 中比较重要的一点。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20q.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>




<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/2-1-1.png"></p>
<p>整个算法就是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。Qlearning 是一个 off-policy 的算法，因为里面的 <code>max</code> action 让 Q table 的更新可以不基于正在经历的经验(可以是现在学习着很久以前的经验，甚至是学习他人的经验)。不过这一次的例子，我们没有运用到 off-policy，而是把 Qlearning 用在了 on-policy 上，也就是现学现卖，将现在经历的直接当场学习并运用。On-policy 和 off-policy 的差别我们会在之后的 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">Deep Q network (off-policy)</a> 学习中见识到。而之后的教程也会讲到一个 on-policy (Sarsa) 的形式，我们之后再对比。</p>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>首先我们先 import 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/2_Q_Learning_maze/maze_env.py">这里下载</a>，<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。我也有<a href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的。而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> QLearningTable</span><br></pre></td></tr></table></figure>

<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Qlearning 最重要的迭代更新部分啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>



<h3 id="Q-learning-思维决策"><a href="#Q-learning-思维决策" class="headerlink" title="Q-learning 思维决策"></a>Q-learning 思维决策</h3><p>接着上节内容，我们来实现 <code>RL_brain</code> 的 <code>QLearningTable</code> 部分，这也是 RL 的大脑部分，负责决策和思考。</p>
<h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>与上回不一样的地方是，我们将要以一个 class 形式定义 Q learning，并把这种 tabular q learning 方法叫做 <code>QLearningTable</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>



<h4 id="预设值-1"><a href="#预设值-1" class="headerlink" title="预设值"></a>预设值</h4><p>初始的参数意义不会在这里提及了，请参考这个快速了解通道 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/tabular-q2/#">机器学习系列-Q learning</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        self.actions = actions  <span class="comment"># a list</span></span><br><span class="line">        self.lr = learning_rate <span class="comment"># 学习率</span></span><br><span class="line">        self.gamma = reward_decay   <span class="comment"># 奖励衰减</span></span><br><span class="line">        self.epsilon = e_greedy     <span class="comment"># 贪婪度</span></span><br><span class="line">        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)   <span class="comment"># 初始 q_table</span></span><br></pre></td></tr></table></figure>



<h4 id="决定行为"><a href="#决定行为" class="headerlink" title="决定行为"></a>决定行为</h4><p>这里是定义如何根据所在的 state，或者是在这个 state 上的 观测值 (observation) 来决策。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        self.check_state_exist(observation) <span class="comment"># 检测本 state 是否在 q_table 中存在(见后面标题内容)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选择 action</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; self.epsilon:  <span class="comment"># 选择 Q value 最高的 action</span></span><br><span class="line">            state_action = self.q_table.loc[observation, :]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 同一个 state, 可能会有多个相同的 Q action value, 所以我们乱序一下</span></span><br><span class="line">            action = np.random.choice(state_action[state_action == np.<span class="built_in">max</span>(state_action)].index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 随机选择 action</span></span><br><span class="line">            action = np.random.choice(self.actions)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>同<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/general-rl">上一个简单的 q learning 例子</a>一样，我们根据是否是 <code>terminal</code> state (回合终止符) 来判断应该如何更行 <code>q_table</code>。更新的方式是不是很熟悉呢:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update &#x3D; self.lr * (q_target - q_predict)</span><br></pre></td></tr></table></figure>

<p>这可以理解成神经网络中的更新方式，学习率 * (真实值 - 预测值)。将判断误差传递回去，有着和神经网络更新的异曲同工之处。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)  <span class="comment"># 检测 q_table 中是否存在 s_ (见后面标题内容)</span></span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()  <span class="comment"># 下个 state 不是 终止符</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># 下个 state 是终止符</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># 更新对应的 state-action 值</span></span><br></pre></td></tr></table></figure>



<h4 id="检测-state-是否存在"><a href="#检测-state-是否存在" class="headerlink" title="检测 state 是否存在"></a>检测 state 是否存在</h4><p>这个功能就是检测 <code>q_table</code> 中有没有当前 state 的步骤了，如果还没有当前 state，那我我们就插入一组全 0 数据，当做这个 state 的所有 action 初始 values。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            self.q_table = self.q_table.append(</span><br><span class="line">                pd.Series(</span><br><span class="line">                    [<span class="number">0</span>]*<span class="built_in">len</span>(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>

<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/1_command_line_reinforcement_learning/treasure_on_right.py">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/2_Q_Learning_maze">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">什么是 Q Learning 短视频</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（Sarsa）</title>
    <url>/YingYingMonstre.github.io/2021/11/10/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88Sarsa%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是-Sarsa"><a href="#什么是-Sarsa" class="headerlink" title="什么是 Sarsa"></a>什么是 Sarsa</h3><p>今天我们会来说说强化学习中一个和 Q learning 类似的算法，叫做 Sarsa。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<p><img src="https://static.mofanpy.com/results/ML-intro/s1.png"></p>
<p>在强化学习中 Sarsa 和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 及其类似，这节内容会基于之前我们所讲的 Q learning。所以还不熟悉 Q learning 的朋友们，请前往我制作的 Q learning 简介 (知乎专栏)。我们会对比 Q learning，来看看 Sarsa 是特殊在哪些方面。和上次一样，我们还是使用写作业和看电视这个例子。没写完作业去看电视被打，写完了作业有糖吃。</p>
<h4 id="Sarsa-决策"><a href="#Sarsa-决策" class="headerlink" title="Sarsa 决策"></a>Sarsa 决策</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s2.png"></p>
<p>Sarsa 的决策部分和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 一模一样，因为我们使用的是 Q 表的形式决策，所以我们会在 Q 表中挑选值较大的动作值施加在环境中来换取奖惩。但是不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<h4 id="Sarsa-更新行为准则"><a href="#Sarsa-更新行为准则" class="headerlink" title="Sarsa 更新行为准则"></a>Sarsa 更新行为准则</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s3.png"></p>
<p>同样，我们会经历正在写作业的状态 s1，然后再挑选一个带来最大潜在奖励的动作 a2，这样我们就到达了 继续写作业状态 s2，而在这一步，如果你用的是 Q learning，你会观看一下在 s2 上选取哪一个动作会带来最大的奖励，但是在真正要做决定时，却不一定会选取到那个带来最大奖励的动作，Q-learning 在这一步只是估计了一下接下来的动作值。而 Sarsa 是实践派，他说到做到，在 s2 这一步估算的动作也是接下来要做的动作。所以 Q(s1, a2) 现实的计算值，我们也会稍稍改动，去掉maxQ，取而代之的是在 s2 上我们实实在在选取的 a2 的 Q 值。最后像 Q learning 一样，求出现实和估计的差距 并更新 Q 表里的 Q(s1, a2)。</p>
<h4 id="对比-Sarsa-和-Qlearning-算法"><a href="#对比-Sarsa-和-Qlearning-算法" class="headerlink" title="对比 Sarsa 和 Qlearning 算法"></a>对比 Sarsa 和 Qlearning 算法</h4><p><img src="https://static.mofanpy.com/results/ML-intro/s4.png"></p>
<p>从算法来看，这就是他们两最大的不同之处了。因为 Sarsa 是说到做到型，所以我们也叫他 on-policy、在线学习，学着自己在做的事情。而 Q learning 是说到但并不一定做到，所以它也叫作 Off-policy、离线学习。而因为有了 maxQ，Q-learning 也是一个特别勇敢的算法。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/s5.png"></p>
<p>为什么说他勇敢呢，因为 Q learning 机器人永远都会选择最近的一条通往成功的道路，不管这条路会有多危险。而 Sarsa 则是相当保守，他会选择离危险远远的，拿到宝藏是次要的，保住自己的小命才是王道。这就是使用 Sarsa 方法的不同之处。</p>
<h3 id="Sarsa-算法更新"><a href="#Sarsa-算法更新" class="headerlink" title="Sarsa 算法更新"></a>Sarsa 算法更新</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>这次我们用同样的迷宫例子来实现 RL 中另一种和 Qlearning 类似的算法，叫做 Sarsa (state-action-reward-state*-action*)。我们从这一个简称可以了解到，Sarsa 的整个循环都将是在一个路径上，也就是 on-policy，下一个 state，和下一个 <em>action</em> 将会变成他真正采取的 action 和 state。和 Qlearning 的不同之处就在这。Qlearning 的下个一个 state_ action_ 在算法更新的时候都还是不确定的 (off-policy)。而 Sarsa 的 state，<em>action</em> 在这次算法更新的时候已经确定好了 (on-policy)。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-1-1.png"></p>
<p>整个算法还是一直不断更新 Q table 里的值，然后再根据新的值来判断要在某个 state 采取怎样的 action。不过于 Qlearning 不同之处：</p>
<ul>
<li>他在当前 <code>state</code> 已经想好了 <code>state</code> 对应的 <code>action</code>，而且想好了下一个 <code>state_</code> 和下一个 <code>action_</code> (Qlearning 还没有想好下一个 <code>action_</code>)</li>
<li>更新 <code>Q(s,a)</code> 的时候基于的是下一个 <code>Q(s_, a_)</code> (Qlearning 是基于 <code>maxQ(s_)</code>)</li>
</ul>
<p>这种不同之处使得 Sarsa 相对于 Qlearning 更加的胆小。因为 Qlearning 永远都是想着 <code>maxQ</code> 最大化，因为这个 <code>maxQ</code> 而变得贪婪，不考虑其他非 <code>maxQ</code> 的结果。我们可以理解成 Qlearning 是一种贪婪、大胆、勇敢的算法，对于错误、死亡并不在乎。而 Sarsa 是一种保守的算法，他在乎每一步决策，对于错误和死亡比较铭感。这一点我们会在可视化的部分看出他们的不同。两种算法都有他们的好处，比如在实际中，你比较在乎机器的损害，用一种保守的算法，在训练时就能减少损坏的次数。</p>
<h4 id="算法的代码形式"><a href="#算法的代码形式" class="headerlink" title="算法的代码形式"></a>算法的代码形式</h4><p>首先我们先 import 两个模块，<code>maze_env</code> 是我们的环境模块，已经编写好了，大家可以直接在<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/3_Sarsa_maze/maze_env.py">这里下载</a>。<code>maze_env</code> 模块我们可以不深入研究，如果你对编辑环境感兴趣，可以去看看如何使用 python 自带的简单 GUI 模块 <code>tkinter</code> 来编写虚拟环境。我也有<a href="https://mofanpy.com/tutorials/python-basic/tkinter/">对应的教程</a>。<code>maze_env</code> 就是用 <code>tkinter</code> 编写的。而 <code>RL_brain</code> 这个模块是 RL 的大脑部分，我们下节会讲。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> maze_env <span class="keyword">import</span> Maze</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> SarsaTable</span><br></pre></td></tr></table></figure>

<p>下面的代码，我们可以根据上面的图片中的算法对应起来，这就是整个 Sarsa 最重要的迭代更新部分啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>():</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sarsa 根据 state 观测选择行为</span></span><br><span class="line">        action = RL.choose_action(<span class="built_in">str</span>(observation))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 根据下一个 state (obervation_) 选取下一个 action_</span></span><br><span class="line">            action_ = RL.choose_action(<span class="built_in">str</span>(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa</span></span><br><span class="line">            RL.learn(<span class="built_in">str</span>(observation), action, reward, <span class="built_in">str</span>(observation_), action_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个当成下一步的 state (observation) and action</span></span><br><span class="line">            observation = observation_</span><br><span class="line">            action = action_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 终止时跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 大循环完毕</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;game over&#x27;</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = SarsaTable(actions=<span class="built_in">list</span>(<span class="built_in">range</span>(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure>

<p>下一节我们会来讲解 <code>SarsaTable</code> 这种算法具体要怎么编。</p>
<h3 id="Sarsa思维决策"><a href="#Sarsa思维决策" class="headerlink" title="Sarsa思维决策"></a>Sarsa思维决策</h3><h4 id="代码主结构"><a href="#代码主结构" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>和之前定义 Qlearning 中的 <code>QLearningTable</code> 一样，因为使用 tabular 方式的 <code>Sarsa</code> 和 <code>Qlearning</code> 的相似度极高，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>

<p>我们甚至可以定义一个主class <code>RL</code>，然后将 <code>QLearningTable</code> 和 <code>SarsaTable</code> 作为 主class <code>RL</code> 的衍生，这个主 <code>RL</code> 可以这样定义。所以我们将之前的 <code>__init__</code>、<code>check_state_exist</code>、<code>choose_action</code>、<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RL</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, action_space, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        ... <span class="comment"># 和 QLearningTable 中的代码一样</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># 每种的都有点不同, 所以用 pass</span></span><br></pre></td></tr></table></figure>

<p>如果是这样定义父类的 <code>RL</code> class，通过继承关系，那之子类 <code>QLearningTable</code> class 就能简化成这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLearningTable</span>(<span class="params">RL</span>):</span>   <span class="comment"># 继承了父类 RL</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    <span class="comment"># 表示继承关系</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span>   <span class="comment"># learn 的方法在每种类型中有不一样, 需重新定义</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, :].<span class="built_in">max</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r</span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)</span><br></pre></td></tr></table></figure>



<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaTable</span>(<span class="params">RL</span>):</span>   <span class="comment"># 继承 RL class</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)    <span class="comment"># 表示继承关系</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.loc[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.loc[s_, a_]  <span class="comment"># q_target 基于选好的 a_ 而不是 Q(s_) 的最大值</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r  <span class="comment"># 如果 s_ 是终止符</span></span><br><span class="line">        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  <span class="comment"># 更新 q_table</span></span><br></pre></td></tr></table></figure>



<h3 id="什么是-Sarsa-lambda"><a href="#什么是-Sarsa-lambda" class="headerlink" title="什么是 Sarsa(lambda)"></a>什么是 Sarsa(lambda)</h3><p>今天我们会来说说强化学习中基于 Sarsa 的一种提速方法，叫做 sarsa-lambda。</p>
<p><strong>注: 本文不会涉及数学推导。大家可以在很多其他地方找到优秀的数学推导文章。</strong></p>
<h4 id="Sarsa-n"><a href="#Sarsa-n" class="headerlink" title="Sarsa(n)"></a>Sarsa(n)</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl1.png"></p>
<p>通过上个视频的介绍，我们知道这个 [Sarsa]/tutorials/machine-learning/reinforcement-learning/intro-sarsa)) 的算法是一种在线学习法，on-policy。但是这个 lambda 到底是什么。其实吧，Sarsa 是一种单步更新法，在环境中每走一步，更新一次自己的行为准则，我们可以在这样的 Sarsa 后面打一个括号，说他是 Sarsa(0)，因为他等走完这一步以后直接更新行为准则。如果延续这种想法，走完这步，再走一步，然后再更新，我们可以叫他 Sarsa(1)。同理，如果等待回合完毕我们一次性再更新呢，比如这回合我们走了 n 步，那我们就叫 Sarsa(n)。为了统一这样的流程，我们就有了一个 lambda 值来代替我们想要选择的步数，这也就是 Sarsa(lambda) 的由来。我们看看最极端的两个例子，对比单步更新和回合更新，看看回合更新的优势在哪里。</p>
<h4 id="单步更新-and-回合更新"><a href="#单步更新-and-回合更新" class="headerlink" title="单步更新 and 回合更新"></a>单步更新 and 回合更新</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl2.png"></p>
<p>虽然我们每一步都在更新，但是在没有获取宝藏的时候，我们现在站着的这一步也没有得到任何更新，也就是直到获取宝藏时，我们才为获取到宝藏的上一步更新为：这一步很好，和获取宝藏是有关联的，而之前为了获取宝藏所走的所有步都被认为和获取宝藏没关系。回合更新虽然我要等到这回合结束，才开始对本回合所经历的所有步都添加更新，但是这所有的步都是和宝藏有关系的，都是为了得到宝藏需要学习的步，所以每一个脚印在下回合被选则的几率又高了一些。在这种角度来看，回合更新似乎会有效率一些。</p>
<h4 id="有时迷茫"><a href="#有时迷茫" class="headerlink" title="有时迷茫"></a>有时迷茫</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl3.png"></p>
<p>我们看看这种情况，还是使用单步更新的方法在每一步都进行更新，但是同时记下之前的寻宝之路。你可以想像，每走一步，插上一个小旗子，这样我们就能清楚的知道除了最近的一步，找到宝物时还需要更新哪些步了。不过，有时候情况可能没有这么乐观。开始的几次，因为完全没有头绪，我可能在原地打转了很久，然后才找到宝藏，那些重复的脚步真的对我拿到宝藏很有必要吗？答案我们都知道。所以Sarsa(lambda)就来拯救你啦。</p>
<h4 id="Lambda-含义"><a href="#Lambda-含义" class="headerlink" title="Lambda 含义"></a>Lambda 含义</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl4.png"></p>
<p>其实 lambda 就是一个衰变值，他可以让你知道离奖励越远的步可能并不是让你最快拿到奖励的步，所以我们想象我们站在宝藏的位置，回头看看我们走过的寻宝之路，离宝藏越近的脚印越看得清，远处的脚印太渺小，我们都很难看清，那我们就索性记下离宝藏越近的脚印越重要，越需要被好好的更新。和之前我们提到过的奖励衰减值 gamma 一样，lambda 是脚步衰减值，都是一个在 0 和 1 之间的数。</p>
<h4 id="Lambda-取值"><a href="#Lambda-取值" class="headerlink" title="Lambda 取值"></a>Lambda 取值</h4><p><img src="https://static.mofanpy.com/results/ML-intro/sl5.png"></p>
<p>当 lambda 取0，就变成了 Sarsa 的单步更新，当 lambda 取 1，就变成了回合更新，对所有步更新的力度都是一样。当 lambda 在 0 和 1 之间，取值越大，离宝藏越近的步更新力度越大。这样我们就不用受限于单步更新的每次只能更新最近的一步，我们可以更有效率的更新所有相关步了。</p>
<h3 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa-lambda"></a>Sarsa-lambda</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>Sarsa-lambda 是基于 Sarsa 方法的升级版，他能更有效率地学习到怎么样获得好的 reward。如果说 Sarsa 和 Qlearning 都是每次获取到 reward，只更新获取到 reward 的前一步。那 Sarsa-lambda 就是更新获取到 reward 的前 lambda 步。lambda 是在 [0, 1] 之间取值，</p>
<p>如果 lambda = 0，Sarsa-lambda 就是 Sarsa，只更新获取到 reward 前经历的最后一步。</p>
<p>如果 lambda = 1，Sarsa-lambda 更新的是获取到 reward 前所有经历的步。</p>
<p>这样解释起来有点抽象，还是建议大家观看我制作的 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda">什么是 Sarsa-lambda 短视频</a>, 用动画展示具体的区别。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/maze%20sarsa_lambda.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="代码主结构-1"><a href="#代码主结构-1" class="headerlink" title="代码主结构"></a>代码主结构</h4><p>使用 <code>SarsaLambdaTable</code> 在算法更新迭代的部分，是和之前的 <code>SarsaTable</code> 一样的，所以这一节，我们没有算法更新部分, 直接变成思维决策部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>:</span></span><br><span class="line">    <span class="comment"># 初始化 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选行为 (与之前一样)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习更新参数 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检测 state 是否存在 (有改变)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br></pre></td></tr></table></figure>

<p>同样, 我们选择继承的方式，将 <code>SarsaLambdaTable</code> 继承到 <code>RL</code>，所以我们将之前的 <code>__init__</code>，<code>check_state_exist</code>，<code>choose_action</code>，<code>learn</code> 全部都放在这个主结构中，之后根据不同的算法更改对应的内容就好了。所以还没弄懂这些功能的朋友们，请回到之前的教程再看一遍。</p>
<p>算法的相应更改请参考这个：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-3-1.png"></p>
<h4 id="预设值"><a href="#预设值" class="headerlink" title="预设值"></a>预设值</h4><p>在预设值当中，我们添加了 <code>trace_decay=0.9</code> 这个就是 <code>lambda</code> 的值了。这个值将会使得拿到 reward 前的每一步都有价值。如果还不太明白其他预设值的意思，请查看我的 <a href="https://mofanpy.com/tutorials/machine-learning/ML-intro">关于强化学习的短视频列表</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SarsaLambdaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 后向观测算法, eligibility trace.</span></span><br><span class="line">        self.lambda_ = trace_decay</span><br><span class="line">        self.eligibility_trace = self.q_table.copy()    <span class="comment"># 空的 eligibility trace 表</span></span><br></pre></td></tr></table></figure>



<h4 id="检测-state-是否存在"><a href="#检测-state-是否存在" class="headerlink" title="检测 state 是否存在"></a>检测 state 是否存在</h4><p><code>check_state_exist</code> 和之前的是高度相似的。唯一不同的地方是我们考虑了 <code>eligibility_trace</code>，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">not</span> <span class="keyword">in</span> self.q_table.index:</span><br><span class="line">            <span class="comment"># append new state to q table</span></span><br><span class="line">            to_be_append = pd.Series(</span><br><span class="line">                    [<span class="number">0</span>] * <span class="built_in">len</span>(self.actions),</span><br><span class="line">                    index=self.q_table.columns,</span><br><span class="line">                    name=state,</span><br><span class="line">                )</span><br><span class="line">            self.q_table = self.q_table.append(to_be_append)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># also update eligibility trace</span></span><br><span class="line">            self.eligibility_trace = self.eligibility_trace.append(to_be_append)</span><br></pre></td></tr></table></figure>



<h4 id="学习-1"><a href="#学习-1" class="headerlink" title="学习"></a>学习</h4><p>有了父类的 <code>RL</code>，我们这次的编写就很简单，只需要编写 <code>SarsaLambdaTable</code> 中 <code>learn</code> 这个功能就完成了。因为其他功能都和父类是一样的。这就是我们所有的 <code>SarsaLambdaTable</code> 于父类 <code>RL</code> 不同之处的代码。是不是很简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SarsaLambdaTable</span>(<span class="params">RL</span>):</span> <span class="comment"># 继承 RL class</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, actions, learning_rate=<span class="number">0.01</span>, reward_decay=<span class="number">0.9</span>, e_greedy=<span class="number">0.9</span>, trace_decay=<span class="number">0.9</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_state_exist</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self, s, a, r, s_, a_</span>):</span></span><br><span class="line">        <span class="comment"># 这部分和 Sarsa 一样</span></span><br><span class="line">        self.check_state_exist(s_)</span><br><span class="line">        q_predict = self.q_table.ix[s, a]</span><br><span class="line">        <span class="keyword">if</span> s_ != <span class="string">&#x27;terminal&#x27;</span>:</span><br><span class="line">            q_target = r + self.gamma * self.q_table.ix[s_, a_]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            q_target = r</span><br><span class="line">        error = q_target - q_predict</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里开始不同:</span></span><br><span class="line">        <span class="comment"># 对于经历过的 state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环</span></span><br><span class="line">        self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q table 更新</span></span><br><span class="line">        self.q_table += self.lr * error * self.eligibility_trace</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随着时间衰减 eligibility trace 的值, 离获取 reward 越远的步, 他的&quot;不可或缺性&quot;越小</span></span><br><span class="line">        self.eligibility_trace *= self.gamma*self.lambda_</span><br></pre></td></tr></table></figure>

<p>除了图中和上面代码这种更新方式，还有一种会更加有效率。我们可以将上面的这一步替换成下面这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上面代码中的方式:</span></span><br><span class="line">self.eligibility_trace.ix[s, a] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更有效的方式:</span></span><br><span class="line">self.eligibility_trace.ix[s, :] *= <span class="number">0</span></span><br><span class="line">self.eligibility_trace.ix[s, a] = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>他们两的不同之处可以用这张图来概括:</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/3-3-2.png"></p>
<p>这是针对于一个 state-action 值按经历次数的变化。最上面是经历 state-action 的时间点，第二张图是使用这种方式所带来的 不可或缺性值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.eligibility_trace.ix[s, a] +&#x3D; 1</span><br></pre></td></tr></table></figure>

<p>下面图是使用这种方法带来的不可或缺性值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">self.eligibility_trace.ix[s, :] *&#x3D; 0; self.eligibility_trace.ix[s, a] &#x3D; 1</span><br></pre></td></tr></table></figure>

<p>实验证明选择下面这种方法会有更好的效果。大家也可以自己玩一玩，试试两种方法的不同表现。</p>
<p>最后不要忘了，eligibility trace 只是记录每个回合的每一步，新回合开始的时候需要将 Trace 清零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 新回合, 清零</span></span><br><span class="line">    RL.eligibility_trace *= <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>: <span class="comment"># 开始回合</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/3_Sarsa_maze">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p>学习资料:</p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/3_Sarsa_maze">全部代码</a></li>
<li>模拟视频效果<a href="https://www.youtube.com/playlist?list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">Youtube</a>, <a href="http://list.youku.com/albumlist/show/id_27485743">Youku</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q-learning 简介视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa">什么是 Sarsa 短视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda">什么是 Sarsa-lambda 短视频</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/tabular-sarsa1">Sarsa Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>学习书籍 <a href="https://mofanpy.com/static/files/Reinforcement_learning_An_introduction.pdf">Reinforcement learning: An introduction</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>莫烦强化学习（DQN下）</title>
    <url>/YingYingMonstre.github.io/2021/11/30/%E8%8E%AB%E7%83%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88DQN%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<h3 id="OpenAI-gym-环境库"><a href="#OpenAI-gym-环境库" class="headerlink" title="OpenAI gym 环境库"></a>OpenAI gym 环境库</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>手动编环境是一件很耗时间的事情，所以如果有能力使用别人已经编好的环境，可以节约我们很多时间。OpenAI gym 就是这样一个模块，他提供了我们很多优秀的模拟环境。我们的各种 RL 算法都能使用这些环境。不过 OpenAI gym 暂时只支持 MacOS 和 Linux 系统。Windows 已经支持，但是听说还没有全面支持，大家时不时查看下官网，可能就有惊喜。实在等不及更新了，也行用 tkinter 来手动编写一下环境。这里有我制作的很好的 <a href="https://mofanpy.com/tutorials/python-basic/tkinter">tkinter 入门教程</a>，之前的 maze 环境也是用 tkinter 编出来的。大家可以仿照<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/maze_env.py">这个文件</a>编写就 ok 啦。或者还可以玩玩更厉害的，像 OpenAI 一样，使用 pyglet 模块来编写，我做了一个从环境开始编写的<a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a>。</p>
<h4 id="安装-gym"><a href="#安装-gym" class="headerlink" title="安装 gym"></a>安装 gym</h4><p>在 MacOS 和 Linux 系统下，安装 gym 很方便，首先确定你是 python 2.7 或者 python 3.5 版本。然后在你的 terminal 中复制下面这些。但是 gym 暂时还不完全支持 Windows，不过有些虚拟环境已经的到了支持，想立杆子那个已经支持了。所以接下来要说的安装方法只有 MacOS 和 Linux 的。Windows 用户的安装方式应该也差不多，如果 Windows 用户遇到了问题，欢迎在留言区分享解决的方法。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> python 2.7, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install gym</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python 3.5, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip3 install gym</span></span><br></pre></td></tr></table></figure>

<p>如果没有报错，恭喜你，这样你就装好了 gym 的最基本款，可以开始玩以下游戏啦：</p>
<ul>
<li><a href="https://gym.openai.com/envs#algorithmic">algorithmic</a></li>
<li><a href="https://gym.openai.com/envs#toy_text">toy_text</a></li>
<li><a href="https://gym.openai.com/envs#classic_control">classic_control</a> (这个需要 pyglet 模块)</li>
</ul>
<p>如果在安装中遇到问题。可能是缺少了一些必要模块，可以使用下面语句来安装这些模块(安装时间可能有点久)：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> MacOS:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> brew install cmake boost boost-python sdl2 swig wget</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Ubuntu 14.04:</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig</span></span><br></pre></td></tr></table></figure>

<p>如果想要玩 gym 提供的全套游戏，下面这几句就能满足你：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> python 2.7, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install gym[all]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> python 3.5, 复制下面</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip3 install gym[all]</span></span><br></pre></td></tr></table></figure>



<h4 id="CartPole-例子"><a href="#CartPole-例子" class="headerlink" title="CartPole 例子"></a>CartPole 例子</h4><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/cartpole%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>

<p>之前我编写的 <code>maze_env</code> 基本上是按照 <code>gym</code> 环境格式写的，所以你换成 <code>gym</code> 格式会很简单。</p>
<p>接下来我们对应上面的算法，来实现主循环。首先 import 所需模块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DeepQNetwork</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)   <span class="comment"># 定义使用 gym 库中的那一个环境</span></span><br><span class="line">env = env.unwrapped <span class="comment"># 不做这个会有很多限制</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space) <span class="comment"># 查看这个环境中可用的 action 有多少个</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space)    <span class="comment"># 查看这个环境中可用的 state 的 observation 有多少个</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space.high)   <span class="comment"># 查看 observation 最高取值</span></span><br><span class="line"><span class="built_in">print</span>(env.observation_space.low)    <span class="comment"># 查看 observation 最低取值</span></span><br></pre></td></tr></table></figure>

<p>与之前使用 tkinter 定义的环境有点不一样，我们可以不适用 <code>if __name__ == &quot;__main__&quot;</code> 的方式，下面是一种类似，却更简单的写法。之中我们会用到里面的 reward，但是 <code>env.step()</code> 说提供的 <code>reward</code> 不一定是最有效率的 <code>reward</code>，我们大可对这些进行修改，使 DQN 学得更有效率。你可以自己对比一下不修改 reward 和按我这样修改，他们学习过程的不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义使用 DQN 的算法</span></span><br><span class="line">RL = DeepQNetwork(n_actions=env.action_space.n,</span><br><span class="line">                  n_features=env.observation_space.shape[<span class="number">0</span>],</span><br><span class="line">                  learning_rate=<span class="number">0.01</span>, e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                  replace_target_iter=<span class="number">100</span>, memory_size=<span class="number">2000</span>,</span><br><span class="line">                  e_greedy_increment=<span class="number">0.0008</span>,)</span><br><span class="line"></span><br><span class="line">total_steps = <span class="number">0</span> <span class="comment"># 记录步数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取回合 i_episode 第一个 observation</span></span><br><span class="line">    observation = env.reset()</span><br><span class="line">    ep_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        env.render()    <span class="comment"># 刷新环境</span></span><br><span class="line"></span><br><span class="line">        action = RL.choose_action(observation)  <span class="comment"># 选行为</span></span><br><span class="line"></span><br><span class="line">        observation_, reward, done, info = env.step(action) <span class="comment"># 获取下一个 state</span></span><br><span class="line"></span><br><span class="line">        x, x_dot, theta, theta_dot = observation_   <span class="comment"># 细分开, 为了修改原配的 reward</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># x 是车的水平位移, 所以 r1 是车越偏离中心, 分越少</span></span><br><span class="line">        <span class="comment"># theta 是棒子离垂直的角度, 角度越大, 越不垂直. 所以 r2 是棒越垂直, 分越高</span></span><br><span class="line"></span><br><span class="line">        x, x_dot, theta, theta_dot = observation_</span><br><span class="line">        r1 = (env.x_threshold - <span class="built_in">abs</span>(x))/env.x_threshold - <span class="number">0.8</span></span><br><span class="line">        r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta))/env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">        reward = r1 + r2   <span class="comment"># 总 reward 是 r1 和 r2 的结合, 既考虑位置, 也考虑角度, 这样 DQN 学习更有效率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存这一组记忆</span></span><br><span class="line">        RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; <span class="number">1000</span>:</span><br><span class="line">            RL.learn()  <span class="comment"># 学习</span></span><br><span class="line"></span><br><span class="line">        ep_r += reward</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;episode: &#x27;</span>, i_episode,</span><br><span class="line">                  <span class="string">&#x27;ep_r: &#x27;</span>, <span class="built_in">round</span>(ep_r, <span class="number">2</span>),</span><br><span class="line">                  <span class="string">&#x27; epsilon: &#x27;</span>, <span class="built_in">round</span>(RL.epsilon, <span class="number">2</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        observation = observation_</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line"><span class="comment"># 最后输出 cost 曲线</span></span><br><span class="line">RL.plot_cost()</span><br></pre></td></tr></table></figure>

<p>这是更为典型的 RL cost 曲线：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-4-1.png"></p>
<h4 id="MountainCar-例子"><a href="#MountainCar-例子" class="headerlink" title="MountainCar 例子"></a>MountainCar 例子</h4><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>

<p>代码基本和上述代码相同，就只是在 reward 上动了下手脚。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DeepQNetwork</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;MountainCar-v0&#x27;</span>)</span><br><span class="line">env = env.unwrapped</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space.high)</span><br><span class="line"><span class="built_in">print</span>(env.observation_space.low)</span><br><span class="line"></span><br><span class="line">RL = DeepQNetwork(n_actions=<span class="number">3</span>, n_features=<span class="number">2</span>, learning_rate=<span class="number">0.001</span>, e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                  replace_target_iter=<span class="number">300</span>, memory_size=<span class="number">3000</span>,</span><br><span class="line">                  e_greedy_increment=<span class="number">0.0001</span>,)</span><br><span class="line"></span><br><span class="line">total_steps = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    observation = env.reset()</span><br><span class="line">    ep_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        env.render()</span><br><span class="line"></span><br><span class="line">        action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">        observation_, reward, done, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        position, velocity = observation_</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 车开得越高 reward 越大</span></span><br><span class="line">        reward = <span class="built_in">abs</span>(position - (-<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">        RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; <span class="number">1000</span>:</span><br><span class="line">            RL.learn()</span><br><span class="line"></span><br><span class="line">        ep_r += reward</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            get = <span class="string">&#x27;| Get&#x27;</span> <span class="keyword">if</span> observation_[<span class="number">0</span>] &gt;= env.unwrapped.goal_position <span class="keyword">else</span> <span class="string">&#x27;| ----&#x27;</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Epi: &#x27;</span>, i_episode,</span><br><span class="line">                  get,</span><br><span class="line">                  <span class="string">&#x27;| Ep_r: &#x27;</span>, <span class="built_in">round</span>(ep_r, <span class="number">4</span>),</span><br><span class="line">                  <span class="string">&#x27;| Epsilon: &#x27;</span>, <span class="built_in">round</span>(RL.epsilon, <span class="number">2</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        observation = observation_</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">RL.plot_cost()</span><br></pre></td></tr></table></figure>

<p>出来的 cost 曲线是这样：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-4-2.png"></p>
<p>这两个都只是例子而已，具体的实施你也可以大动手脚，比如你的 reward 定义得更好，你的神经网络结构更好，使得他们学的更快。这些都是自己定义的。</p>
<h3 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p><strong>本篇教程是基于 Deep Q network (DQN) 的选学教程。以下教程缩减了在 DQN 方面的介绍，着重强调 Double DQN 和 DQN 在代码上不同的地方。所以还没了解 DQN 的同学们，有关于 DQN 的知识，请从 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">这个视频</a> 和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">这个Python教程</a> 开始学习。</strong></p>
<p>接下来我们说说为什么会有 Double DQN 这种算法。所以我们从 Double DQN 相对于 Natural DQN (传统 DQN) 的优势说起。</p>
<p>一句话概括，DQN 基于 Q-learning, Q-Learning 中有 <code>Qmax</code>，<code>Qmax</code> 会导致 <code>Q现实</code> 当中的过估计 (overestimate)。而 Double DQN 就是用来解决过估计的。在实际问题中，如果你输出你的 DQN 的 Q 值，可能就会发现，Q 值都超级大。这就是出现了 overestimate。</p>
<p>这次的 Double DQN 的算法基于的是 OpenAI Gym 中的 <code>Pendulum</code> 环境。如果还不了解如何使用 OpenAI 的话，这里有<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/gym">我的一个教程</a>。以下就是这次的结果，先睹为快。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/Pendulum%20DQN.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="Double-DQN-算法"><a href="#Double-DQN-算法" class="headerlink" title="Double DQN 算法"></a>Double DQN 算法</h4><p>我们知道 DQN 的神经网络部分可以看成一个 <code>最新的神经网络</code> + <code>老神经网络</code>，他们有相同的结构，但内部的参数更新却有时差。而它的 <code>Q现实</code> 部分是这样的：</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-5-1.png"></p>
<p>因为我们的神经网络预测 <code>Qmax</code> 本来就有误差，每次也向着最大误差的 <code>Q现实</code> 改进神经网络，就是因为这个 <code>Qmax</code> 导致了 overestimate。所以 Double DQN 的想法就是引入另一个神经网络来打消一些最大误差的影响。而 DQN 中本来就有两个神经网络，我们何不利用一下这个地理优势呢。所以，我们用 <code>Q估计</code> 的神经网络估计 <code>Q现实</code> 中 <code>Qmax(s&#39;, a&#39;)</code> 的最大动作值。然后用这个被 <code>Q估计</code> 估计出来的动作来选择 <code>Q现实</code> 中的 <code>Q(s&#39;)</code>。总结一下：</p>
<p>有两个神经网络：<code>Q_eval</code> (Q估计中的)，<code>Q_next</code> (Q现实中的)。</p>
<p>原本的 <code>Q_next = max(Q_next(s&#39;, a_all))</code>。</p>
<p>Double DQN 中的 <code>Q_next = Q_next(s&#39;, argmax(Q_eval(s&#39;, a_all)))</code>。也可以表达成下面那样。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-5-2.png"></p>
<h4 id="更新方法"><a href="#更新方法" class="headerlink" title="更新方法"></a>更新方法</h4><p>好了，有了理论，我们就来用 Python 实现它吧。</p>
<p>这里的代码都是基于之前 DQN 教程中的代码 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py">(github)</a>，在 <code>RL_brain</code> 中，我们将 class 的名字改成 <code>DoubleDQN</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>double_q</code> 参数来表示使用的是 Natural DQN 还是 Double DQN。为了对比的需要，我们的 <code>tf.Session()</code> 也单独传入。并移除原本在 DQN 代码中的这一句：</p>
<p><code>self.sess.run(tf.global_variables_initializer())</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">..., double_q=<span class="literal">True</span>, sess=<span class="literal">None</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.double_q = double_q</span><br><span class="line">        <span class="keyword">if</span> sess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sess = sess</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<p>接着我们来修改 <code>learn()</code> 中的代码。我们对比 Double DQN 和 Natural DQN 在 tensorboard 中的图，发现他们的结构并没有不同，但是在计算 <code>q_target</code> 的时候，方法是不同的。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-5-3.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 这一段和 DQN 一样:</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % self.replace_target_iter == <span class="number">0</span>:</span><br><span class="line">            self.sess.run(self.replace_target_op)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\ntarget_params_replaced\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.memory_counter &gt; self.memory_size:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)</span><br><span class="line">        batch_memory = self.memory[sample_index, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这一段和 DQN 不一样</span></span><br><span class="line">        q_next, q_eval4next = self.sess.run(</span><br><span class="line">            [self.q_next, self.q_eval],</span><br><span class="line">            feed_dict=&#123;self.s_: batch_memory[:, -self.n_features:],    <span class="comment"># next observation</span></span><br><span class="line">                       self.s: batch_memory[:, -self.n_features:]&#125;)    <span class="comment"># next observation</span></span><br><span class="line">        q_eval = self.sess.run(self.q_eval, &#123;self.s: batch_memory[:, :self.n_features]&#125;)</span><br><span class="line">        q_target = q_eval.copy()</span><br><span class="line">        batch_index = np.arange(self.batch_size, dtype=np.int32)</span><br><span class="line">        eval_act_index = batch_memory[:, self.n_features].astype(<span class="built_in">int</span>)</span><br><span class="line">        reward = batch_memory[:, self.n_features + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.double_q:   <span class="comment"># 如果是 Double DQN</span></span><br><span class="line">            max_act4next = np.argmax(q_eval4next, axis=<span class="number">1</span>)        <span class="comment"># q_eval 得出的最高奖励动作</span></span><br><span class="line">            selected_q_next = q_next[batch_index, max_act4next]  <span class="comment"># Double DQN 选择 q_next 依据 q_eval 选出的动作</span></span><br><span class="line">        <span class="keyword">else</span>:       <span class="comment"># 如果是 Natural DQN</span></span><br><span class="line">            selected_q_next = np.<span class="built_in">max</span>(q_next, axis=<span class="number">1</span>)    <span class="comment"># natural DQN</span></span><br><span class="line"></span><br><span class="line">        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这下面和 DQN 一样:</span></span><br><span class="line">        _, self.cost = self.sess.run([self._train_op, self.loss],</span><br><span class="line">                                     feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                self.q_target: q_target&#125;)</span><br><span class="line">        self.cost_his.append(self.cost)</span><br><span class="line">        self.epsilon = self.epsilon + self.epsilon_increment <span class="keyword">if</span> self.epsilon &lt; self.epsilon_max <span class="keyword">else</span> self.epsilon_max</span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h4 id="记录-Q-值"><a href="#记录-Q-值" class="headerlink" title="记录 Q 值"></a>记录 Q 值</h4><p>为了记录下我们选择动作时的 Q 值，接下来我们就修改 <code>choose_action()</code> 功能，让它记录下每次选择的 Q 值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubleDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, observation</span>):</span></span><br><span class="line">        observation = observation[np.newaxis, :]</span><br><span class="line">        actions_value = self.sess.run(self.q_eval, feed_dict=&#123;self.s: observation&#125;)</span><br><span class="line">        action = np.argmax(actions_value)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;q&#x27;</span>):  <span class="comment"># 记录选的 Qmax 值</span></span><br><span class="line">            self.q = []</span><br><span class="line">            self.running_q = <span class="number">0</span></span><br><span class="line">        self.running_q = self.running_q*<span class="number">0.99</span> + <span class="number">0.01</span> * np.<span class="built_in">max</span>(actions_value)</span><br><span class="line">        self.q.append(self.running_q)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &gt; self.epsilon:  <span class="comment"># 随机选动作</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, self.n_actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>



<h4 id="对比结果"><a href="#对比结果" class="headerlink" title="对比结果"></a>对比结果</h4><p>接着我们就来对比 Natural DQN 和 Double DQN 带来的不同结果啦。<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/run_Pendulum.py">代码在这</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">from</span> RL_brain <span class="keyword">import</span> DoubleDQN</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">env.seed(<span class="number">1</span>) <span class="comment"># 可重复实验</span></span><br><span class="line">MEMORY_SIZE = <span class="number">3000</span></span><br><span class="line">ACTION_SPACE = <span class="number">11</span>    <span class="comment"># 将原本的连续动作分离成 11 个动作</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Natural_DQN&#x27;</span>):</span><br><span class="line">    natural_DQN = DoubleDQN(</span><br><span class="line">        n_actions=ACTION_SPACE, n_features=<span class="number">3</span>, memory_size=MEMORY_SIZE,</span><br><span class="line">        e_greedy_increment=<span class="number">0.001</span>, double_q=<span class="literal">False</span>, sess=sess</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Double_DQN&#x27;</span>):</span><br><span class="line">    double_DQN = DoubleDQN(</span><br><span class="line">        n_actions=ACTION_SPACE, n_features=<span class="number">3</span>, memory_size=MEMORY_SIZE,</span><br><span class="line">        e_greedy_increment=<span class="number">0.001</span>, double_q=<span class="literal">True</span>, sess=sess, output_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">RL</span>):</span></span><br><span class="line">    total_steps = <span class="number">0</span></span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># if total_steps - MEMORY_SIZE &gt; 8000: env.render()</span></span><br><span class="line"></span><br><span class="line">        action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">        f_action = (action-(ACTION_SPACE-<span class="number">1</span>)/<span class="number">2</span>)/((ACTION_SPACE-<span class="number">1</span>)/<span class="number">4</span>)   <span class="comment"># 在 [-2 ~ 2] 内离散化动作</span></span><br><span class="line"></span><br><span class="line">        observation_, reward, done, info = env.step(np.array([f_action]))</span><br><span class="line"></span><br><span class="line">        reward /= <span class="number">10</span>     <span class="comment"># normalize 到这个区间 (-1, 0). 立起来的时候 reward = 0.</span></span><br><span class="line">        <span class="comment"># 立起来以后的 Q target 会变成 0, 因为 Q_target = r + gamma * Qmax(s&#x27;, a&#x27;) = 0 + gamma * 0</span></span><br><span class="line">        <span class="comment"># 所以这个状态时的 Q 值大于 0 时, 就出现了 overestimate.</span></span><br><span class="line"></span><br><span class="line">        RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; MEMORY_SIZE:   <span class="comment"># learning</span></span><br><span class="line">            RL.learn()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> total_steps - MEMORY_SIZE &gt; <span class="number">20000</span>:   <span class="comment"># stop game</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        observation = observation_</span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> RL.q <span class="comment"># 返回所有动作 Q 值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train 两个不同的 DQN</span></span><br><span class="line">q_natural = train(natural_DQN)</span><br><span class="line">q_double = train(double_DQN)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 出对比图</span></span><br><span class="line">plt.plot(np.array(q_natural), c=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;natural&#x27;</span>)</span><br><span class="line">plt.plot(np.array(q_double), c=<span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;double&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Q eval&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;training steps&#x27;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>所以这个出来的图是这样：</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-5-4.png"></p>
<p>可以看出，Natural DQN 学得差不多后，在立起来时，大部分时间都是估计的 Q值要大于0，这时就出现了 overestimate，而 Double DQN 的 Q值就消除了一些 overestimate，将估计值保持在 0 左右。</p>
<h3 id="Prioritized-Experience-Replay-DQN"><a href="#Prioritized-Experience-Replay-DQN" class="headerlink" title="Prioritized Experience Replay (DQN)"></a>Prioritized Experience Replay (DQN)</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>这一次还是使用 MountainCar 来进行实验，因为这次我们不需要重度改变他的 reward 了。所以只要是没有拿到小旗子，reward=-1，拿到小旗子时，我们定义它获得了 +10 的 reward。比起之前 DQN 中，这个 reward 定义更加准确。如果使用这种 reward 定义方式，可以想象 Natural DQN 会花很久的时间学习，因为记忆库中只有很少很少的 +10 reward 可以学习。正负样本不一样。而使用 Prioritized replay，就会重视这种少量的，但值得学习的样本。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/mountaincar%20dqn.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="Prioritized-replay-算法"><a href="#Prioritized-replay-算法" class="headerlink" title="Prioritized replay 算法"></a>Prioritized replay 算法</h4><p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-6-1.png"></p>
<p>这一套算法重点就在我们 batch 抽样的时候并不是随机抽样，而是按照 Memory 中的样本优先级来抽。所以这能更有效地找到我们需要学习的样本。</p>
<p>那么样本的优先级是怎么定的呢？原来我们可以用到 <code>TD-error</code>，也就是 <code>Q现实 - Q估计</code> 来规定优先学习的程度。如果 <code>TD-error</code> 越大，就代表我们的预测精度还有很多上升空间，那么这个样本就越需要被学习，也就是优先级 <code>p</code> 越高。</p>
<p>有了 <code>TD-error</code> 就有了优先级 <code>p</code>，那我们如何有效地根据 <code>p</code> 来抽样呢？如果每次抽样都需要针对 <code>p</code> 对所有样本排序，这将会是一件非常消耗计算能力的事。好在我们还有其他方法，这种方法不会对得到的样本进行排序。这就是这篇 <a href="https://arxiv.org/abs/1511.05952">paper</a> 中提到的 <code>SumTree</code>。</p>
<p>SumTree 是一种树形结构，每片树叶存储每个样本的优先级 <code>p</code>，每个树枝节点只有两个分叉，节点的值是两个分叉的合，所以 SumTree 的顶端就是所有 <code>p</code> 的合。正如下面<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/">图片(来自Jaromír Janisch)</a>，最下面一层树叶存储样本的 <code>p</code>，叶子上一层最左边的 13 = 3 + 10，按这个规律相加，顶层的 root 就是全部 <code>p</code> 的合了。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-6-2.png"></p>
<p>抽样时，我们会将 <code>p</code> 的总合除以 batch size，分成 batch size 那么多区间，(n=sum(p)/batch_size)。如果将所有 node 的 priority 加起来是42的话，我们如果抽6个样本，这时的区间拥有的 priority 可能是这样。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0-7), [7-14), [14-21), [21-28), [28-35), [35-42]</span><br></pre></td></tr></table></figure>

<p>然后在每个区间里随机选取一个数。比如在第区间 <code>&lt;a href=&quot;https://github.com/jaara/AI-blog/blob/master/SumTree.py&quot; target=&#39;_blank&#39; &gt;21-28)</code> 里选到了24，就按照这个 24 从最顶上的42开始向下搜索。首先看到最顶上 <code>42</code> 下面有两个 child nodes，拿着手中的24对比左边的 child <code>29</code>，如果 左边的 child 比自己手中的值大，那我们就走左边这条路，接着再对比 <code>29</code> 下面的左边那个点 <code>13</code>，这时，手中的 24 比 <code>13</code> 大，那我们就走右边的路，并且将手中的值根据 <code>13</code> 修改一下，变成 24-13 = 11。接着拿着 11 和 <code>13</code> 左下角的 <code>12</code> 比，结果 <code>12</code> 比 11 大，那我们就选 12 当做这次选到的 priority，并且也选择 12 对应的数据。</p>
<h4 id="SumTree-有效抽样"><a href="#SumTree-有效抽样" class="headerlink" title="SumTree 有效抽样"></a>SumTree 有效抽样</h4><p><strong>注意: 下面的代码和视频中有一点点不同, 下面的代码是根据评论中讨论的进行了修改, 多谢大家的评论.</strong></p>
<p>首先要提的是，这个 SumTree 的算法是对于 Jaromír Janisch 写的 Sumtree 的修改版。Jaromír Janisch 的代码在更新 sumtree 的时候和抽样的时候多次用到了 recursive 递归结构，我使用的是 while 循环，测试要比递归结构运行快。在 class 中的功能也比它的代码少几个，我优化了一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SumTree</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 建立 tree 和 data,</span></span><br><span class="line">    <span class="comment"># 因为 SumTree 有特殊的数据结构,</span></span><br><span class="line">    <span class="comment"># 所以两者都能用一个一维 np.array 来存储</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, capacity</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当有新 sample 时, 添加进 tree 和 data</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">self, p, data</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当 sample 被 train, 有了新的 TD-error, 就在 tree 中更新</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, tree_idx, p</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据选取的 v 点抽取样本</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_leaf</span>(<span class="params">self, v</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取 sum(priorities)</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">totoal_p</span>(<span class="params">self</span>):</span></span><br></pre></td></tr></table></figure>

<p>具体的抽要和更新值的规则和上面说的类似。具体的代码在这里呈现的话比较累赘，详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L18-L86">Github对应的位置</a></p>
<h4 id="Memory-类"><a href="#Memory-类" class="headerlink" title="Memory 类"></a>Memory 类</h4><p>这个 Memory 类也是基于 <a href="https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py">Jaromír Janisch 所写的 Memory</a> 进行了修改和优化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Memory</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="comment"># 建立 SumTree 和各种参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, capacity</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储数据, 更新 SumTree</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 抽取 sample</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, n</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># train 完被抽取的 samples 后更新在 tree 中的 sample 的 priority</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">batch_update</span>(<span class="params">self, tree_idx, abs_errors</span>):</span></span><br></pre></td></tr></table></figure>

<p>具体的代码在这里呈现的话比较累赘，详细代码请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py#L89-L129">Github对应的位置</a> 下面有很多朋友经常问的一个问题，这个 ISweight 到底怎么算。需要提到的一点是，代码中的计算方法是经过了简化的，将 paper 中的步骤合并了一些。比如 <code>prob = p / self.tree.total_p; ISWeights = np.power(prob/min_prob, -self.beta)</code></p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-6-5.png"></p>
<p>下面是我的推导，如果有不正确还请指出。在paper 中，<code>ISWeight = (N*Pj)^(-beta) / maxi_wi</code> 里面的 <code>maxi_wi</code> 是为了 normalize ISWeight，所以我们先把他放在一边。所以单纯的 importance sampling 就是 <code>(N*Pj)^(-beta)</code>，那 <code>maxi_wi = maxi[(N*Pi)^(-beta)]</code>。</p>
<p>如果将这两个式子合并，</p>
<p><code>ISWeight = (N*Pj)^(-beta) / maxi[ (N*Pi)^(-beta) ]</code></p>
<p>而且如果将 <code>maxi[ (N*Pi)^(-beta) ]</code> 中的 (-beta) 提出来，这就变成了 <code>mini[ (N*Pi) ] ^ (-beta)</code></p>
<p>看出来了吧，有的东西可以抵消掉的。最后</p>
<p><code>ISWeight = (Pj / mini[Pi])^(-beta)</code></p>
<p>这样我们就有了代码中的样子。</p>
<p>还有代码中的 <code>alpha</code> 是一个决定我们要使用多少 ISweight 的影响，如果 <code>alpha = 0</code>，我们就没使用到任何 Importance Sampling。</p>
<h4 id="更新方法-1"><a href="#更新方法-1" class="headerlink" title="更新方法"></a>更新方法</h4><p>基于之前的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.1_Double_DQN/RL_brain.py">DQN 代码</a>，我们做出以下修改。我们将 class 的名字改成 <code>DQNPrioritiedReplay</code>，为了对比 Natural DQN，我们也保留原来大部分的 DQN 的代码。我们在 <code>__init__</code> 中加一个 <code>prioritized</code> 参数来表示 DQN 是否具备 prioritized 能力。为了对比的需要，我们的 <code>tf.Session()</code> 也单独传入。并移除原本在 DQN 代码中的这一句：</p>
<p><code>self.sess.run(tf.global_variables_initializer())</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritiedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">..., prioritized=<span class="literal">True</span>, sess=<span class="literal">None</span></span>)</span></span><br><span class="line">        self.prioritized = prioritized</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            self.memory = Memory(capacity=memory_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.memory = np.zeros((self.memory_size, n_features*<span class="number">2</span>+<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sess = sess</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-6-3.png"></p>
<p>搭建神经网络时，我们发现 DQN with Prioritized replay 只多了一个 <code>ISWeights</code>，这个正是<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/4-6-prioritized-replay/#algorithm">刚刚算法中</a>提到的 <code>Importance-Sampling Weights</code>，用来恢复被 Prioritized replay 打乱的抽样概率分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritizedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>)</span></span><br><span class="line"><span class="function">        ...</span></span><br><span class="line"><span class="function">        # <span class="title">self</span>.<span class="title">prioritized</span> 时 <span class="title">eval</span> <span class="title">net</span> 的 <span class="title">input</span> 多加了一个 <span class="title">ISWeights</span></span></span><br><span class="line">        self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#x27;s&#x27;)  # input</span><br><span class="line">        self.q_target = tf.placeholder(tf.float32, [<span class="literal">None</span>, self.n_actions], name=<span class="string">&#x27;Q_target&#x27;</span>)  <span class="comment"># for calculating loss</span></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            self.ISWeights = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">&#x27;IS_weights&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 为了得到 abs 的 TD error 并用于修改这些 sample 的 priority, 我们修改如下</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> self.prioritized:</span><br><span class="line">                self.abs_errors = tf.reduce_sum(tf.<span class="built_in">abs</span>(self.q_target - self.q_eval), axis=<span class="number">1</span>)    <span class="comment"># for updating Sumtree</span></span><br><span class="line">                self.loss = tf.reduce_mean(self.ISWeights * tf.squared_difference(self.q_target, self.q_eval))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br></pre></td></tr></table></figure>

<p>因为和 Natural DQN 使用的 Memory 不一样，所以在存储 transition 的时候方式也略不相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritizedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:    <span class="comment"># prioritized replay</span></span><br><span class="line">            transition = np.hstack((s, [a, r], s_))</span><br><span class="line">            self.memory.store(transition)</span><br><span class="line">        <span class="keyword">else</span>:       <span class="comment"># random replay</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(self, <span class="string">&#x27;memory_counter&#x27;</span>):</span><br><span class="line">                self.memory_counter = <span class="number">0</span></span><br><span class="line">            transition = np.hstack((s, [a, r], s_))</span><br><span class="line">            index = self.memory_counter % self.memory_size</span><br><span class="line">            self.memory[index, :] = transition</span><br><span class="line">            self.memory_counter += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>接下来是相对于 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py">Natural DQN 代码</a>，我们在 <code>learn()</code> 改变的部分也在如下展示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQNPrioritizedReplay</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># 相对于 DQN 代码, 改变的部分</span></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            tree_idx, batch_memory, ISWeights = self.memory.sample(self.batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sample_index = np.random.choice(self.memory_size, size=self.batch_size)</span><br><span class="line">            batch_memory = self.memory[sample_index, :]</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.prioritized:</span><br><span class="line">            _, abs_errors, self.cost = self.sess.run([self._train_op, self.abs_errors, self.loss],</span><br><span class="line">                                         feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                    self.q_target: q_target,</span><br><span class="line">                                                    self.ISWeights: ISWeights&#125;)</span><br><span class="line">            self.memory.batch_update(tree_idx, abs_errors)   <span class="comment"># update priority</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            _, self.cost = self.sess.run([self._train_op, self.loss],</span><br><span class="line">                                         feed_dict=&#123;self.s: batch_memory[:, :self.n_features],</span><br><span class="line">                                                    self.q_target: q_target&#125;)</span><br><span class="line"></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<h4 id="对比结果-1"><a href="#对比结果-1" class="headerlink" title="对比结果"></a>对比结果</h4><p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-6-4.png"></p>
<p>运行我 Github 中的这个 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/run_MountainCar.py">MountainCar 脚本</a>，我们就不难发现，我们都从两种方法最初拿到第一个 <code>R=+10</code> 奖励的时候算起，看看经历过一次 <code>R=+10</code> 后，他们有没有好好利用这次的奖励，可以看出，有 Prioritized replay 的可以高效的利用这些不常拿到的奖励，并好好学习他们。所以 Prioritized replay 会更快结束每个 episode，很快就到达了小旗子。</p>
<h3 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h3><h4 id="要点-3"><a href="#要点-3" class="headerlink" title="要点"></a>要点</h4><p>只要稍稍修改 DQN 中神经网络的结构，就能大幅提升学习效果，加速收敛。这种新方法叫做 Dueling DQN。用一句话来概括 Dueling DQN 就是。它将每个动作的 Q 拆分成了 state 的 Value 加上每个动作的 Advantage。</p>
<div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;">
<iframe src="https://static.mofanpy.com/results/reinforcement-learning/Pendulum%20DQN.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; Left: 0; top: 0;" ></iframe></div>



<h4 id="Dueling-算法"><a href="#Dueling-算法" class="headerlink" title="Dueling 算法"></a>Dueling 算法</h4><p>上一个 Paper 中的经典解释图片，上者是一般的 DQN 的 Q值神经网络。下者就是 Dueling DQN 中的 Q值神经网络了。那具体是哪里不同了呢？</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-7-1.png"></p>
<p>下面这个公式解释了不同之处。原来 DQN 神经网络直接输出的是每种动作的 Q值，而 Dueling DQN 每个动作的 Q值是由下面的公式确定的。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-7-2.png"></p>
<p>它分成了这个 state 的值，加上每个动作在这个 state 上的 advantage。因为有时候在某种 state，无论做什么动作，对下一个 state 都没有多大影响。比如 paper 中的这张图。</p>
<p><img src="https://static.mofanpy.com/results/reinforcement-learning/4-7-3.png"></p>
<p>这是开车的游戏，左边是 state value，发红的部分证明了 state value 和前面的路线有关，右边是 advantage，发红的部分说明了 advantage 很在乎旁边要靠近的车子，这时的动作会受更多 advantage 的影响。发红的地方左右了自己车子的移动原则。</p>
<h4 id="更新方法-2"><a href="#更新方法-2" class="headerlink" title="更新方法"></a>更新方法</h4><p>下面的修改都是基于我之前写的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py">DQN 代码</a>。这次修改的部分比较少。我们把它们写在一块。如果想直接看全部代码，<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5.3_Dueling_DQN">请戳这里</a>。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-7-4.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuelingDQN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">..., dueling=<span class="literal">True</span>, sess=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function">        ...</span></span><br><span class="line">        self.dueling = dueling  # 会建立两个 DQN, 其中一个是 Dueling DQN</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">if</span> sess <span class="keyword">is</span> <span class="literal">None</span>:    <span class="comment"># 针对建立两个 DQN 的模式修改了 tf.Session() 的建立方式</span></span><br><span class="line">            self.sess = tf.Session()</span><br><span class="line">            self.sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.sess = sess</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_net</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">build_layers</span>(<span class="params">s, c_names, n_l1, w_initializer, b_initializer</span>):</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;l1&#x27;</span>):   <span class="comment"># 第一层, 两种 DQN 都一样</span></span><br><span class="line">                w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, [self.n_features, n_l1], initializer=w_initializer, collections=c_names)</span><br><span class="line">                b1 = tf.get_variable(<span class="string">&#x27;b1&#x27;</span>, [<span class="number">1</span>, n_l1], initializer=b_initializer, collections=c_names)</span><br><span class="line">                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.dueling:</span><br><span class="line">                <span class="comment"># Dueling DQN</span></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Value&#x27;</span>):    <span class="comment"># 专门分析 state 的 Value</span></span><br><span class="line">                    w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, <span class="number">1</span>], initializer=w_initializer, collections=c_names)</span><br><span class="line">                    b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, <span class="number">1</span>], initializer=b_initializer, collections=c_names)</span><br><span class="line">                    self.V = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Advantage&#x27;</span>):    <span class="comment"># 专门分析每种动作的 Advantage</span></span><br><span class="line">                    w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                    b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                    self.A = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):    <span class="comment"># 合并 V 和 A, 为了不让 A 直接学成了 Q, 我们减掉了 A 的均值</span></span><br><span class="line">                    out = self.V + (self.A - tf.reduce_mean(self.A, axis=<span class="number">1</span>, keep_dims=<span class="literal">True</span>))     <span class="comment"># Q = V(s) + A(s,a)</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Q&#x27;</span>):    <span class="comment"># 普通的 DQN 第二层</span></span><br><span class="line">                    w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)</span><br><span class="line">                    b2 = tf.get_variable(<span class="string">&#x27;b2&#x27;</span>, [<span class="number">1</span>, self.n_actions], initializer=b_initializer, collections=c_names)</span><br><span class="line">                    out = tf.matmul(l1, w2) + b2</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>



<h4 id="对比结果-2"><a href="#对比结果-2" class="headerlink" title="对比结果"></a>对比结果</h4><p>对比的代码不在这里呈现，如果想观看对比的详细代码，请去往我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.3_Dueling_DQN/run_Pendulum.py">Github</a>。</p>
<p>这次我们看看累积奖励 reward，杆子立起来的时候奖励 = 0，其他时候都是负值，所以当累积奖励没有在降低时，说明杆子已经被成功立了很久了。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/4-7-5.png"></p>
<p>我们发现当可用动作越高，学习难度就越大，不过 Dueling DQN 还是会比 Natural DQN 学习得更快、收敛效果更好。</p>
<p>如果想一次性看到全部代码，请去我的 <a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/5_Deep_Q_Network">Github</a></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习 (Reinforcement Learning) | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li><a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/6_OpenAI_gym">全部代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">什么是 DQN 短视频</a></li>
<li><a href="https://gym.openai.com/envs/">OpenAI gym 官网</a></li>
<li>本节内容的模拟视频效果:<ul>
<li>CartPole: <a href="https://www.youtube.com/watch?v=qlqqezju0xo">Youtube</a>, <a href="https://www.youtube.com/watch?v=qlqqezju0xo">Youtube</a></li>
<li>Mountain Car: <a href="https://www.youtube.com/watch?v=r1mNIDN3zNM">Youtube</a>, <a href="https://www.youtube.com/watch?v=r1mNIDN3zNM">Youtube</a></li>
</ul>
</li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>论文 <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a></li>
<li>论文 <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a></li>
<li>论文 <a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title>高级神经网络结构（上）</title>
    <url>/YingYingMonstre.github.io/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是卷积神经网络-CNN-Convolutional-Neural-Network"><a href="#什么是卷积神经网络-CNN-Convolutional-Neural-Network" class="headerlink" title="什么是卷积神经网络 CNN (Convolutional Neural Network)"></a>什么是卷积神经网络 CNN (Convolutional Neural Network)</h3><p><img src="https://static.mofanpy.com/results/ML-intro/cnn1.png"></p>
<p>卷积神经网络是近些年逐步兴起的一种人工神经网络结构，因为利用卷积神经网络在图像和语音识别方面能够给出更优预测结果，这一种技术也被广泛的传播和应用。卷积神经网络最常被应用的方面是计算机的图像识别，不过因为不断地创新，它也被应用在视频分析、自然语言处理、药物发现，等等。近期最火的 Alpha Go，让计算机看懂围棋，同样也是有运用到这门技术。</p>
<h4 id="卷积-和-神经网络"><a href="#卷积-和-神经网络" class="headerlink" title="卷积 和 神经网络"></a>卷积 和 神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/cnn2.png"></p>
<p>我们来具体说说卷积神经网络是如何运作的吧，举一个识别图片的例子，我们知道神经网络是由一连串的神经层组成，每一层神经层里面存在有很多的神经元。这些神经元就是神经网络识别事物的关键。每一种神经网络都会有输入输出值，当输入值是图片的时候，实际上输入神经网络的并不是那些色彩缤纷的图案，而是一堆堆的数字。就比如说这个。当神经网络需要处理这么多输入信息的时候，也就是卷积神经网络就可以发挥它的优势的时候了。那什么是卷积神经网络呢？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/cnn3.png"></p>
<p>我们先把卷积神经网络这个词拆开来看。“卷积” 和 “神经网络”。卷积也就是说神经网络不再是对每个像素的输入信息做处理了，而是图片上每一小块像素区域进行处理，这种做法加强了图片信息的连续性。使得神经网络能看到图形，而非一个点。这种做法同时也加深了神经网络对图片的理解。具体来说，卷积神经网络有一个批量过滤器，持续不断的在图片上滚动收集图片里的信息，每一次收集的时候都只是收集一小块像素区域，然后把收集来的信息进行整理，这时候整理出来的信息有了一些实际上的呈现，比如这时的神经网络能看到一些边缘的图片信息，然后在以同样的步骤，用类似的批量过滤器扫过产生的这些边缘信息，神经网络从这些边缘信息里面总结出更高层的信息结构，比如说总结的边缘能够画出眼睛、鼻子等等。再经过一次过滤，脸部的信息也从这些眼睛鼻子的信息中被总结出来。最后我们再把这些信息套入几层普通的全连接神经层进行分类，这样就能得到输入的图片能被分为哪一类的结果了。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/cnn4.png"></p>
<p>我们截取一段 google 介绍卷积神经网络的视频，具体说说图片是如何被卷积的。下面是一张猫的图片，图片有长、宽、高三个参数。对！图片是有高度的！这里的高指的是计算机用于产生颜色使用的信息。如果是黑白照片的话，高的单位就只有1，如果是彩色照片，就可能有红绿蓝三种颜色的信息，这时的高度为3。我们以彩色照片为例子。过滤器就是影像中不断移动的东西，他不断在图片收集小批小批的像素块，收集完所有信息后，输出的值，我们可以理解成是一个高度更高，长和宽更小的”图片”。这个图片里就能包含一些边缘信息。然后以同样的步骤再进行多次卷积，将图片的长宽再压缩，高度再增加，就有了对输入图片更深的理解。将压缩，增高的信息嵌套在普通的分类神经层上，我们就能对这种图片进行分类了。</p>
<h4 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化(pooling)"></a>池化(pooling)</h4><p><img src="https://static.mofanpy.com/results/ML-intro/cnn5.png"></p>
<p>研究发现，在每一次卷积的时候，神经层可能会无意地丢失一些信息。这时，池化 (pooling) 就可以很好地解决这一问题。而且池化是一个筛选过滤的过程，能将 layer 中有用的信息筛选出来，给下一个层分析。同时也减轻了神经网络的计算负担 (<a href="http://cs231n.github.io/convolutional-networks/#pool">具体细节参考</a>)。也就是说在卷集的时候，我们不压缩长宽，尽量地保留更多信息，压缩的工作就交给池化了，这样的一项附加工作能够很有效的提高准确性。有了这些技术，我们就可以搭建一个属于我们自己的卷积神经网络啦。</p>
<h4 id="流行的-CNN-结构"><a href="#流行的-CNN-结构" class="headerlink" title="流行的 CNN 结构"></a>流行的 CNN 结构</h4><p><img src="https://static.mofanpy.com/results/ML-intro/cnn6.png"></p>
<p>比较流行的一种搭建结构是这样，从下到上的顺序，首先是输入的图片(image)，经过一层卷积层 (convolution)，然后在用池化(pooling)方式处理卷积的信息，这里使用的是 max pooling 的方式。然后在经过一次同样的处理，把得到的第二次处理的信息传入两层全连接的神经层 (fully connected)，这也是一般的两层神经网络层，最后在接上一个分类器(classifier)进行分类预测。这仅仅是对卷积神经网络在图片处理上一次简单的介绍。如果你想知道使用 python 搭建这样的卷积神经网络，欢迎点击下面的内容。</p>
<p><em>Python 教程</em></p>
<ul>
<li><em>Tensorflow CNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/CNN1">教程</a></em></li>
<li><em>PyTorch CNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/CNN">教程</a></em></li>
<li><em>方便快捷的 Keras CNN<a href="https://mofanpy.com/tutorials/machine-learning/keras/CNN">教程</a></em></li>
</ul>
<h3 id="CNN-卷积神经网络"><a href="#CNN-卷积神经网络" class="headerlink" title="CNN 卷积神经网络"></a>CNN 卷积神经网络</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>卷积神经网络目前被广泛地用在图片识别上，已经有层出不穷的应用，如果你对卷积神经网络还没有特别了解，<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-CNN">我制作的 卷积神经网络 动画简介</a> 能让你花几分钟就了解什么是卷积神经网络。接着我们就一步一步做一个分析手写数字的 CNN 吧。</p>
<p>下面是一个 CNN 最后一层的学习过程，我们先可视化看看：</p>
<p><img src="https://static.mofanpy.com/results/torch/4-1-2.gif"></p>
<h4 id="MNIST手写数据"><a href="#MNIST手写数据" class="headerlink" title="MNIST手写数据"></a>MNIST手写数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision      <span class="comment"># 数据库模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>           <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">50</span></span><br><span class="line">LR = <span class="number">0.001</span>          <span class="comment"># 学习率</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mnist 手写数字</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./mnist/&#x27;</span>,    <span class="comment"># 保存或者提取位置</span></span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># this is training data</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),    <span class="comment"># 转换 PIL.Image or numpy.ndarray 成</span></span><br><span class="line">                                                    <span class="comment"># torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间</span></span><br><span class="line">    download=DOWNLOAD_MNIST,          <span class="comment"># 没下载就下载, 下载了就不用再下了</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/4-1-1.png"></p>
<p>黑色的地方的值都是0，白色的地方值大于0。</p>
<p>同样，我们除了训练数据，还给一些测试数据，测试看看它有没有训练好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./mnist/&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></span><br><span class="line">train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了节约时间, 我们测试时只测试前2000个</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255.</span>   <span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>



<h4 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h4><p>和以前一样，我们用一个 class 来建立 CNN 模型。这个 CNN 整体流程是 卷积(<code>Conv2d</code>) -&gt; 激励函数(<code>ReLU</code>) -&gt; 池化, 向下采样 (<code>MaxPooling</code>) -&gt; 再来一遍 -&gt; 展平多维的卷积成的特征图 -&gt; 接入全连接层 (<code>Linear</code>) -&gt; 输出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(  <span class="comment"># input shape (1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=<span class="number">1</span>,      <span class="comment"># input height</span></span><br><span class="line">                out_channels=<span class="number">16</span>,    <span class="comment"># n_filters</span></span><br><span class="line">                kernel_size=<span class="number">5</span>,      <span class="comment"># filter size</span></span><br><span class="line">                stride=<span class="number">1</span>,           <span class="comment"># filter movement/step</span></span><br><span class="line">                padding=<span class="number">2</span>,      <span class="comment"># 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1</span></span><br><span class="line">            ),      <span class="comment"># output shape (16, 28, 28)</span></span><br><span class="line">            nn.ReLU(),    <span class="comment"># activation</span></span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),    <span class="comment"># 在 2x2 空间里向下采样, output shape (16, 14, 14)</span></span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(  <span class="comment"># input shape (16, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># output shape (32, 14, 14)</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># activation</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),  <span class="comment"># output shape (32, 7, 7)</span></span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">10</span>)   <span class="comment"># fully connected layer, output 10 classes</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)   <span class="comment"># 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</span></span><br><span class="line">        output = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">cnn = CNN()</span><br><span class="line"><span class="built_in">print</span>(cnn)  <span class="comment"># net architecture</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">CNN (</span></span><br><span class="line"><span class="string">  (conv1): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (conv2): Sequential (</span></span><br><span class="line"><span class="string">    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span></span><br><span class="line"><span class="string">    (1): ReLU ()</span></span><br><span class="line"><span class="string">    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (out): Linear (1568 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>下面我们开始训练，将 <code>x</code> <code>y</code> 都用 <code>Variable</code> 包起来，然后放入 <code>cnn</code> 中计算 <code>output</code>, 最后再计算误差。下面代码省略了计算精确度 <code>accuracy</code> 的部分，如果想细看 <code>accuracy</code> 代码的同学，请去往我的 <a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/401_CNN.py">github</a> 看全部代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   <span class="comment"># optimize all cnn parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()   <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (b_x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):   <span class="comment"># 分配 batch data, normalize x when iterate train_loader</span></span><br><span class="line">        output = cnn(b_x)               <span class="comment"># cnn output</span></span><br><span class="line">        loss = loss_func(output, b_y)   <span class="comment"># cross entropy loss</span></span><br><span class="line">        optimizer.zero_grad()           <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                 <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                <span class="comment"># apply gradients</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0306 | test accuracy: 0.97</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0147 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0427 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0078 | test accuracy: 0.98</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>最后我们再来取10个数据，看看预测的值到底对不对：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_output = cnn(test_x[:<span class="number">10</span>])</span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line"><span class="built_in">print</span>(pred_y, <span class="string">&#x27;prediction number&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">10</span>].numpy(), <span class="string">&#x27;real number&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9] prediction number</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9] real number</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="可视化训练-视频中没有"><a href="#可视化训练-视频中没有" class="headerlink" title="可视化训练(视频中没有)"></a>可视化训练(视频中没有)</h4><p>这是做完视频后突然想要补充的内容，因为可视化可以帮助理解，所以还是有必要提一下。可视化的代码主要是用 <code>matplotlib</code> 和 <code>sklearn</code> 来完成的，因为其中我们用到了 <code>T-SNE</code> 的降维手段，将高维的 CNN 最后一层输出结果可视化，也就是 CNN forward 代码中的 <code>x = x.view(x.size(0), -1)</code> 这一个结果。</p>
<p>可视化的代码不是重点，我们就直接展示可视化的结果吧。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-1-2.gif"></p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="什么是循环神经网络-RNN-Recurrent-Neural-Network"><a href="#什么是循环神经网络-RNN-Recurrent-Neural-Network" class="headerlink" title="什么是循环神经网络 RNN (Recurrent Neural Network)"></a>什么是循环神经网络 RNN (Recurrent Neural Network)</h3><p>今天我们会来聊聊在语言分析，序列化数据中穿梭自如的循环神经网络 RNN。RNN 是用来干什么的？它和普通的神经网络有什么不同？我会将会一一探讨。</p>
<h4 id="RNN-的用途"><a href="#RNN-的用途" class="headerlink" title="RNN 的用途"></a>RNN 的用途</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/rnn1.png"></p>
<p>现在请你看着这个名字。不出意外，你应该可以脱口而出。因为你很可能就用了他们家的一款产品。那么现在，请抛开这个产品，只想着斯蒂芬乔布斯这个名字，请你再把他逆序念出来。斯布乔(*#&amp;, 有点难吧。这就说明，对于预测，顺序排列是多么重要。我们可以预测下一个按照一定顺序排列的字，但是打乱顺序，我们就没办法分析自己到底在说什么了。</p>
<h4 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/rnn2.png"></p>
<p>我们想象现在有一组序列数据 data 0,1,2,3。在当预测 result0 的时候，我们基于的是 data0，同样在预测其他数据的时候，我们也都只单单基于单个的数据。每次使用的神经网络都是同一个 NN。不过这些数据是有关联顺序的 ，就像在厨房做菜，酱料 A要比酱料 B 早放，不然就串味了。所以普通的神经网络结构并不能让 NN 了解这些数据之间的关联。</p>
<h4 id="处理序列数据的神经网络"><a href="#处理序列数据的神经网络" class="headerlink" title="处理序列数据的神经网络"></a>处理序列数据的神经网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/rnn3.png"></p>
<p>那我们如何让数据间的关联也被 NN 加以分析呢？想想我们人类是怎么分析各种事物的关联吧，最基本的方式，就是记住之前发生的事情。那我们让神经网络也具备这种记住之前发生的事的能力。再分析 Data0 的时候，我们把分析结果存入记忆。然后当分析 data1的时候，NN会产生新的记忆，但是新记忆和老记忆是没有联系的。我们就简单的把老记忆调用过来，一起分析。如果继续分析更多的有序数据，RNN就会把之前的记忆都累积起来，一起分析。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/rnn4.png"></p>
<p>我们再重复一遍刚才的流程，不过这次是以加入一些数学方面的东西。每次 RNN 运算完之后都会产生一个对于当前状态的描述，state。我们用简写 S(t) 代替，然后这个 RNN开始分析 x(t+1)，他会根据 x(t+1)产生s(t+1)，不过此时 y(t+1) 是由 s(t) 和 s(t+1) 共同创造的。所以我们通常看到的 RNN 也可以表达成这种样子。</p>
<h4 id="RNN-的应用"><a href="#RNN-的应用" class="headerlink" title="RNN 的应用"></a>RNN 的应用</h4><p>RNN 的形式不单单这有这样一种，他的结构形式很自由。如果用于分类问题，比如说一个人说了一句话，这句话带的感情色彩是积极的还是消极的。那我们就可以用只有最后一个时间点输出判断结果的RNN。</p>
<p>又或者这是图片描述 RNN，我们只需要一个 X 来代替输入的图片，然后生成对图片描述的一段话。</p>
<p>或者是语言翻译的 RNN，给出一段英文，然后再翻译成中文。</p>
<p>有了这些不同形式的 RNN，RNN 就变得强大了。有很多有趣的 RNN 应用。比如之前提到的，让 RNN 描述照片、让 RNN 写学术论文、让 RNN 写程序脚本、让 RNN 作曲。我们一般人甚至都不能分辨这到底是不是机器写出来的。</p>
<p><em>Python相关教程</em></p>
<ul>
<li><em>Tensorflow RNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/RNN2">例子</a></em></li>
<li><em>PyTorch RNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/RNN-classification">例子</a></em></li>
<li><em>Keras <a href="https://mofanpy.com/tutorials/machine-learning/keras/RNN-classifier">快速搭建 RNN</a></em></li>
</ul>
<h3 id="什么是-LSTM-循环神经网络"><a href="#什么是-LSTM-循环神经网络" class="headerlink" title="什么是 LSTM 循环神经网络"></a>什么是 LSTM 循环神经网络</h3><p>今天我们会来聊聊在普通RNN的弊端和为了解决这个弊端而提出的 LSTM 技术。LSTM 是 long-short term memory 的简称，中文叫做长短期记忆。是当下最流行的 RNN 形式之一。</p>
<h4 id="RNN-的弊端"><a href="#RNN-的弊端" class="headerlink" title="RNN 的弊端"></a>RNN 的弊端</h4><p><img src="https://static.mofanpy.com/results/ML-intro/lstm1.png"></p>
<p>之前我们说过，<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-RNN">RNN</a> 是在有顺序的数据上进行学习的。为了记住这些数据，RNN 会像人一样产生对先前发生事件的记忆。不过一般形式的 RNN 就像一个老爷爷，有时候比较健忘。为什么会这样呢？</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/lstm2.png"></p>
<p>想像现在有这样一个 RNN，他的输入值是一句话：‘我今天要做红烧排骨，首先要准备排骨，然后….，最后美味的一道菜就出锅了’，shua ~ 说着说着就流口水了。现在请 RNN 来分析，我今天做的到底是什么菜呢。RNN可能会给出“辣子鸡”这个答案。由于判断失误，RNN就要开始学习这个长序列 X 和 ‘红烧排骨’ 的关系，而RNN需要的关键信息 ”红烧排骨”却出现在句子开头。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/lstm3.png"></p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/lstm4.png"></p>
<p>再来看看 RNN是怎样学习的吧。红烧排骨这个信息原的记忆要进过长途跋涉才能抵达最后一个时间点。然后我们得到误差，而且在 反向传递得到的误差的时候，他在每一步都会乘以一个自己的参数 W。如果这个 W 是一个小于1 的数，比如0.9。这个0.9不断乘以误差，误差传到初始时间点也会是一个接近于零的数，所以对于初始时刻，误差相当于就消失了。我们把这个问题叫做梯度消失或者梯度弥散 Gradient vanishing。反之如果 W 是一个大于1 的数，比如1.1。不断累乘，则到最后变成了无穷大的数，RNN被这无穷大的数撑死了，这种情况我们叫做剃度爆炸，Gradient exploding。这就是普通 RNN 没有办法回忆起久远记忆的原因。</p>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p><img src="https://static.mofanpy.com/results/ML-intro/lstm5.png"></p>
<p>LSTM 就是为了解决这个问题而诞生的。LSTM 和普通 RNN 相比，多出了三个控制器。(输入控制, 输出控制, 忘记控制)。现在，LSTM RNN 内部的情况是这样。</p>
<p>他多了一个控制全局的记忆，我们用粗线代替。为了方便理解，我们把粗线想象成电影或游戏当中的主线剧情。而原本的 RNN 体系就是分线剧情。三个控制器都是在原始的 RNN 体系上，我们先看输入方面，如果此时的分线剧情对于剧终结果十分重要，输入控制就会将这个分线剧情按重要程度写入主线剧情进行分析。再看忘记方面，如果此时的分线剧情更改了我们对之前剧情的想法，那么忘记控制就会将之前的某些主线剧情忘记，按比例替换成现在的新剧情。所以主线剧情的更新就取决于输入和忘记控制。最后的输出方面，输出控制会基于目前的主线剧情和分线剧情判断要输出的到底是什么。基于这些控制机制，LSTM 就像延缓记忆衰退的良药，可以带来更好的结果。</p>
<h3 id="RNN-循环神经网络-分类"><a href="#RNN-循环神经网络-分类" class="headerlink" title="RNN 循环神经网络 (分类)"></a>RNN 循环神经网络 (分类)</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>循环神经网络让神经网络有了记忆，对于序列话的数据，循环神经网络能达到更好的效果。如果你对循环神经网络还没有特别了解，请观看几分钟的短动画，<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-RNN">RNN 动画简介</a> 和 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-LSTM">LSTM 动画简介</a> 能让你生动理解 RNN。接着我们就一步一步做一个分析手写数字的 RNN 吧。</p>
<h4 id="MNIST手写数据-1"><a href="#MNIST手写数据-1" class="headerlink" title="MNIST手写数据"></a>MNIST手写数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dsets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">EPOCH = <span class="number">1</span>           <span class="comment"># 训练整批数据多少次, 为了节约时间, 我们只训练一次</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">TIME_STEP = <span class="number">28</span>      <span class="comment"># rnn 时间步数 / 图片高度</span></span><br><span class="line">INPUT_SIZE = <span class="number">28</span>     <span class="comment"># rnn 每步输入值 / 图片每行像素</span></span><br><span class="line">LR = <span class="number">0.01</span>           <span class="comment"># learning rate</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>  <span class="comment"># 如果你已经下载好了mnist数据就写上 Fasle</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mnist 手写数字</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./mnist/&#x27;</span>,    <span class="comment"># 保存或者提取位置</span></span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># this is training data</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),    <span class="comment"># 转换 PIL.Image or numpy.ndarray 成</span></span><br><span class="line">                                                    <span class="comment"># torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间</span></span><br><span class="line">    download=DOWNLOAD_MNIST,          <span class="comment"># 没下载就下载, 下载了就不用再下了</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/4-1-1.png"></p>
<p>黑色的地方的值都是0，白色的地方值大于0。</p>
<p>同样，我们除了训练数据，还给一些测试数据，测试看看它有没有训练好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_data = torchvision.datasets.MNIST(root=<span class="string">&#x27;./mnist/&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)</span></span><br><span class="line">train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了节约时间, 我们测试时只测试前2000个</span></span><br><span class="line">test_x = torch.unsqueeze(test_data.test_data, dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.FloatTensor)[:<span class="number">2000</span>]/<span class="number">255.</span>   <span class="comment"># shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)</span></span><br><span class="line">test_y = test_data.test_labels[:<span class="number">2000</span>]</span><br></pre></td></tr></table></figure>



<h4 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h4><p>和以前一样，我们用一个 class 来建立 RNN 模型。这个 RNN 整体流程是</p>
<ol>
<li><code>(input0, state0)</code> -&gt; <code>LSTM</code> -&gt; <code>(output0, state1)</code>;</li>
<li><code>(input1, state1)</code> -&gt; <code>LSTM</code> -&gt; <code>(output1, state2)</code>;</li>
<li>…</li>
<li><code>(inputN, stateN)</code>-&gt; <code>LSTM</code> -&gt; <code>(outputN, stateN+1)</code>;</li>
<li><code>outputN</code> -&gt; <code>Linear</code> -&gt; <code>prediction</code>. 通过<code>LSTM</code>分析每一时刻的值，并且将这一时刻和前面时刻的理解合并在一起，生成当前时刻对前面数据的理解或记忆。传递这种理解给下一时刻分析。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.LSTM(     <span class="comment"># LSTM 效果要比 nn.RNN() 好多了</span></span><br><span class="line">            input_size=<span class="number">28</span>,      <span class="comment"># 图片每行的数据像素点</span></span><br><span class="line">            hidden_size=<span class="number">64</span>,     <span class="comment"># rnn hidden unit</span></span><br><span class="line">            num_layers=<span class="number">1</span>,       <span class="comment"># 有几层 RNN layers</span></span><br><span class="line">            batch_first=<span class="literal">True</span>,   <span class="comment"># input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)    <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># x shape (batch, time_step, input_size)</span></span><br><span class="line">        <span class="comment"># r_out shape (batch, time_step, output_size)</span></span><br><span class="line">        <span class="comment"># h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线</span></span><br><span class="line">        <span class="comment"># h_c shape (n_layers, batch, hidden_size)</span></span><br><span class="line">        r_out, (h_n, h_c) = self.rnn(x, <span class="literal">None</span>)   <span class="comment"># None 表示 hidden state 会用全0的 state</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选取最后一个时间点的 r_out 输出</span></span><br><span class="line">        <span class="comment"># 这里 r_out[:, -1, :] 的值也是 h_n 的值</span></span><br><span class="line">        out = self.out(r_out[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">rnn = RNN()</span><br><span class="line"><span class="built_in">print</span>(rnn)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RNN (</span></span><br><span class="line"><span class="string">  (rnn): LSTM(28, 64, batch_first=True)</span></span><br><span class="line"><span class="string">  (out): Linear (64 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>我们将图片数据看成一个时间上的连续数据，每一行的像素点都是这个时刻的输入，读完整张图片就是从上而下的读完了每行的像素点。然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了。下面的代码省略了计算 <code>accuracy</code> 的部分，你可以在我的 github 中看到全部代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   <span class="comment"># optimize all parameters</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()   <span class="comment"># the target label is not one-hotted</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># training and testing</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, b_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):   <span class="comment"># gives batch data</span></span><br><span class="line">        b_x = x.view(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)   <span class="comment"># reshape x to (batch, time_step, input_size)</span></span><br><span class="line"></span><br><span class="line">        output = rnn(b_x)               <span class="comment"># rnn output</span></span><br><span class="line">        loss = loss_func(output, b_y)   <span class="comment"># cross entropy loss</span></span><br><span class="line">        optimizer.zero_grad()           <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                 <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                <span class="comment"># apply gradients</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0945 | test accuracy: 0.94</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0984 | test accuracy: 0.94</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.0332 | test accuracy: 0.95</span></span><br><span class="line"><span class="string">Epoch:  0 | train loss: 0.1868 | test accuracy: 0.96</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>最后我们再来取10个数据，看看预测的值到底对不对：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_output = rnn(test_x[:<span class="number">10</span>].view(-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">pred_y = torch.<span class="built_in">max</span>(test_output, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line"><span class="built_in">print</span>(pred_y, <span class="string">&#x27;prediction number&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(test_y[:<span class="number">10</span>], <span class="string">&#x27;real number&#x27;</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9] prediction number</span></span><br><span class="line"><span class="string">[7 2 1 0 4 1 4 9 5 9] real number</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/402_RNN_classifier.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="RNN-循环神经网络-回归"><a href="#RNN-循环神经网络-回归" class="headerlink" title="RNN 循环神经网络 (回归)"></a>RNN 循环神经网络 (回归)</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>循环神经网络让神经网络有了记忆，对于序列话的数据,循环神经网络能达到更好的效果。如果你对循环神经网络还没有特别了解，请观看几分钟的短动画，<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-RNN">RNN 动画简介</a> 和 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-LSTM">LSTM 动画简介</a> 能让你生动理解 RNN。上次我们提到了用 RNN 的最后一个时间点输出来判断之前看到的图片属于哪一类，这次我们来真的了，用 RNN 来及时预测时间序列。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-3-1.gif"></p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>我们要用到的数据就是这样的一些数据，我们想要用 <code>sin</code> 的曲线预测出 <code>cos</code> 的曲线。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-3-2.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyper Parameters</span></span><br><span class="line">TIME_STEP = <span class="number">10</span>      <span class="comment"># rnn time step / image height</span></span><br><span class="line">INPUT_SIZE = <span class="number">1</span>      <span class="comment"># rnn input size / image width</span></span><br><span class="line">LR = <span class="number">0.02</span>           <span class="comment"># learning rate</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">False</span>  <span class="comment"># set to True if haven&#x27;t download the data</span></span><br></pre></td></tr></table></figure>



<h4 id="RNN模型-1"><a href="#RNN模型-1" class="headerlink" title="RNN模型"></a>RNN模型</h4><p>这一次的 RNN，我们对每一个 <code>r_out</code> 都得放到 <code>Linear</code> 中去计算出预测的 <code>output</code>，所以我们能用一个 for loop 来循环计算。<strong>这点是 Tensorflow 望尘莫及的!</strong> 除了这点，还有一些动态的过程都可以在<a href="https://mofanpy.com/tutorials/machine-learning/torch/dynamic">这个教程</a>中查看，看看我们的 PyTorch 和 Tensorflow 到底哪家强。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.RNN(  <span class="comment"># 这回一个普通的 RNN 就能胜任</span></span><br><span class="line">            input_size=<span class="number">1</span>,</span><br><span class="line">            hidden_size=<span class="number">32</span>,     <span class="comment"># rnn hidden unit</span></span><br><span class="line">            num_layers=<span class="number">1</span>,       <span class="comment"># 有几层 RNN layers</span></span><br><span class="line">            batch_first=<span class="literal">True</span>,   <span class="comment"># input &amp; output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)</span></span><br><span class="line">        )</span><br><span class="line">        self.out = nn.Linear(<span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, h_state</span>):</span>  <span class="comment"># 因为 hidden state 是连续的, 所以我们要一直传递这一个 state</span></span><br><span class="line">        <span class="comment"># x (batch, time_step, input_size)</span></span><br><span class="line">        <span class="comment"># h_state (n_layers, batch, hidden_size)</span></span><br><span class="line">        <span class="comment"># r_out (batch, time_step, output_size)</span></span><br><span class="line">        r_out, h_state = self.rnn(x, h_state)   <span class="comment"># h_state 也要作为 RNN 的一个输入</span></span><br><span class="line"></span><br><span class="line">        outs = []    <span class="comment"># 保存所有时间点的预测值</span></span><br><span class="line">        <span class="keyword">for</span> time_step <span class="keyword">in</span> <span class="built_in">range</span>(r_out.size(<span class="number">1</span>)):    <span class="comment"># 对每一个时间点计算 output</span></span><br><span class="line">            outs.append(self.out(r_out[:, time_step, :]))</span><br><span class="line">        <span class="keyword">return</span> torch.stack(outs, dim=<span class="number">1</span>), h_state</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rnn = RNN()</span><br><span class="line"><span class="built_in">print</span>(rnn)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RNN (</span></span><br><span class="line"><span class="string">  (rnn): RNN(1, 32, batch_first=True)</span></span><br><span class="line"><span class="string">  (out): Linear (32 -&gt; 1)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>其实熟悉 RNN 的朋友应该知道，<code>forward</code> 过程中的对每个时间点求输出还有一招使得计算量比较小的。不过上面的内容主要是为了呈现 PyTorch 在动态构图上的优势，所以我用了一个 <code>for loop</code> 来搭建那套输出系统。下面介绍一个替换方式。使用 reshape 的方式整批计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, h_state</span>):</span></span><br><span class="line">    r_out, h_state = self.rnn(x, h_state)</span><br><span class="line">    r_out = r_out.view(-<span class="number">1</span>, <span class="number">32</span>)</span><br><span class="line">    outs = self.out(r_out)</span><br><span class="line">    <span class="keyword">return</span> outs.view(-<span class="number">1</span>, <span class="number">32</span>, TIME_STEP), h_state</span><br></pre></td></tr></table></figure>



<h4 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h4><p>下面的代码就能实现动图的效果啦~开心，可以看出，我们使用 <code>x</code> 作为输入的 <code>sin</code> 值，然后 <code>y</code> 作为想要拟合的输出，<code>cos</code> 值。因为他们两条曲线是存在某种关系的，所以我们就能用 <code>sin</code> 来预测 <code>cos</code>。<code>rnn</code> 会理解他们的关系，并用里面的参数分析出来这个时刻 <code>sin</code> 曲线上的点如何对应上 <code>cos</code> 曲线上的点。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-3-1.gif"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   <span class="comment"># optimize all rnn parameters</span></span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">h_state = <span class="literal">None</span>   <span class="comment"># 要使用初始 hidden state, 可以设成 None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    start, end = step * np.pi, (step+<span class="number">1</span>)*np.pi   <span class="comment"># time steps</span></span><br><span class="line">    <span class="comment"># sin 预测 cos</span></span><br><span class="line">    steps = np.linspace(start, end, <span class="number">10</span>, dtype=np.float32)</span><br><span class="line">    x_np = np.sin(steps)    <span class="comment"># float32 for converting torch FloatTensor</span></span><br><span class="line">    y_np = np.cos(steps)</span><br><span class="line"></span><br><span class="line">    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    <span class="comment"># shape (batch, time_step, input_size)</span></span><br><span class="line">    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])</span><br><span class="line"></span><br><span class="line">    prediction, h_state = rnn(x, h_state)   <span class="comment"># rnn 对于每个 step 的 prediction, 还有最后一个 step 的 h_state</span></span><br><span class="line">    <span class="comment"># !!  下一步十分重要 !!</span></span><br><span class="line">    h_state = h_state.data  <span class="comment"># 要把 h_state 重新包装一下才能放入下一个 iteration, 不然会报错</span></span><br><span class="line"></span><br><span class="line">    loss = loss_func(prediction, y)     <span class="comment"># cross entropy loss</span></span><br><span class="line">    optimizer.zero_grad()               <span class="comment"># clear gradients for this training step</span></span><br><span class="line">    loss.backward()                     <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">    optimizer.step()                    <span class="comment"># apply gradients</span></span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/4-3-3.png"></p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py">github 代码</a> 中的每一步的意义啦。</p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li>Tensorflow CNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/CNN1">教程1</a></li>
<li>Tensorflow CNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/CNN2">教程2</a></li>
<li>Tensorflow CNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/CNN3">教程3</a></li>
<li>PyTorch CNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/CNN">教程</a></li>
<li>方便快捷的 Keras CNN<a href="https://mofanpy.com/tutorials/machine-learning/keras/CNN">教程</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/401_CNN.py">第二节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/401_CNN.py">Tensorflow 的 50行 CNN 代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-CNN">我制作的 卷积神经网络 动画简介</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
<hr>
<ul>
<li>Tensorflow RNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/RNN1">例子1</a></li>
<li>Tensorflow RNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/RNN2">例子2</a></li>
<li>Tensorflow RNN <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/RNN3">例子3</a></li>
<li>PyTorch RNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/RNN-classification">例子1</a></li>
<li>PyTorch RNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/RNN-regression">例子2</a></li>
<li>Keras <a href="https://mofanpy.com/tutorials/machine-learning/keras/RNN-classifier">快速搭建 RNN 1</a></li>
<li>Keras <a href="https://mofanpy.com/tutorials/machine-learning/keras/RNN-LSTM-Regressor">快速搭建 RNN 2</a></li>
<li>RNN 作曲 <a href="http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/">链接</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/402_RNN_classifier.py">第五节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/402_RNN_classification.py">Tensorflow 的 50行 RNN 代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-RNN">我制作的 循环神经网络 RNN 动画简介</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-LSTM">我制作的 循环神经网络 LSTM 动画简介</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/403_RNN_regressor.py">第六节的全部代码</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch 神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>高级神经网络结构（下）</title>
    <url>/YingYingMonstre.github.io/2021/12/12/%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%EF%BC%88%E4%B8%8B%EF%BC%89/</url>
    <content><![CDATA[<h3 id="什么是自编码-Autoencoder"><a href="#什么是自编码-Autoencoder" class="headerlink" title="什么是自编码 (Autoencoder)"></a>什么是自编码 (Autoencoder)</h3><p>今天我们会来聊聊用神经网络如何进行非监督形式的学习。也就是 autoencoder，自编码。</p>
<p>自编码 autoencoder 是一种什么码呢。他是不是条形码？二维码？打码？其中的一种呢？NONONONO。和他们统统没有关系。自编码是一种神经网络的形式。如果你一定要把他们扯上关系，我想也只能这样解释啦。</p>
<h4 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/auto1.png"></p>
<p>有一个神经网络，它在做的事情是接收一张图片，然后 给它打码，最后再从打码后的图片中还原。太抽象啦？行，我们再具体点。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto2.png"></p>
<p>假设刚刚那个神经网络是这样，对应上刚刚的图片，可以看出图片其实是经过了压缩，再解压的这一道工序。当压缩的时候，原有的图片质量被缩减，解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片。为什么要这样做呢？</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto3.png"></p>
<p>原来有时神经网络要接受大量的输入信息，比如输入信息是高清图片时，输入信息量可能达到上千万，让神经网络直接从上千万个信息源中学习是一件很吃力的工作。所以，何不压缩一下，提取出原图片中的最具代表性的信息，缩减输入信息量，再把缩减过后的信息放进神经网络学习。这样学习起来就简单轻松了。所以，自编码就能在这时发挥作用。通过将原数据白色的X 压缩，解压成黑色的X，然后通过对比黑白 X，求出预测误差，进行反向传递，逐步提升自编码的准确性。训练好的自编码中间这一部分就是能总结原数据的精髓。可以看出，从头到尾，我们只用到了输入数据 X，并没有用到 X 对应的数据标签，所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候。通常只会用到自编码前半部分。</p>
<h4 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h4><p><img src="https://static.mofanpy.com/results/ML-intro/auto4.png"></p>
<p>这部分也叫作 encoder 编码器。编码器能得到原数据的精髓，然后我们只需要再创建一个小的神经网络学习这个精髓的数据，不仅减少了神经网络的负担，而且同样能达到很好的效果。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto5.png"></p>
<p>这是一个通过自编码整理出来的数据，他能从原数据中总结出每种类型数据的特征，如果把这些特征类型都放在一张二维的图片上，每种类型都已经被很好的用原数据的精髓区分开来。如果你了解 PCA 主成分分析，再提取主要特征时，自编码和它一样，甚至超越了 PCA。换句话说，自编码可以像 PCA 一样给特征属性降维。</p>
<h4 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h4><p>至于解码器 Decoder，我们也能那它来做点事情。我们知道，解码器在训练的时候是要将精髓信息解压成原始信息，那么这就提供了一个解压器的作用，甚至我们可以认为是一个生成器 (类似于<a href="https://mofanpy.com/tutorials/machine-learning/torch/intro-GAN">GAN</a>)。那做这件事的一种特殊自编码叫做 variational autoencoders，你能在<a href="http://kvfrans.com/variational-autoencoders-explained/">这里</a>找到他的具体说明。</p>
<p>有一个例子就是让它能模仿并生成手写数字。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/auto6.jpg"></p>
<p><em>Python相关教程</em></p>
<ul>
<li><em>Tensorflow Autoencoder <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/autoencoder">链接</a></em></li>
<li><em>PyTorch RNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/autoencoder">例子</a></em></li>
<li><em>Keras Autoencoder <a href="https://mofanpy.com/tutorials/machine-learning/keras/autoencoder">链接</a></em></li>
</ul>
<h3 id="AutoEncoder-自编码-非监督学习"><a href="#AutoEncoder-自编码-非监督学习" class="headerlink" title="AutoEncoder (自编码/非监督学习)"></a>AutoEncoder (自编码/非监督学习)</h3><h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>神经网络也能进行非监督学习，只需要训练数据，不需要标签数据。自编码就是这样一种形式。自编码能自动分类数据，而且也能嵌套在半监督学习的上面，用少量的有标签样本和大量的无标签样本学习。如果对自编码还没有太多概念，强烈推荐我的这个<a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-autoencoder">动画短片</a>，让你秒懂自编码。</p>
<p>这次我们还用 MNIST 手写数字数据来压缩再解压图片。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-4-1.gif"></p>
<p>然后用压缩的特征进行非监督分类。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-4-2.gif"></p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>自编码只用训练集就好了，而且只需要训练 training data 的 image，不用训练 labels。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">EPOCH = <span class="number">10</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line">DOWNLOAD_MNIST = <span class="literal">True</span>   <span class="comment"># 下过数据的话, 就可以设置成 False</span></span><br><span class="line">N_TEST_IMG = <span class="number">5</span>          <span class="comment"># 到时候显示 5张图片看效果, 如上图一</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mnist digits dataset</span></span><br><span class="line">train_data = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./mnist/&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,                                     <span class="comment"># this is training data</span></span><br><span class="line">    transform=torchvision.transforms.ToTensor(),    <span class="comment"># Converts a PIL.Image or numpy.ndarray to</span></span><br><span class="line">                                                    <span class="comment"># torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]</span></span><br><span class="line">    download=DOWNLOAD_MNIST,                        <span class="comment"># download it if you don&#x27;t have it</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/4-4-3.png"></p>
<p>这就是一张我们要训练的手写数字 4。</p>
<h4 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h4><p>AutoEncoder 形式很简单，分别是 <code>encoder</code> 和 <code>decoder</code>，压缩和解压，压缩后得到压缩的特征值，再从压缩的特征值解压成原图片。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AutoEncoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AutoEncoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 压缩</span></span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">12</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">3</span>),   <span class="comment"># 压缩成3个特征, 进行 3D 图像可视化</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 解压</span></span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3</span>, <span class="number">12</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">12</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">128</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            nn.Sigmoid(),       <span class="comment"># 激励函数让输出值在 (0, 1)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        encoded = self.encoder(x)</span><br><span class="line">        decoded = self.decoder(encoded)</span><br><span class="line">        <span class="keyword">return</span> encoded, decoded</span><br><span class="line"></span><br><span class="line">autoencoder = AutoEncoder()</span><br></pre></td></tr></table></figure>



<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>训练，并可视化训练的过程。我们可以有效的利用 <code>encoder</code> 和 <code>decoder</code> 来做很多事，比如这里我们用 <code>decoder</code> 的信息输出看和原图片的对比，还能用 <code>encoder</code> 来看经过压缩后，神经网络对原图片的理解。<code>encoder</code> 能将不同图片数据大概的分离开来。这样就是一个无监督学习的过程。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-4-1.gif"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)</span><br><span class="line">loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (x, b_label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        b_x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)   <span class="comment"># batch x, shape (batch, 28*28)</span></span><br><span class="line">        b_y = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)   <span class="comment"># batch y, shape (batch, 28*28)</span></span><br><span class="line"></span><br><span class="line">        encoded, decoded = autoencoder(b_x)</span><br><span class="line"></span><br><span class="line">        loss = loss_func(decoded, b_y)      <span class="comment"># mean square error</span></span><br><span class="line">        optimizer.zero_grad()               <span class="comment"># clear gradients for this training step</span></span><br><span class="line">        loss.backward()                     <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">        optimizer.step()                    <span class="comment"># apply gradients</span></span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results/torch/4-4-4.png"></p>
<h4 id="画3D图"><a href="#画3D图" class="headerlink" title="画3D图"></a>画3D图</h4><p><img src="https://static.mofanpy.com/results/torch/4-4-2.gif"></p>
<p>3D 的可视化图挺有趣的，还能挪动观看，更加直观，好理解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 要观看的数据</span></span><br><span class="line">view_data = train_data.train_data[:<span class="number">200</span>].view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>).<span class="built_in">type</span>(torch.FloatTensor)/<span class="number">255.</span></span><br><span class="line">encoded_data, _ = autoencoder(view_data)    <span class="comment"># 提取压缩的特征值</span></span><br><span class="line">fig = plt.figure(<span class="number">2</span>)</span><br><span class="line">ax = Axes3D(fig)    <span class="comment"># 3D 图</span></span><br><span class="line"><span class="comment"># x, y, z 的数据值</span></span><br><span class="line">X = encoded_data.data[:, <span class="number">0</span>].numpy()</span><br><span class="line">Y = encoded_data.data[:, <span class="number">1</span>].numpy()</span><br><span class="line">Z = encoded_data.data[:, <span class="number">2</span>].numpy()</span><br><span class="line">values = train_data.train_labels[:<span class="number">200</span>].numpy()  <span class="comment"># 标签值</span></span><br><span class="line"><span class="keyword">for</span> x, y, z, s <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, Z, values):</span><br><span class="line">    c = cm.rainbow(<span class="built_in">int</span>(<span class="number">255</span>*s/<span class="number">9</span>))    <span class="comment"># 上色</span></span><br><span class="line">    ax.text(x, y, z, s, backgroundcolor=c)  <span class="comment"># 标位子</span></span><br><span class="line">ax.set_xlim(X.<span class="built_in">min</span>(), X.<span class="built_in">max</span>())</span><br><span class="line">ax.set_ylim(Y.<span class="built_in">min</span>(), Y.<span class="built_in">max</span>())</span><br><span class="line">ax.set_zlim(Z.<span class="built_in">min</span>(), Z.<span class="built_in">max</span>())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://static.mofanpy.com/results-small/torch/4-4-5.png"></p>
<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py">github 代码</a> 中的每一步的意义啦。</p>
<h3 id="什么是-DQN"><a href="#什么是-DQN" class="headerlink" title="什么是 DQN"></a>什么是 DQN</h3><p>今天我们会来说说强化学习中的一种强大武器，Deep Q Network 简称为 DQN。Google Deep mind 团队就是靠着这 DQN 使计算机玩电动玩得比我们还厉害。</p>
<h4 id="强化学习与神经网络"><a href="#强化学习与神经网络" class="headerlink" title="强化学习与神经网络"></a>强化学习与神经网络</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN1.png"></p>
<p>之前我们所谈论到的强化学习方法都是比较传统的方式，而如今，随着机器学习在日常生活中的各种应用，各种机器学习方法也在融汇、合并、升级。而我们今天所要探讨的强化学习则是这么一种融合了神经网络和 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-q-learning">Q learning</a> 的方法，名字叫做 Deep Q Network。这种新型结构是为什么被提出来呢？原来，传统的表格形式的强化学习有这样一个瓶颈。</p>
<h4 id="神经网络的作用"><a href="#神经网络的作用" class="headerlink" title="神经网络的作用"></a>神经网络的作用</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN2.png"></p>
<p>我们使用表格来存储每一个状态 state，和在这个 state 每个行为 action 所拥有的 Q 值。而当今问题是在太复杂，状态可以多到比天上的星星还多(比如下围棋)。如果全用表格来存储它们，恐怕我们的计算机有再大的内存都不够，而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事。不过，在机器学习中，有一种方法对这种事情很在行，那就是神经网络。我们可以将状态和动作当成神经网络的输入，然后经过神经网络分析后得到动作的 Q 值，这样我们就没必要在表格中记录 Q 值，而是直接使用神经网络生成 Q 值。还有一种形式的是这样，我们也能只输入状态值，输出所有的动作值，然后按照 Q learning 的原则，直接选择拥有最大值的动作当做下一步要做的动作。我们可以想象，神经网络接受外部的信息，相当于眼睛鼻子耳朵收集信息，然后通过大脑加工输出每种动作的值，最后通过强化学习的方式选择动作。</p>
<p><img src="https://static.mofanpy.com/results/ML-intro/DQN4.png"></p>
<p>我们通过 NN 预测出Q(s2, a1) 和 Q(s2,a2) 的值，这就是 Q 估计。然后我们选取 Q 估计中最大值的动作来换取环境中的奖励 reward。而 Q 现实中也包含从神经网络分析出来的两个 Q 估计值，不过这个 Q 估计是针对于下一步在 s’ 的估计。最后再通过刚刚所说的算法更新神经网络中的参数。但是这并不是 DQN 会玩电动的根本原因。还有两大因素支撑着 DQN 使得它变得无比强大。这两大因素就是 Experience replay 和 Fixed Q-targets。</p>
<h4 id="DQN-两大利器"><a href="#DQN-两大利器" class="headerlink" title="DQN 两大利器"></a>DQN 两大利器</h4><p><img src="https://static.mofanpy.com/results/ML-intro/DQN5.png"></p>
<p>简单来说，DQN 有一个记忆库用于学习之前的经历。在之前的简介影片中提到过，Q learning 是一种 off-policy 离线学习法，它能学习当前经历着的，也能学习过去经历过的，甚至是学习别人的经历。所以每次 DQN 更新的时候，我们都可以随机抽取一些之前的经历进行学习。随机抽取这种做法打乱了经历之间的相关性，也使得神经网络更新更有效率。Fixed Q-targets 也是一种打乱相关性的机理，如果使用 fixed Q-targets，我们就会在 DQN 中使用到两个结构相同但参数不同的神经网络，预测 Q 估计的神经网络具备最新的参数，而预测 Q 现实的神经网络使用的参数则是很久以前的。有了这两种提升手段，DQN 才能在一些游戏中超越人类。</p>
<blockquote>
<p>Q：在DQN中，为什么要用两个网络呢？</p>
<p>A：以前的研究者都是用一个网络，但是发现了问题，一个网络的抖动太厉害了，学习极不稳定。强化学习对于数据分布的变化非常敏感，所以有人尝试用两个网络，将target net半固定住，降低抖动，增强收敛性。</p>
</blockquote>
<h3 id="DQN-强化学习"><a href="#DQN-强化学习" class="headerlink" title="DQN 强化学习"></a>DQN 强化学习</h3><h4 id="要点-1"><a href="#要点-1" class="headerlink" title="要点"></a>要点</h4><p>Torch 是神经网络库，那么也可以拿来做强化学习，之前我用另一个强大神经网络库 <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/">Tensorflow</a> 来制作了这一个 <a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">从浅入深强化学习教程</a>，你同样也可以用 PyTorch 来实现，这次我们就举 DQN 的例子，我对比了我的 Tensorflow DQN 的代码，发现 PyTorch 写的要简单很多。如果对 DQN 或者强化学习还没有太多概念，强烈推荐我的这个<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">DQN动画短片</a>，让你秒懂DQN。还有强推这套花了我几个月来制作的<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">强化学习教程</a>！</p>
<h4 id="模块导入和参数设置"><a href="#模块导入和参数设置" class="headerlink" title="模块导入和参数设置"></a>模块导入和参数设置</h4><p>这次除了 Torch 自家模块，我们还要导入 Gym 环境库模块，<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/gym">如何安装 gym 模块请看这节教程</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">LR = <span class="number">0.01</span>                   <span class="comment"># learning rate</span></span><br><span class="line">EPSILON = <span class="number">0.9</span>               <span class="comment"># 最优选择动作百分比</span></span><br><span class="line">GAMMA = <span class="number">0.9</span>                 <span class="comment"># 奖励递减参数</span></span><br><span class="line">TARGET_REPLACE_ITER = <span class="number">100</span>   <span class="comment"># Q 现实网络的更新频率</span></span><br><span class="line">MEMORY_CAPACITY = <span class="number">2000</span>      <span class="comment"># 记忆库大小</span></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)   <span class="comment"># 立杆子游戏</span></span><br><span class="line">env = env.unwrapped</span><br><span class="line">N_ACTIONS = env.action_space.n  <span class="comment"># 杆子能做的动作</span></span><br><span class="line">N_STATES = env.observation_space.shape[<span class="number">0</span>]   <span class="comment"># 杆子能获取的环境信息数</span></span><br></pre></td></tr></table></figure>



<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p>DQN 当中的神经网络模式，我们将依据这个模式建立两个神经网络，一个是现实网络 (Target Net)，一个是估计网络 (Eval Net)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, </span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(N_STATES, <span class="number">10</span>)</span><br><span class="line">        self.fc1.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)   <span class="comment"># initialization</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">10</span>, N_ACTIONS)</span><br><span class="line">        self.out.weight.data.normal_(<span class="number">0</span>, <span class="number">0.1</span>)   <span class="comment"># initialization</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        actions_value = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> actions_value</span><br></pre></td></tr></table></figure>



<h4 id="DQN体系"><a href="#DQN体系" class="headerlink" title="DQN体系"></a>DQN体系</h4><p>简化的 DQN 体系是这样，我们有两个 net，有选动作机制，有存经历机制，有学习机制。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 建立 target net 和 eval net 还有 memory</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 根据环境观测值选择动作的机制</span></span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        <span class="comment"># 存储记忆</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># target 网络更新</span></span><br><span class="line">        <span class="comment"># 学习记忆库中的记忆</span></span><br></pre></td></tr></table></figure>

<p>接下来就是具体的啦，在 DQN 中每个功能都是怎么做的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.eval_net, self.target_net = Net(), Net()</span><br><span class="line"></span><br><span class="line">        self.learn_step_counter = <span class="number">0</span>     <span class="comment"># 用于 target 更新计时</span></span><br><span class="line">        self.memory_counter = <span class="number">0</span>         <span class="comment"># 记忆库记数</span></span><br><span class="line">        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * <span class="number">2</span> + <span class="number">2</span>))     <span class="comment"># 初始化记忆库</span></span><br><span class="line">        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    <span class="comment"># torch 的优化器</span></span><br><span class="line">        self.loss_func = nn.MSELoss()   <span class="comment"># 误差公式</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = torch.unsqueeze(torch.FloatTensor(x), <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 这里只输入一个 sample</span></span><br><span class="line">        <span class="keyword">if</span> np.random.uniform() &lt; EPSILON:   <span class="comment"># 选最优动作</span></span><br><span class="line">            actions_value = self.eval_net.forward(x)</span><br><span class="line">            action = torch.<span class="built_in">max</span>(actions_value, <span class="number">1</span>)[<span class="number">1</span>].data.numpy()[<span class="number">0</span>, <span class="number">0</span>]     <span class="comment"># return the argmax</span></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment"># 选随机动作</span></span><br><span class="line">            action = np.random.randint(<span class="number">0</span>, N_ACTIONS)</span><br><span class="line">        <span class="keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store_transition</span>(<span class="params">self, s, a, r, s_</span>):</span></span><br><span class="line">        transition = np.hstack((s, [a, r], s_))</span><br><span class="line">        <span class="comment"># 如果记忆库满了, 就覆盖老数据</span></span><br><span class="line">        index = self.memory_counter % MEMORY_CAPACITY</span><br><span class="line">        self.memory[index, :] = transition</span><br><span class="line">        self.memory_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># target net 参数更新</span></span><br><span class="line">        <span class="keyword">if</span> self.learn_step_counter % TARGET_REPLACE_ITER == <span class="number">0</span>:</span><br><span class="line">            self.target_net.load_state_dict(self.eval_net.state_dict())</span><br><span class="line">        self.learn_step_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 抽取记忆库中的批数据</span></span><br><span class="line">        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)</span><br><span class="line">        b_memory = self.memory[sample_index, :]</span><br><span class="line">        b_s = torch.FloatTensor(b_memory[:, :N_STATES])</span><br><span class="line">        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+<span class="number">1</span>].astype(<span class="built_in">int</span>))</span><br><span class="line">        b_r = torch.FloatTensor(b_memory[:, N_STATES+<span class="number">1</span>:N_STATES+<span class="number">2</span>])</span><br><span class="line">        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 针对做过的动作b_a, 来选 q_eval 的值, (q_eval 原本有所有动作的值)</span></span><br><span class="line">        q_eval = self.eval_net(b_s).gather(<span class="number">1</span>, b_a)  <span class="comment"># shape (batch, 1)</span></span><br><span class="line">        q_next = self.target_net(b_s_).detach()     <span class="comment"># q_next 不进行反向传递误差, 所以 detach</span></span><br><span class="line">        q_target = b_r + GAMMA * q_next.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>]   <span class="comment"># shape (batch, 1)</span></span><br><span class="line">        loss = self.loss_func(q_eval, q_target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算, 更新 eval net</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br></pre></td></tr></table></figure>



<h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><p>按照 Qlearning 的形式进行 off-policy 的更新。我们进行回合制更行，一个回合完了，进入下一回合。一直到他们将杆子立起来很久。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dqn = DQN() <span class="comment"># 定义 DQN 系统</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">400</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        env.render()    <span class="comment"># 显示实验动画</span></span><br><span class="line">        a = dqn.choose_action(s)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选动作, 得到环境反馈</span></span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 修改 reward, 使 DQN 快速学习</span></span><br><span class="line">        x, x_dot, theta, theta_dot = s_</span><br><span class="line">        r1 = (env.x_threshold - <span class="built_in">abs</span>(x)) / env.x_threshold - <span class="number">0.8</span></span><br><span class="line">        r2 = (env.theta_threshold_radians - <span class="built_in">abs</span>(theta)) / env.theta_threshold_radians - <span class="number">0.5</span></span><br><span class="line">        r = r1 + r2</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存记忆</span></span><br><span class="line">        dqn.store_transition(s, a, r, s_)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dqn.memory_counter &gt; MEMORY_CAPACITY:</span><br><span class="line">            dqn.learn() <span class="comment"># 记忆库满了就进行学习</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> done:    <span class="comment"># 如果回合结束, 进入下回合</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        s = s_</span><br></pre></td></tr></table></figure>

<p>所以这也就是在我 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/405_DQN_Reinforcement_learning.py">github 代码</a> 中的每一步的意义啦。</p>
<h4 id="附加-A3C"><a href="#附加-A3C" class="headerlink" title="附加 A3C"></a>附加 A3C</h4><p>强化学习中还有一个非常厉害的算法，叫做 <a href="https://arxiv.org/pdf/1602.01783.pdf">A3C</a>。我做过一个这个算法的<a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-A3C">简介视频</a>，它非常合理地运用了多核计算机的能力，让我们能使用多个核来训练强化学习。我也用 pytorch 将这个算法给<a href="https://github.com/MorvanZhou/pytorch-A3C">实现</a>了。代码非常简单，可以用来做连续动作的环境。训练的效果可以看到这里。</p>
<p><img src="https://static.mofanpy.com/results-small/reinforcement-learning/6-3-8.png"></p>
<h3 id="什么是生成对抗网络-GAN"><a href="#什么是生成对抗网络-GAN" class="headerlink" title="什么是生成对抗网络 (GAN)"></a>什么是生成对抗网络 (GAN)</h3><p>GAN，又称生成对抗网络，也是 Generative Adversarial Nets 的简称。</p>
<h4 id="常见神经网络形式"><a href="#常见神经网络形式" class="headerlink" title="常见神经网络形式"></a>常见神经网络形式</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan2.png"></p>
<p>神经网络分很多种，有普通的前向传播神经网络，有分析图片的 CNN 卷积神经网络，有分析序列化数据，比如语音的 RNN 循环神经网络，这些神经网络都是用来输入数据，得到想要的结果，我们看中的是这些神经网络能很好的将数据与结果通过某种关系联系起来。</p>
<h4 id="生成网络"><a href="#生成网络" class="headerlink" title="生成网络"></a>生成网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan3.png"></p>
<p>但是还有另外一种形式的神经网络，他不是用来把数据对应上结果的，而是用来”凭空”捏造结果，这就是我们要说的生成网络啦。GAN 就是其中的一种形式。那么 GAN 是怎么做到的呢？当然这里的”凭空”并不是什么都没有的空盒子，而是一些随机数。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/gan4.png"></p>
<p>对，你没听错，我们就是用没有意义的随机数来生成有有意义的作品，比如著名画作。当然，这还不是全部，这只是一个 GAN 的一部分而已，这一部分的神经网络我们可以想象成是一个新手画家。</p>
<h4 id="新手画家"><a href="#新手画家" class="headerlink" title="新手画家"></a>新手画家</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan5.png"></p>
<p>画家作画都需要点灵感，他们都是依照自己的灵感来完成作品。有了灵感不一定有用，因为他的作画技术并没有我们想象得好，画出来有可能是一团糟。这可怎么办，聪明的新手画家找到了自己的一个正在学鉴赏的好朋友 – 新手鉴赏家。</p>
<h4 id="新手鉴赏家"><a href="#新手鉴赏家" class="headerlink" title="新手鉴赏家"></a>新手鉴赏家</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan6.png"></p>
<p>可是新手鉴赏家也没什么能耐，他也不知道如何鉴赏著名画作，所以坐在电脑旁边的你实在看不下去了，拿起几个标签往屏幕上一甩，然后新手鉴赏家就被你这样一次次的甩来甩去着甩乖了，慢慢也学会了怎么样区分著名画家的画了。重要的是，新手鉴赏家和新手画家是好朋友，他们总爱分享学习到的东西。</p>
<h4 id="新手鉴赏家和新手画家"><a href="#新手鉴赏家和新手画家" class="headerlink" title="新手鉴赏家和新手画家"></a>新手鉴赏家和新手画家</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan7.png"></p>
<p>所以新手鉴赏家告诉新手画家，“你的画实在太丑了，你看看人家达芬奇，你也学学它呀，比如这里要多加一点，这里要画淡一点。” 就这样，新手鉴赏家将他从你这里所学到的知识都分享给了新手画家，让好朋友新手画家也能越画越像达芬奇。这就是 GAN 的整套流程，我们在来理一下。</p>
<p><img src="https://static.mofanpy.com/results-small/ML-intro/gan8.png"></p>
<p>新手画家用随机灵感画画，新手鉴赏家会接收一些画作，但是他不知道这是新手画家画的还是著名画家画的，他说出他的判断，你来纠正他的判断，新手鉴赏家一边学如何判断，一边告诉新手画家要怎么画才能画得更像著名画家，新手画家就能学习到如何从自己的灵感画出更像著名画家的画了。GAN 也就这么回事。</p>
<h4 id="GAN-网络"><a href="#GAN-网络" class="headerlink" title="GAN 网络"></a>GAN 网络</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan9.png"></p>
<p>Generator 会根据随机数来生成有意义的数据，Discriminator 会学习如何判断哪些是真实数据，哪些是生成数据，然后将学习的经验反向传递给 Generator，让 Generator 能根据随机数生成更像真实数据的数据。这样训练出来的 Generator 可以有很多用途，比如最近有人就拿它来生成各种卧室的图片。</p>
<h4 id="GAN-应用"><a href="#GAN-应用" class="headerlink" title="GAN 应用"></a>GAN 应用</h4><p><img src="https://static.mofanpy.com/results-small/ML-intro/gan10.png"></p>
<p>甚至你还能玩点新花样，比如让图片来做加减法，戴眼镜的男人减去男人加上女人, 他居然能生成戴眼镜的女人的图片。甚至还能根据你随便画的几笔草图来生成可能是你需要的蓝天白云大草地图片。哈哈，看起来机器也能有想象力啦。如果你想试着动手做一个 GAN 的实践，却不知道如何做，不用担心，我也为准备好了一个使用 Python 和他神经网络模块搭建的最简单的 GAN 实践代码。欢迎大家访问莫烦 Python 了解更多机器学习的内容。</p>
<p><em>Python相关教程</em></p>
<ul>
<li><em>Tensorflow <a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/406_GAN.py">50行 GAN 代码</a></em></li>
<li><em>PyTorch <a href="https://mofanpy.com/tutorials/machine-learning/torch/GAN">GAN 教程</a></em></li>
</ul>
<h3 id="GAN-Generative-Adversarial-Nets-生成对抗网络"><a href="#GAN-Generative-Adversarial-Nets-生成对抗网络" class="headerlink" title="GAN (Generative Adversarial Nets 生成对抗网络)"></a>GAN (Generative Adversarial Nets 生成对抗网络)</h3><h4 id="要点-2"><a href="#要点-2" class="headerlink" title="要点"></a>要点</h4><p>GAN 是一个近几年比较流行的生成网络形式。对比起传统的生成模型，他减少了模型限制和生成器限制，他具有有更好的生成能力。人们常用假钞鉴定者和假钞制造者来打比喻，但是我不喜欢这个比喻，觉得没有真实反映出 GAN 里面的机理。</p>
<p>所以我的一句话介绍 GAN 就是：Generator 是新手画家，Discriminator 是新手鉴赏家，你是高级鉴赏家。你将著名画家的品和新手画家的作品都给新手鉴赏家评定，并告诉新手鉴赏家哪些是新手画家画的，哪些是著名画家画的，新手鉴赏家就慢慢学习怎么区分新手画家和著名画家的画，但是新手画家和新手鉴赏家是好朋友，新手鉴赏家会告诉新手画家要怎么样画得更像著名画家，新手画家就能将自己的突然来的灵感 (random noise) 画得更像著名画家。我用一个短动画形式来诠释了整个过程 (<a href="https://mofanpy.com/tutorials/machine-learning/torch/intro-GAN">GAN 动画简介</a>)。</p>
<p>下面是本节内容的效果，绿线的变化是新手画家慢慢学习如何踏上画家之路的过程。而能被认定为著名的画作在 <code>upper bound</code> 和 <code>lower bound</code> 之间。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-6-1.gif"></p>
<h4 id="超参数设置"><a href="#超参数设置" class="headerlink" title="超参数设置"></a>超参数设置</h4><p>新手画家 (Generator) 在作画的时候需要有一些灵感 (random noise)，我们这些灵感的个数定义为 <code>N_IDEAS</code>。而一幅画需要有一些规格，我们将这幅画的画笔数定义一下，<code>N_COMPONENTS</code> 就是一条一元二次曲线(这幅画画)上的点个数。为了进行批训练，我们将一整批话的点都规定一下(<code>PAINT_POINTS</code>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)    <span class="comment"># reproducible</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">LR_G = <span class="number">0.0001</span>           <span class="comment"># learning rate for generator</span></span><br><span class="line">LR_D = <span class="number">0.0001</span>           <span class="comment"># learning rate for discriminator</span></span><br><span class="line">N_IDEAS = <span class="number">5</span>             <span class="comment"># think of this as number of ideas for generating an art work (Generator)</span></span><br><span class="line">ART_COMPONENTS = <span class="number">15</span>     <span class="comment"># it could be total point G can draw in the canvas</span></span><br><span class="line">PAINT_POINTS = np.vstack([np.linspace(-<span class="number">1</span>, <span class="number">1</span>, ART_COMPONENTS) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(BATCH_SIZE)])</span><br></pre></td></tr></table></figure>



<h4 id="著名画家的画"><a href="#著名画家的画" class="headerlink" title="著名画家的画"></a>著名画家的画</h4><p>我们需要有很多画是来自著名画家的(real data)，将这些著名画家的画，和新手画家的画都传给新手鉴赏家，让鉴赏家来区分哪些是著名画家，哪些是新手画家的画。如何区分我们在后面呈现。这里我们生成一些著名画家的画 (batch 条不同的一元二次方程曲线)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">artist_works</span>():</span>     <span class="comment"># painting from the famous artist (real target)</span></span><br><span class="line">    a = np.random.uniform(<span class="number">1</span>, <span class="number">2</span>, size=BATCH_SIZE)[:, np.newaxis]</span><br><span class="line">    paintings = a * np.power(PAINT_POINTS, <span class="number">2</span>) + (a-<span class="number">1</span>)</span><br><span class="line">    paintings = torch.from_numpy(paintings).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> paintings</span><br></pre></td></tr></table></figure>

<p>下面就是会产生曲线的一个上限和下限。</p>
<p><img src="https://static.mofanpy.com/results/torch/4-6-1.png"></p>
<h4 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h4><p>这里会创建两个神经网络，分别是 Generator (新手画家)，Discriminator(新手鉴赏家)。<code>G</code> 会拿着自己的一些灵感当做输入，输出一元二次曲线上的点 (<code>G</code> 的画)。</p>
<p><code>D</code> 会接收一幅画作 (一元二次曲线)，输出这幅画作到底是不是著名画家的画(是著名画家的画的概率)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">G = nn.Sequential(                      <span class="comment"># Generator</span></span><br><span class="line">    nn.Linear(N_IDEAS, <span class="number">128</span>),            <span class="comment"># random ideas (could from normal distribution)</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, ART_COMPONENTS),     <span class="comment"># making a painting from these random ideas</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">D = nn.Sequential(                      <span class="comment"># Discriminator</span></span><br><span class="line">    nn.Linear(ART_COMPONENTS, <span class="number">128</span>),     <span class="comment"># receive art work either from the famous artist or a newbie like G</span></span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">    nn.Sigmoid(),                       <span class="comment"># tell the probability that the art work is made by artist</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>



<h4 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h4><p>接着我们来同时训练 <code>D</code> 和 <code>G</code>。训练之前，我们来看看<code>G</code>作画的原理。<code>G</code> 首先会有些灵感，<code>G_ideas</code> 就会拿到这些随机灵感 (可以是正态分布的随机数)，然后 <code>G</code> 会根据这些灵感画画。接着我们拿着著名画家的画和 <code>G</code> 的画，让 <code>D</code> 来判定这两批画作是著名画家画的概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    artist_paintings = artist_works()           <span class="comment"># real painting from artist</span></span><br><span class="line">    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)    <span class="comment"># random ideas</span></span><br><span class="line">    G_paintings = G(G_ideas())                  <span class="comment"># fake painting from G (random ideas)</span></span><br><span class="line"></span><br><span class="line">    prob_artist0 = D(artist_paintings)          <span class="comment"># D try to increase this prob</span></span><br><span class="line">    prob_artist1 = D(G_paintings)               <span class="comment"># D try to reduce this prob</span></span><br></pre></td></tr></table></figure>

<p>然后计算有多少来之画家的画猜对了，有多少来自 <code>G</code> 的画猜对了，我们想最大化这些猜对的次数。这也就是 <code>log(D(x)) + log(1-D(G(z))</code> 在<a href="https://arxiv.org/abs/1406.2661">论文</a>中的形式。而因为 torch 中提升参数的形式是最小化误差，那我们把最大化 <code>score</code> 转换成最小化 <code>loss</code>，在两个 <code>score</code> 的合的地方加一个符号就好。而 <code>G</code> 的提升就是要减小 <code>D</code> 猜测 <code>G</code> 生成数据的正确率，也就是减小 <code>D_score1</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(<span class="number">1.</span> - prob_artist1))</span><br><span class="line">    G_loss = torch.mean(torch.log(<span class="number">1.</span> - prob_artist1))</span><br></pre></td></tr></table></figure>

<p>最后我们在根据 <code>loss</code> 提升神经网络就好了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt_D.zero_grad()</span><br><span class="line">    D_loss.backward(retain_graph=<span class="literal">True</span>)      <span class="comment"># retain_graph 这个参数是为了再次使用计算图纸</span></span><br><span class="line">    opt_D.step()</span><br><span class="line"></span><br><span class="line">    opt_G.zero_grad()</span><br><span class="line">    G_loss.backward()</span><br><span class="line">    opt_G.step()</span><br></pre></td></tr></table></figure>

<p>上面的全部代码内容在我的 <a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py">github</a>。</p>
<h4 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h4><p>可视化的代码很简单，在这里就不会意义叙说了，大家直接看<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py">代码</a> 吧。在本节的最上面就是这次的动图效果，最后达到收敛时，结果如下，<code>G</code> 能成功的根据自己的灵感，产生出一条很像 <code>artist</code> 画出的曲线，而 <code>D</code> 再也没有能力猜出这到底是 <code>G</code> 的画作还是 <code>artist</code> 的画作，他只能一半时间猜是 <code>G</code> 的，一半时间猜是 <code>artist</code>的。</p>
<p><img src="https://static.mofanpy.com/results-small/torch/4-6-2.png"></p>
<blockquote>
<p><strong>原文地址：</strong><a href="https://mofanpy.com/tutorials/machine-learning/torch/">PyTorch | 莫烦Python (mofanpy.com)</a></p>
<p><strong>学习资料：</strong></p>
<ul>
<li>Tensorflow Autoencoder <a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/autoencoder">链接</a></li>
<li>PyTorch RNN <a href="https://mofanpy.com/tutorials/machine-learning/torch/autoencoder">例子</a></li>
<li>Keras Autoencoder <a href="https://mofanpy.com/tutorials/machine-learning/keras/autoencoder">链接</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/404_autoencoder.py">第二节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/404_AutoEncoder.py">Tensorflow 的 50行 AutoEncoder 代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/tensorflow/intro-autoencoder">我制作的 自编码 动画简介</a></li>
<li><a href="http://pytorch.org/">PyTorch 官网</a></li>
</ul>
<hr>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning">强化学习教程</a></li>
<li><a href="https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_">强化学习模拟程序</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">DQN Tensorflow Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/DQN">DQN PyTorch Python 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1">强化学习实战</a></li>
<li>论文 <a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/405_DQN_Reinforcement_learning.py">第四节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/405_DQN_reinforcement_learning.py">Tensorflow 的 100行 DQN 代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN">我制作的 DQN 动画简介</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/DQN1">我的 DQN Tensorflow 教程</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">我的 强化学习 教程</a></li>
</ul>
<hr>
<ul>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/GAN">PyTorch GAN 教程</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/406_GAN.py">Tensorflow 50行 GAN 代码</a></li>
<li><a href="https://arxiv.org/abs/1406.2661">论文 Generative Adversarial Networks</a></li>
</ul>
<hr>
<ul>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_GAN.py">第六节的全部代码</a></li>
<li><a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/406_conditional_GAN.py">Conditional GAN 代码</a></li>
<li><a href="https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/406_GAN.py">Tensorflow 50行 GAN 代码</a></li>
<li><a href="https://mofanpy.com/tutorials/machine-learning/torch/intro-GAN">我制作的 GAN 动画简介</a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch 神经网络</tag>
      </tags>
  </entry>
</search>
